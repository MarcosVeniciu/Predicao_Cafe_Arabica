{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb787a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requeriment\n",
    "#%capture\n",
    "!pip cache purge\n",
    "!mkdir -p ~/tmp_pip\n",
    "!TMPDIR=~/tmp_pip pip install --no-cache-dir neuralforecast statsforecast optuna plotly scikit-learn lightning ipywidgets openpyxl nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c298a4",
   "metadata": {},
   "source": [
    "# Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71d5ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "from neuralforecast.losses.pytorch import HuberLoss\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from neuralforecast import NeuralForecast\n",
    "from IPython.display import clear_output\n",
    "from neuralforecast.models import LSTM\n",
    "from datetime import datetime\n",
    "from torch.optim import AdamW\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import shutil\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c67d025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe para facilitar o uso de cores no terminal\n",
    "class CoresTerminal:\n",
    "    \"\"\"Contém códigos ANSI para colorir o output no terminal.\"\"\"\n",
    "    VERMELHO = '\\033[91m'\n",
    "    VERDE = '\\033[92m'\n",
    "    FIM = '\\033[0m'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b108d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch\n",
    "import time\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# VARIÁVEIS GLOBAIS PARA LOGGING DE MÉTRICAS DE TREINAMENTO\n",
    "score_media_simples = 0.0\n",
    "score_rmse = 0.0\n",
    "training_duration = 0.0\n",
    "tempo_total = 0.0\n",
    "avg_training_time = 0.0\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# FUNÇÃO OBJECTIVE (RESPONSABILIDADE: TREINAR E AVALIAR)\n",
    "# ===================================================================\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Treina e avalia um modelo para um conjunto de hiperparâmetros.\n",
    "    Salva os artefatos do trial em uma pasta temporária.\n",
    "    \"\"\"\n",
    "    global score_media_simples, score_rmse, training_duration, avg_training_time, tempo_total\n",
    "\n",
    "    string_saidas = f\"\\nTeste do Modelo: {treino_id}_({ano_teste})\\n\"\n",
    "    clear_output()\n",
    "    print(string_saidas)\n",
    "    # 1) Hiperparâmetros a serem otimizados\n",
    "    #steps_options = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "    #encoder_n_layers = trial.suggest_int(\"encoder_n_layers\", 3, 8)\n",
    "    #encoder_hidden_size = trial.suggest_categorical(\"encoder_hidden_size\", [64, 128, 192, 256])\n",
    "    #decoder_layers = trial.suggest_int(\"decoder_layers\", 1, 4)\n",
    "    #decoder_hidden_size = trial.suggest_categorical(\"decoder_hidden_size\", [64, 128, 192, 256])\n",
    "    #weight_decay = trial.suggest_categorical(\"weight_decay\", [1e-5, 1e-4, 1e-2])\n",
    "    #steps = trial.suggest_categorical(\"steps\", steps_options)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    # Parâmetros fixos\n",
    "    batch_size = 32\n",
    "    dropout = 0.3\n",
    "    steps = 1000\n",
    "    encoder_n_layers= 4\n",
    "    encoder_hidden_size= 192\n",
    "    decoder_layers= 1\n",
    "    decoder_hidden_size= 64\n",
    "    weight_decay= 0.0001\n",
    "    # --- Gerenciamento de Logs por Trial ---\n",
    "    trial_log_dir = log_dir_base / f\"trial_{trial.number}\"\n",
    "    if trial_log_dir.exists():\n",
    "        shutil.rmtree(trial_log_dir)\n",
    "    trial_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    csv_logger = CSVLogger(save_dir=str(trial_log_dir.parent), name=trial_log_dir.name)\n",
    "\n",
    "    # 2) Instanciação do modelo LSTM\n",
    "    model = LSTM(\n",
    "        h=h,\n",
    "        input_size=input_size,\n",
    "        batch_size=batch_size,\n",
    "        scaler_type=\"revin\",\n",
    "        encoder_dropout=dropout,\n",
    "        encoder_n_layers=encoder_n_layers,\n",
    "        encoder_hidden_size=encoder_hidden_size,\n",
    "        decoder_layers=decoder_layers,\n",
    "        decoder_hidden_size=decoder_hidden_size,\n",
    "        futr_exog_list=exog_list,\n",
    "        learning_rate=learning_rate,\n",
    "        max_steps=steps,\n",
    "        loss=HuberLoss(delta=1.0),\n",
    "        optimizer=AdamW,\n",
    "        optimizer_kwargs={\"weight_decay\": weight_decay},\n",
    "        lr_scheduler=StepLR,\n",
    "        lr_scheduler_kwargs={\"step_size\": int(steps * 0.5), \"gamma\": 0.1},\n",
    "        logger=csv_logger,\n",
    "        random_seed=42\n",
    "    )\n",
    "\n",
    "    # 3) Treinamento\n",
    "    nf = NeuralForecast(models=[model], freq=\"YE\")\n",
    "    \n",
    "    if trial.number != 0:\n",
    "        print(f\"Resultados da Trial anterior:\")\n",
    "        print(f\"  - WMAPE: {score_media_simples:.4f}\")\n",
    "        print(f\"  - RMSE:  {score_rmse:.4f}\")\n",
    "        print(f\"  - Tempo Total Treino: {training_duration:.2f}s\")\n",
    "        print(f\"  - Tempo médio de treino até agora: {avg_training_time:.2f}s\")\n",
    "        print(f\"  - Tempo estimado para conclusão (baseado na média): {(num_trials - trial.number) * avg_training_time / 60:.2f} minutos\\n\")\n",
    "\n",
    "    print(f\"Iniciando treinamento do Trial {trial.number}...\")\n",
    "    start_time = time.time()\n",
    "    nf.fit(df=train_ds)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Cálculo do tempo de treinamento\n",
    "    training_duration = end_time - start_time\n",
    "    tempo_total += training_duration\n",
    "    avg_training_time = tempo_total / (trial.number + 1)\n",
    "\n",
    "    # 4) Avaliação\n",
    "    combined_df = evaluate_simple_forecast(\n",
    "        model=nf,\n",
    "        train_df=train_ds,\n",
    "        test_df=val_ds,\n",
    "        string_saidas = string_saidas\n",
    "    )\n",
    "    \n",
    "\n",
    "    # 5) Cálculo da métrica\n",
    "    actual = combined_df[\"y\"]\n",
    "    predicted = combined_df[\"LSTM\"]\n",
    "\n",
    "    # 5) Cálculo das métricas\n",
    "    if np.isfinite(combined_df[\"LSTM\"]).all():\n",
    "        score_media_simples = calcular_media_wmape_simples(combined_df)\n",
    "        score_rmse = root_mean_squared_error(actual, predicted)\n",
    "        score = score_media_simples\n",
    "    else:\n",
    "        # Penalidade para previsões inválidas (NaN, inf)\n",
    "        score = 1e10 \n",
    "        print(\"Previsão inválida encontrada. Atribuindo score de penalidade.\")\n",
    "\n",
    "\n",
    "    # --- SALVAMENTO TEMPORÁRIO DOS ARTEFATOS DO TRIAL ---\n",
    "    trial_artifact_path = temp_model_dir / f\"trial_{trial.number}\"\n",
    "    nf.save(path=str(trial_artifact_path), overwrite=True)\n",
    "    \n",
    "    # --- LÓGICA DE ESPERA ATIVA (POLLING) ---\n",
    "    # CORREÇÃO: Construímos o caminho para version_0 explicitamente, pois sabemos que será sempre este.\n",
    "    log_version_dir = trial_log_dir / \"version_0\"\n",
    "    metrics_path = log_version_dir / \"metrics.csv\"\n",
    "    hparams_path = log_version_dir / \"hparams.yaml\"\n",
    "    \n",
    "    file_found = False\n",
    "    # Tenta encontrar o arquivo por até 10 segundos\n",
    "    for _ in range(100):\n",
    "        if metrics_path.exists():\n",
    "            file_found = True\n",
    "            break\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # Copia os arquivos de log se eles foram encontrados\n",
    "    if file_found:\n",
    "        shutil.copy(metrics_path, trial_artifact_path / \"metrics.csv\")\n",
    "        shutil.copy(hparams_path, trial_artifact_path / \"hparams.yaml\")\n",
    "    else:\n",
    "        print(f\"AVISO: Arquivo metrics.csv não encontrado em {log_version_dir} para o trial {trial.number}\")\n",
    "\n",
    "    # Anexa informações ao trial para o callback poder acessá-las\n",
    "    trial.set_user_attr(\"artifact_path\", str(trial_artifact_path))\n",
    "    \n",
    "    # Converte a coluna de data para string para garantir que seja serializável em JSON.\n",
    "    combined_df_serializable = combined_df.copy()\n",
    "    if 'ds' in combined_df_serializable.columns:\n",
    "        combined_df_serializable['ds'] = combined_df_serializable['ds'].astype(str)\n",
    "    trial.set_user_attr(\"prediction_df\", combined_df_serializable.to_dict())\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa759f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def teste_modelo(local, csv_dir, dataset, train_ds, val_ds, test_ds, comentario, nome_dataset, string_saidas = \"\"\"\"\"\", h = 1):\n",
    "    string_saidas += f\"\\nTeste do Modelo: {local}\"\n",
    "    clear_output()\n",
    "    print(string_saidas)\n",
    "    # carregamento dos hiperparâmetros do melhor modelo salvo\n",
    "    path_para_hparams = f\"{local}/hparams.yaml\" \n",
    "    params_extraidos = get_hyperparameters_from_yaml(path_para_hparams)\n",
    "\n",
    "    # carregamento do modelo salvo\n",
    "    model = NeuralForecast.load(path=f\"{local}\")\n",
    "    \n",
    "    # --- Execução da avaliação do dados de teste --- \n",
    "    predictions = evaluate_simple_forecast(\n",
    "        model=model,\n",
    "        train_df=dataset,\n",
    "        test_df=test_ds\n",
    "    )\n",
    "\n",
    "    dados_teste = predictions[['unique_id', 'ds', 'y', 'LSTM', 'diferença_%']].copy()\n",
    "    dados_teste.columns = ['unique_id', 'ds', 'y', 'y_pred', 'diferença_%'] # Renomeando as colunas para o formato esperado\n",
    "    dados_teste['flag'] = 'teste'\n",
    "    dados_teste['ds'] = pd.to_datetime(dados_teste['ds']).fillna(dados_teste['ds'])\n",
    "    dados_teste['ds'] = (pd.to_datetime(dados_teste['ds'], errors='coerce').fillna(pd.Timestamp.now()).dt.strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "\n",
    "\n",
    "    # --- Execução da avaliação do dados de validação --- \n",
    "    if val_ds is not None:\n",
    "        dados_validacao = evaluate_simple_forecast(\n",
    "            model=model,\n",
    "            train_df=dataset,\n",
    "            test_df=val_ds\n",
    "        )\n",
    "        dados_validacao = dados_validacao[['unique_id', 'ds', 'y', 'LSTM', 'diferença_%']].copy()\n",
    "        dados_validacao.columns = ['unique_id', 'ds', 'y', 'y_pred', 'diferença_%'] # Renomeando as colunas para o formato esperado\n",
    "        dados_validacao['flag'] = 'validacao'\n",
    "        dados_validacao['ds'] = pd.to_datetime(dados_validacao['ds']).fillna(dados_validacao['ds'])\n",
    "        dados_validacao['ds'] = (pd.to_datetime(dados_validacao['ds'], errors='coerce').fillna(pd.Timestamp.now()).dt.strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "    \n",
    "    # --- Execução da avaliação do dados de treino --- \n",
    "    dados_treino = dataset[['unique_id', 'ds', 'y']].copy()\n",
    "    dados_treino['y'] = round(np.expm1(dados_treino['y']),2)\n",
    "    # identifica o ano de termino do treino (pode ser o inicio da validação ou dos testes)\n",
    "    if val_ds is not None:\n",
    "        data_inicio = val_ds['ds'].min() # identifica o periodo de inicio da validação\n",
    "        print(f\"Periodo de inicio da Validação: {data_inicio}\\n\")\n",
    "    else:\n",
    "        data_inicio = dados_teste['ds'].min() # identifica o periodo de inicio dos teste\n",
    "        print(f\"Periodo de inicio dos Testes: {data_inicio}\\n\")\n",
    "    dados_treino = dados_treino[dados_treino['ds'] < data_inicio] # pega apenas as datas anteriores a data de inicio\n",
    "    dados_treino['y_pred'] = \"-\"  # \"- \" para previsões no treino, nesse ponto elas não exitem. (preenche com valor padão que sera substituido)\n",
    "\n",
    "\n",
    "    # Extrai as previsões in-sample do modelo para o conjunto de treino\n",
    "    insample_df = model.predict_insample(step_size=h)\n",
    "    # Inversão da transformação log1p\n",
    "    insample_df['y'] = np.expm1(insample_df['y'])\n",
    "    insample_df['LSTM'] = np.expm1(insample_df['LSTM'])\n",
    "    # Filtrar para garantir contexto suficiente\n",
    "    contexto =  params_extraidos['input_size'] # verifica o tanho da janela de contexto.\n",
    "    # remove os primeiros registros que não tem contexto suficiente para ter uma previsão válida.\n",
    "    min_cutoff_valido = pd.to_datetime(insample_df['cutoff'].min()) + pd.DateOffset(years=contexto) \n",
    "    insample_df = insample_df[insample_df['cutoff'] >= min_cutoff_valido].copy()\n",
    "\n",
    "    insample_preds = insample_df[['unique_id', 'ds', 'LSTM']].copy()\n",
    "    insample_preds = insample_preds.rename(columns={'LSTM': 'y_pred_temp'})\n",
    "    # Faz o merge dos dados de treino com as previsões disponíveis no insample_df\n",
    "    dados_treino = dados_treino.merge(insample_preds, on=['unique_id', 'ds'], how='left')\n",
    "    # Atualiza a coluna y_pred: onde tiver previsão do insample_df usa ela, senão mantém o \"-\"\n",
    "    dados_treino['y_pred'] = dados_treino['y_pred_temp'].combine_first(dados_treino['y_pred'])\n",
    "    # Remove a coluna temporária\n",
    "    dados_treino = dados_treino.drop(columns=['y_pred_temp'])\n",
    "\n",
    "    dados_treino['diferença_%'] = \"-\"  # \"-\" para a diferença percentual entre predito e real, nesse ponto elas não exitem.\n",
    "    dados_treino['flag'] = 'treino'\n",
    "    dados_treino['ds'] = (pd.to_datetime(dados_treino['ds'], errors='coerce').fillna(pd.Timestamp.now()).dt.strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "\n",
    "    dados_treino['diferença_%'] = dados_treino.apply(calcula_diferenca_pct, axis=1)\n",
    "    dados_treino['flag'] = 'treino'\n",
    "    dados_treino['ds'] = (pd.to_datetime(dados_treino['ds'], errors='coerce').fillna(pd.Timestamp.now()).dt.strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "\n",
    "    # Prepara o dataframe final unindo treino, validação e teste\n",
    "    colunas = [\n",
    "        'treino_id',    # Identificador do treino\n",
    "        'unique_id',    # Identificador de cada série\n",
    "        'ds',           # Data de cada registro\n",
    "        'y',            # Valor real\n",
    "        'y_pred',       # Valor predito pelo modelo\n",
    "        'diferença_%',  # Valor da diferença percentual entre o valor predito e o real\n",
    "        'flag',         # Indica se foi usado em treino, validação ou teste\n",
    "        'dataset',      # Nome do dataset usado\n",
    "        'modelo',       # Nome do algoritmo (LSTM, Random Forest, etc)\n",
    "        'comentario',   # Anotações adicionais (pode ser vazio)\n",
    "        'data_treino'   # Data que o modelo foi treinado\n",
    "    ]\n",
    "\n",
    "    # Verificar existência do arquivo\n",
    "    # Se o dataframe com os dados de previsão não existir, ele será criado\n",
    "    if not os.path.exists(f\"{csv_dir}\"):\n",
    "        # Criar DataFrame vazio com as colunas especificadas\n",
    "        df = pd.DataFrame(columns=colunas)\n",
    "\n",
    "        # Salvar o DataFrame vazio como CSV\n",
    "        df.to_csv(f\"{csv_dir}\", index=False)\n",
    "        print(f\"Arquivo {csv_dir} criado com DataFrame vazio.\")\n",
    "    else:\n",
    "        df = pd.read_csv(f\"{csv_dir}\")\n",
    "        df['ds'] = (pd.to_datetime(df['ds'], errors='coerce').fillna(pd.Timestamp.now()).dt.strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "        print(f\"Arquivo {csv_dir} já existe. Nenhuma ação necessária.\")\n",
    "\n",
    "    \n",
    "    # Concatena os dados de treino, validação e teste em um único DataFrame. Considerando a posibilidade de val_ds ser None\n",
    "    if val_ds is not None:\n",
    "        # Concatena os dados de treino, validação e teste\n",
    "        dados_completos = pd.concat([dados_treino, dados_validacao, dados_teste], ignore_index=True)\n",
    "    else:\n",
    "        # Concatena apenas os dados de treino e teste\n",
    "        dados_completos = pd.concat([dados_treino, dados_teste], ignore_index=True)\n",
    "    \n",
    "\n",
    "    dados_completos['treino_id'] = local  # Identificador do treino\n",
    "    dados_completos['dataset'] = nome_dataset  # Nome do dataset usado\n",
    "    dados_completos['modelo'] = \"LSTM\"\n",
    "    dados_completos['data_treino'] = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "    dados_completos['comentario'] =f\"\"\"{local}\n",
    "    Modelo LSTM treinado com dados de {train_ds['ds'].dt.year.min()} a {train_ds['ds'].dt.year.max()}, validado com os dados de {val_ds['ds'].dt.year.min()} e testado com os dados de {test_ds['ds'].dt.year.min()}.\n",
    "    Para esse treinamento foi utilizado o dataset {nome_dataset}.\n",
    "    {comentario}\n",
    "    O modelo foi treinado com as seguintes configurações:\n",
    "    input_size: {params_extraidos['input_size']}\n",
    "\n",
    "    encoder_n_layers = {params_extraidos['encoder_n_layers']}\n",
    "    learning_rate: {params_extraidos['learning_rate']}\n",
    "    encoder_hidden_size: {params_extraidos['encoder_hidden_size']}\n",
    "    decoder_layers: {params_extraidos['decoder_layers']}\n",
    "    decoder_hidden_size: {params_extraidos['decoder_hidden_size']}\n",
    "    batch_size: {params_extraidos['batch_size']}\n",
    "    dropout: {params_extraidos['dropout']}\n",
    "    weight_decay: {params_extraidos['weight_decay']}\n",
    "    steps: {params_extraidos['steps']}\n",
    "    \"\"\"\n",
    "\n",
    "    # Reordenar as colunas\n",
    "    nova_ordem_colunas = ['treino_id', 'unique_id', 'ds', 'y', 'y_pred', 'diferença_%', 'flag', 'dataset', 'modelo', 'comentario', 'data_treino']\n",
    "    dados_completos = dados_completos[nova_ordem_colunas]\n",
    "    # Inclui as novas previsões no arquivo existente\n",
    "    df_final = pd.concat([df, dados_completos], ignore_index=True)\n",
    "    # Salva o DataFrame atualizado de volta ao CSV\n",
    "    df_final.to_csv(f\"{csv_dir}\", index=False)\n",
    "    clear_output()\n",
    "    print(string_saidas)\n",
    "    print(f\"Previsões salvas no arquivo {csv_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fc97131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a diferença percentual absoluta entre previsão e valor real, ela é usada durante para gerar o dataframe com as previsões.\n",
    "# E como o dataframe de previsões inclui os dados de teste e validação, ela precisa lidar com o periodo de treino que não tem previsão,\n",
    "# os anos referentes a janela de contexto e nesse caso retorna \"-\".\n",
    "def calcula_diferenca_pct(row):\n",
    "    if row['y_pred'] == \"-\" or row['y_pred'] is None:\n",
    "        return \"-\"\n",
    "    try:\n",
    "        y_pred_float = float(row['y_pred'])\n",
    "        y_real = float(row['y'])\n",
    "        # Evita divisão por zero\n",
    "        if y_real == 0:\n",
    "            return \"-\"\n",
    "        # Calcula diferença percentual absoluta\n",
    "        diff_pct = abs((y_pred_float - y_real) / y_real) * 100\n",
    "        return round(diff_pct, 2)\n",
    "    except:\n",
    "        return \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a94a19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters_from_yaml(yaml_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Lê um arquivo hparams.yaml, extrai um conjunto específico de hiperparâmetros\n",
    "    e os retorna em um dicionário.\n",
    "\n",
    "    Esta função lida com os arquivos hparams.yaml gerados pelo PyTorch Lightning,\n",
    "    que podem conter tags de objetos Python e requerem o uso de 'UnsafeLoader'.\n",
    "\n",
    "    Args:\n",
    "        yaml_path (str): O caminho para o arquivo hparams.yaml.\n",
    "\n",
    "    Returns:\n",
    "        dict: Um dicionário contendo os hiperparâmetros extraídos.\n",
    "              Ex: {'learning_rate': 0.001, 'batch_size': 32, ...}\n",
    "        \n",
    "        None: Retorna None se o arquivo não for encontrado, se houver um erro de\n",
    "              leitura do YAML ou se uma chave esperada não for encontrada.\n",
    "    \"\"\"\n",
    "    # Tenta ler o arquivo YAML\n",
    "    try:\n",
    "        with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "            config_data = yaml.load(f, Loader=yaml.UnsafeLoader)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERRO: Arquivo não encontrado no caminho: {yaml_path}\")\n",
    "        return None\n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"ERRO: Ocorreu um problema ao processar o arquivo YAML: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Tenta extrair os parâmetros específicos\n",
    "    try:\n",
    "        hyperparameters = {\n",
    "            'encoder_n_layers': config_data['encoder_n_layers'],\n",
    "            'learning_rate': config_data['learning_rate'],\n",
    "            'input_size': config_data['input_size'],\n",
    "            'encoder_hidden_size': config_data['encoder_hidden_size'],\n",
    "            'decoder_layers': config_data['decoder_layers'],\n",
    "            'decoder_hidden_size': config_data['decoder_hidden_size'],\n",
    "            'batch_size': config_data['batch_size'],\n",
    "            'dropout': config_data['encoder_dropout'],  # Mapeado de 'encoder_dropout'\n",
    "            'weight_decay': config_data['optimizer_kwargs']['weight_decay'], # Mapeado de um dicionário aninhado\n",
    "            'steps': config_data['max_steps'] # Mapeado de 'max_steps'\n",
    "        }\n",
    "        return hyperparameters\n",
    "    except KeyError as e:\n",
    "        print(f\"ERRO: A chave de hiperparâmetro esperada {e} não foi encontrada no arquivo.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66e6d8a",
   "metadata": {},
   "source": [
    "### Resumo das Métricas WMAPE\n",
    "\n",
    "O objetivo de cada função é avaliar o desempenho do seu modelo de previsão sob uma ótativa diferente.\n",
    "\n",
    "#### 1. Média Simples dos WMAPEs (`calcular_media_wmape_simples`)\n",
    "\n",
    "* **Como Funciona:**\n",
    "    1.  O modelo olha para cada município de forma isolada.\n",
    "    2.  Calcula o erro percentual (WMAPE) apenas para aquele município.\n",
    "    3.  Depois de fazer isso para todos os municípios, ele simplesmente tira a **média aritmética** de todos esses erros individuais.\n",
    "\n",
    "* **Principal Objetivo:** Responder à pergunta: **\"Em média, qual é o desempenho do meu modelo em cada localidade, tratando todas como igualmente importantes?\"**\n",
    "    * É uma visão \"democrática\". Um erro de 20% em um município pequeno tem o mesmo peso na nota final que um erro de 20% em um município gigante. Use esta métrica para garantir que seu modelo seja consistente e não tenha um desempenho muito ruim em localidades específicas, independentemente do tamanho delas.\n",
    "\n",
    "#### 2. WMAPE da Soma Total ou Agregado (`calcular_wmape_agregado`)\n",
    "\n",
    "* **Como Funciona:**\n",
    "    1.  Primeiro, ele ignora as divisões por município.\n",
    "    2.  Soma **toda a produção real** de todos os municípios para obter a safra total real do estado.\n",
    "    3.  Soma **toda a produção prevista** de todos os municípios para obter a safra total prevista.\n",
    "    4.  Calcula um único WMAPE comparando esses dois totais (Total Real vs. Total Previsto).\n",
    "\n",
    "* **Principal Objetivo:** Responder à pergunta: **\"No final, o quão perto eu cheguei de acertar a safra TOTAL do estado de Minas Gerais?\"**\n",
    "    * Esta é a visão do \"resultado final\". Ela foca no número mais importante para o negócio. Esta métrica permite que erros se cancelem (prever a mais em um município pode compensar prever a menos em outro), refletindo a precisão da previsão no nível mais alto.\n",
    "\n",
    "#### 3. Média Ponderada dos WMAPEs (`calcular_wmape_ponderado`)\n",
    "\n",
    "* **Como Funciona:**\n",
    "    1.  Começa como a primeira métrica: calcula o erro (WMAPE) individualmente para cada município.\n",
    "    2.  Porém, antes de calcular a média final, ele atribui um **\"peso de importância\"** a cada município, baseado no seu volume de produção.\n",
    "    3.  A nota final é uma **média ponderada**, onde os erros dos municípios que produzem mais têm um impacto muito maior no resultado.\n",
    "\n",
    "* **Principal Objetivo:** Responder à pergunta: **\"Qual é o desempenho médio do meu modelo, considerando que um erro nos municípios grandes é muito mais prejudicial que um erro nos pequenos?\"**\n",
    "    * É uma visão de \"impacto no negócio\". Ela mede a performance média, mas de uma forma que reflete a importância econômica de cada localidade. É um excelente meio-termo entre a visão puramente local (Métrica 1) и a visão puramente agregada (Métrica 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66eff7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Função base (helper) que calcula o WMAPE para um par de vetores\n",
    "# -----------------------------------------------------------------------------\n",
    "def wmape_base(actual: np.ndarray, predicted: np.ndarray) -> float:\n",
    "    \"\"\"Calcula o WMAPE para um conjunto de valores reais e previstos.\"\"\"\n",
    "    # Garante que não haverá divisão por zero. Se a soma real for 0, o erro é 0.\n",
    "    sum_actual = np.sum(np.abs(actual))\n",
    "    if sum_actual == 0:\n",
    "        return 0.0\n",
    "    return np.sum(np.abs(predicted - actual)) / sum_actual\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Métrica 1: Média Simples dos WMAPEs por Município\n",
    "# -----------------------------------------------------------------------------\n",
    "def calcular_media_wmape_simples(df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calcula o WMAPE para cada município (unique_id) individualmente e\n",
    "    retorna a média simples desses WMAPEs.\n",
    "    Trata todos os municípios como igualmente importantes.\n",
    "    \"\"\"\n",
    "    # Agrupa por município e calcula o WMAPE para cada um\n",
    "    wmapes_individuais = df.groupby('unique_id')[['y', 'LSTM']].apply(\n",
    "        lambda g: wmape_base(g['y'].values, g['LSTM'].values)\n",
    "    )\n",
    "    \n",
    "    # Retorna a média dos WMAPEs calculados\n",
    "    return wmapes_individuais.mean()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Métrica 2: WMAPE da Soma Total (Agregado) - O que você já usa\n",
    "# -----------------------------------------------------------------------------\n",
    "def calcular_wmape_agregado(df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Soma todas as previsões e todos os valores reais do dataframe inteiro\n",
    "    e calcula um único WMAPE sobre esses totais.\n",
    "    Foca na acurácia da previsão agregada (total do estado).\n",
    "    \"\"\"\n",
    "    actual_total = df['y'].values\n",
    "    predicted_total = df['LSTM'].values\n",
    "    \n",
    "    return wmape_base(actual_total, predicted_total)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Métrica 3: Média Ponderada dos WMAPEs por Município\n",
    "# -----------------------------------------------------------------------------\n",
    "def calcular_wmape_ponderado(df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calcula o WMAPE para cada município e depois faz uma média ponderada,\n",
    "    onde o peso de cada município é sua participação na produção total.\n",
    "    \"\"\"\n",
    "    # 1. Calcula o WMAPE para cada município\n",
    "    wmapes_individuais = df.groupby('unique_id')[['y', 'LSTM']].apply(\n",
    "        lambda g: wmape_base(g['y'].values, g['LSTM'].values)\n",
    "    )\n",
    "    \n",
    "    # 2. Calcula o peso de cada município (sua produção total / produção geral)\n",
    "    producao_por_municipio = df.groupby('unique_id')['y'].sum()\n",
    "    producao_total = df['y'].sum()\n",
    "    \n",
    "    # Evita divisão por zero se a produção total for 0\n",
    "    if producao_total == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    pesos = producao_por_municipio / producao_total\n",
    "    \n",
    "    # 3. Multiplica os WMAPEs pelos pesos e soma o resultado\n",
    "    wmape_final_ponderado = np.sum(wmapes_individuais * pesos)\n",
    "    \n",
    "    return wmape_final_ponderado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22c5dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_simple_forecast(\n",
    "    model,\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    model_name: str = 'LSTM',  \n",
    "    string_saidas: str = \"\",\n",
    "    inteiro: bool = False,\n",
    "    log_transformed: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Executa avaliação de previsão considerando o nome do modelo dinâmico.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Limpeza e Logs (conforme seu original)\n",
    "    if string_saidas:\n",
    "        clear_output()\n",
    "        print(string_saidas)\n",
    "\n",
    "    print(f\"Iniciando a previsão com o modelo: {model_name}...\")\n",
    "\n",
    "    # --- 1. Preparação dos Dados ---\n",
    "    history_df = train_df.copy()\n",
    "    future_data_to_evaluate = test_df.copy()\n",
    "\n",
    "    # Conversão de datas\n",
    "    history_df['ds'] = pd.to_datetime(history_df['ds'])\n",
    "    future_data_to_evaluate['ds'] = pd.to_datetime(future_data_to_evaluate['ds'])\n",
    "\n",
    "    # Filtra histórico para evitar vazamento de dados\n",
    "    cutoff = future_data_to_evaluate['ds'].min()\n",
    "    history_df = history_df[history_df['ds'] < cutoff]\n",
    "  \n",
    "    # Prepara df futuro para predict (sem y)\n",
    "    futr_df = future_data_to_evaluate.drop(columns=[\"y\"], errors='ignore')\n",
    "\n",
    "    # --- 2. Geração da Previsão ---\n",
    "    forecasts_df = model.predict(\n",
    "        df=history_df,\n",
    "        futr_df=futr_df\n",
    "    )\n",
    "    \n",
    "    # Verificação de segurança: A coluna do modelo existe?\n",
    "    if model_name not in forecasts_df.columns:\n",
    "        # Tenta fallback inteligente ou erro\n",
    "        cols_disponiveis = [c for c in forecasts_df.columns if c not in ['unique_id', 'ds']]\n",
    "        raise ValueError(f\"A coluna '{model_name}' não foi encontrada na previsão. Colunas disponíveis: {cols_disponiveis}\")\n",
    "\n",
    "    print(\"Previsão concluída. Combinando com dados reais...\")\n",
    "\n",
    "    # --- 3. Pós-processamento e Avaliação ---\n",
    "    evaluation_df = forecasts_df.merge(\n",
    "        future_data_to_evaluate[[\"unique_id\", \"ds\", \"y\"]],\n",
    "        on=[\"unique_id\", \"ds\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Tratamento de Log (Reversão)\n",
    "    if log_transformed:\n",
    "        evaluation_df['y'] = np.expm1(evaluation_df['y'])\n",
    "        evaluation_df[model_name] = np.expm1(evaluation_df[model_name]) # Usa model_name\n",
    "\n",
    "    # Arredondamento\n",
    "    if inteiro:\n",
    "        evaluation_df[model_name] = evaluation_df[model_name].round().astype(int)\n",
    "        evaluation_df['y'] = evaluation_df['y'].round().astype(int)\n",
    "\n",
    "    # Cálculo da Diferença Percentual (passando o nome da coluna)\n",
    "    evaluation_df = calcular_diferenca_percentual(evaluation_df, col_pred=model_name)\n",
    "\n",
    "    print(\"Avaliação finalizada.\")\n",
    "    return evaluation_df\n",
    "\n",
    "\n",
    "def calcular_diferenca_percentual(df: pd.DataFrame, col_pred: str = 'LSTM') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcula a diferença percentual dinamicamente baseada na coluna predita.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Fórmula: ((Predito - Real) / Real) * 100\n",
    "    # Adicionei tratamento para divisão por zero com np.where\n",
    "    df['diferença_%'] = np.where(\n",
    "        df['y'] == 0, \n",
    "        0,  # Se real for 0, define erro como 0 para evitar inf (ajuste conforme regra de negócio)\n",
    "        ((df[col_pred] - df['y']) / df['y']) * 100\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "794255c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# FÁBRICA DE CALLBACKS\n",
    "# Responsabilidade: Criar uma função de callback configurada com o caminho correto de onde salvar o modelo.\n",
    "# ===================================================================\n",
    "def create_save_best_model_callback(destination_path: Path):\n",
    "    \"\"\"\n",
    "    Esta função é uma \"fábrica\": ela recebe o caminho de destino e retorna\n",
    "    a função de callback que o Optuna vai usar.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Esta é a função que o Optuna realmente vai chamar.\n",
    "    # Note que ela tem acesso à variável 'destination_path' da função externa.\n",
    "    # ===================================================================\n",
    "    # FUNÇÃO CALLBACK (RESPONSABILIDADE: SALVAR O MELHOR MODELO)\n",
    "    # ===================================================================\n",
    "    def callback(study: optuna.study.Study, trial: optuna.trial.FrozenTrial):\n",
    "        \"\"\"\n",
    "        Callback que é acionado após cada trial para salvar o melhor modelo.\n",
    "        \"\"\"\n",
    "        if study.best_trial.number == trial.number:\n",
    "            print(f\"\\n✨ Novo melhor trial encontrado: #{trial.number} com Score: {trial.value:.4f}\")\n",
    "            \n",
    "            source_path_str = trial.user_attrs.get(\"artifact_path\")\n",
    "            prediction_df_dict = trial.user_attrs.get(\"prediction_df\")\n",
    "\n",
    "            if not source_path_str or not prediction_df_dict:\n",
    "                print(\"   -> AVISO: Não foi possível encontrar os artefatos para salvar.\")\n",
    "                return\n",
    "\n",
    "            source_path = Path(source_path_str)\n",
    "            # *** A MUDANÇA CRÍTICA ESTÁ AQUI ***\n",
    "            # Usa o caminho que foi \"injetado\" quando a função foi criada\n",
    "            final_destination = destination_path \n",
    "\n",
    "            print(f\"   -> Salvando artefatos de '{source_path}' para '{final_destination}'...\")\n",
    "\n",
    "            if final_destination.exists():\n",
    "                shutil.rmtree(final_destination)\n",
    "            final_destination.mkdir(parents=True, exist_ok=True) # Adicionado exist_ok=True por segurança\n",
    "\n",
    "            shutil.copytree(source_path, final_destination, dirs_exist_ok=True)\n",
    "            \n",
    "            prediction_df = pd.DataFrame(prediction_df_dict)\n",
    "            prediction_df.to_excel(final_destination / \"valores_predicao.xlsx\", index=False)\n",
    "            \n",
    "            print(\"   -> Modelo e artefatos salvos com sucesso!\")\n",
    "\n",
    "    # A fábrica retorna a função de callback configurada\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a726e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "def filtrar_datasets_por_integridade(\n",
    "    train_df: pd.DataFrame, \n",
    "    val_df: pd.DataFrame, \n",
    "    test_df: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Filtra os datasets de treino, validação e teste para manter apenas os\n",
    "    municípios ('unique_id') que possuem dados completos no período de validação.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): DataFrame de treino.\n",
    "        val_df (pd.DataFrame): DataFrame de validação. Usado como referência para a verificação.\n",
    "        test_df (pd.DataFrame): DataFrame de teste.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: Uma tupla contendo os \n",
    "        DataFrames de treino, validação e teste devidamente filtrados.\n",
    "    \"\"\"\n",
    "    print(\"--- Iniciando verificação de integridade dos dados ---\")\n",
    "    \n",
    "    # Usa o DataFrame de validação como referência para a integridade.\n",
    "    # 1. Conta o número de timestamps únicos que cada município DEVERIA ter.\n",
    "    timestamps_esperados = len(val_df['ds'].unique())\n",
    "    if timestamps_esperados == 0:\n",
    "        print(f\"{CoresTerminal.VERMELHO}ALERTA: O conjunto de validação está vazio. Nenhum filtro será aplicado.{CoresTerminal.FIM}\")\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    # 2. Conta quantos timestamps únicos cada município realmente POSSUI.\n",
    "    contagem_por_id = val_df.groupby('unique_id')['ds'].nunique()\n",
    "    \n",
    "    # 3. Identifica os municípios com dados completos.\n",
    "    ids_completos = contagem_por_id[contagem_por_id == timestamps_esperados].index\n",
    "    \n",
    "    # 4. Compara com o total de municípios para ver se a filtragem é necessária.\n",
    "    ids_originais = val_df['unique_id'].nunique()\n",
    "    \n",
    "    if len(ids_completos) < ids_originais:\n",
    "        total_removido = ids_originais - len(ids_completos)\n",
    "        \n",
    "        print(f\"{CoresTerminal.VERMELHO}\"\n",
    "              \"----------------------------------------------------------------------\\n\"\n",
    "              \"ALERTA: Inconsistência de dados encontrada.\\n\"\n",
    "              f\"Foram encontrados {total_removido} de {ids_originais} municípios com dados INCOMPLETOS no período de validação.\\n\"\n",
    "              \"Todos os datasets serão filtrados para manter apenas os municípios com dados completos.\\n\"\n",
    "              f\"----------------------------------------------------------------------{CoresTerminal.FIM}\")\n",
    "        \n",
    "        # Filtra TODOS os dataframes para manter apenas os municípios com dados completos.\n",
    "        # O uso de .copy() evita o SettingWithCopyWarning do pandas.\n",
    "        train_filtrado = train_df[train_df['unique_id'].isin(ids_completos)].copy()\n",
    "        val_filtrado = val_df[val_df['unique_id'].isin(ids_completos)].copy()\n",
    "        test_filtrado = test_df[test_df['unique_id'].isin(ids_completos)].copy()\n",
    "        \n",
    "        # Se após a filtragem não sobrar nenhum dado, interrompe a execução.\n",
    "        if val_filtrado.empty:\n",
    "            print(f\"{CoresTerminal.VERMELHO}ALERTA: Após a filtragem, não restaram dados válidos. Retornando DataFrames vazios.{CoresTerminal.FIM}\")\n",
    "            return train_filtrado, val_filtrado, test_filtrado\n",
    "        \n",
    "        print(f\"Filtragem concluída. {len(ids_completos)} municípios mantidos.\")\n",
    "        return train_filtrado, val_filtrado, test_filtrado\n",
    "        \n",
    "    else:\n",
    "        print(f\"{CoresTerminal.VERDE}Verificação concluída. Todos os {ids_originais} municípios possuem dados completos no período de validação.{CoresTerminal.FIM}\")\n",
    "        return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01280f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_file):\n",
    "    \"\"\"\n",
    "    Carrega, processa e limpa o dataset, garantindo que todas as séries temporais\n",
    "    para cada 'unique_id' estejam completas no intervalo de datas do dataset.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(dataset_file)\n",
    "\n",
    "    # --- Aplica transformação logarítmica ---\n",
    "    log_cols = [\n",
    "        'Área colhida (Hectares)',\n",
    "        'target',\n",
    "        'precomediocafe', # a partir de novembro 2025\n",
    "        'Área destinada à colheita (Hectares)'\n",
    "    ]\n",
    "    existing_log_cols = [col for col in log_cols if col in dataset.columns]\n",
    "    if existing_log_cols:\n",
    "        dataset[existing_log_cols] = dataset[existing_log_cols].apply(lambda x: np.log1p(x))\n",
    "\n",
    "    # --- Renomeação e formatação para o padrão NeuralForecast ---\n",
    "    dataset = dataset.rename(columns={\n",
    "        \"municipio\": \"unique_id\",\n",
    "        \"ano\": \"ds\",\n",
    "        \"target\": \"y\"\n",
    "    })\n",
    "    # Ordenar por unique_id e ds (ano)\n",
    "    dataset = dataset.sort_values(by=[\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "    dataset['ds'] = pd.to_datetime(dataset['ds'].astype(str) + '-12-31')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ab2cb",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ded4b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"Dataset/V38/2020\"\n",
    "\n",
    "Local_treino =  \"Altitude_Dataset_Unico_Ajuste\" # Nome do arquivo final com todas as predições\n",
    "nome_dataset = \"V38\"\n",
    "intervalo_validacao = [\n",
    "    [2019, 2019] # ano de inicio e fim da validação\n",
    "]\n",
    "num_trials = 150  # Número de trials para cada execução do Optuna\n",
    "h = 1 # horizonte de previsão\n",
    "\n",
    "janela_contexto = None # Tamanho da janela de contexto (input_size) / Use None para usar o máximo possível\n",
    "\n",
    "comentario = \"\"\"Estou usando o dataset V38 para o ano de 2020 para fazer o ajuste de hiperparametros.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19502439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de configurações iniciais (cada dicionário é um conjunto de parâmetros)\n",
    "trials_iniciais = [\n",
    "    {\n",
    "        'learning_rate': 0.00014552082561940832,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1e7dc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teste do Modelo: Treinos/Altitude_Dataset_Unico_Ajuste/Modelos/V38_(2020)\n",
      "Previsões salvas no arquivo Treinos/Altitude_Dataset_Unico_Ajuste/Altitude_Dataset_Unico_Ajuste.csv\n",
      "Limpando arquivos temporários em: Logs e temp_models\n",
      "V38_(2020)\n",
      "\n",
      ")\n",
      "Hiperparâmetros do melhor modelo salvo:\n",
      " - encoder_n_layers: 4\n",
      " - learning_rate: 0.0010330837083315822\n",
      " - input_size: 6\n",
      " - encoder_hidden_size: 192\n",
      " - decoder_layers: 1\n",
      " - decoder_hidden_size: 64\n",
      " - batch_size: 32\n",
      " - dropout: 0.3\n",
      " - weight_decay: 0.0001\n",
      " - steps: 1000\n"
     ]
    }
   ],
   "source": [
    "for dataset_file in os.listdir(dataset_path):\n",
    "    treino_id = dataset_file[:-4]\n",
    "    print(treino_id)\n",
    "    dataset = get_dataset(f\"{dataset_path}/{dataset_file}\")\n",
    "\n",
    "    # coleta a lista variaveis exogenas\n",
    "    exog_list = [col for col in dataset.columns.tolist() if col not in [\"ds\", \"y\", \"unique_id\"]]\n",
    "    \n",
    "    for ano_val in intervalo_validacao: # Para cada ano que sera usado como validação\n",
    "        # --- Preparação do dataset de treino, validação e teste ---\n",
    "        ano_teste = ano_val[1] + h # Define o ano de teste com base no horizonte. \n",
    "\n",
    "        train_ds = dataset[dataset['ds'].dt.year < ano_val[0]].copy()\n",
    "        val_ds = dataset[(dataset['ds'].dt.year >= ano_val[0]) & (dataset['ds'].dt.year < ano_teste)].copy()\n",
    "        if h == 1:\n",
    "            test_ds = dataset[dataset['ds'].dt.year == ano_teste].copy()\n",
    "        else:\n",
    "            # O calculo é ano_teste+h-1, pois se ano_teste= 2024, h=2, daria 2024+2=2026, mas o correto é 2024 e 2025, ou seja, 2024+2-1=2025.\n",
    "            test_ds = dataset[(dataset['ds'].dt.year >= ano_teste) & (dataset['ds'].dt.year <= ano_teste+h-1)].copy()\n",
    "        # Aplica a função de filtragem para garantir a consistência\n",
    "        train_ds, val_ds, test_ds = filtrar_datasets_por_integridade(\n",
    "            train_df=train_ds,\n",
    "            val_df=val_ds,\n",
    "            test_df=test_ds\n",
    "        )\n",
    "        print(f\"Dados de Treino entre os anos de {train_ds['ds'].dt.year.min()} e {train_ds['ds'].dt.year.max()}: {len(train_ds)} registros\")\n",
    "        print(f\"Dados de Validação entre os anos de {val_ds['ds'].dt.year.min() if val_ds is not None else 'N/A'} e {val_ds['ds'].dt.year.max() if val_ds is not None else 'N/A'}: {len(val_ds) if val_ds is not None else 0} registros\")\n",
    "        print(f\"Dados de Teste entre os anos de {test_ds['ds'].dt.year.min()} e {test_ds['ds'].dt.year.max()}: {len(test_ds)} registros\") \n",
    "        # --- Configuração das Pastas ---\n",
    "        # Pasta final para o melhor modelo\n",
    "        base_dir = Path(f\"./Treinos/{Local_treino}/Modelos/{treino_id}_({ano_teste})\") \n",
    "        csv_dir = Path(f\"./Treinos/{Local_treino}/{Local_treino}.csv\")\n",
    "        base_dir.mkdir(parents=True, exist_ok=True)\n",
    "        best_model_callback = create_save_best_model_callback(base_dir) # define a pasta correta para a função de callback\n",
    "\n",
    "        # Pasta para logs de cada trial (será gerenciada dentro da objective)\n",
    "        log_dir_base = Path(\"./Logs\")\n",
    "\n",
    "        # Pasta temporária para salvar o modelo de CADA trial\n",
    "        temp_model_dir = Path(\"./temp_models\")\n",
    "        temp_model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # --- Configuração do treinamento ---\n",
    "        # define a janela de contexto (input_size)\n",
    "        if janela_contexto is not None:\n",
    "            input_size = janela_contexto # Usa valor fixo definido no início\n",
    "        else:\n",
    "            input_size = train_ds['ds'].nunique() - h  # tamanho da janela de entrada (número de períodos anteriores usados para prever o próximo)\n",
    "\n",
    "        # --- Otimização de Hiperparâmetros com Optuna ---\n",
    "        optuna_db_dir = Path(f\"./Treinos/{Local_treino}/optuna_db\")\n",
    "        optuna_db_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Usa o ID do treino e o ano de teste para garantir que cada loop tenha seu próprio DB exclusivo\n",
    "        study_name = f\"{treino_id}_{ano_teste}\"\n",
    "\n",
    "        # Define o caminho do arquivo .db\n",
    "        db_file = optuna_db_dir / f\"{study_name}.db\"\n",
    "        \n",
    "        # Cria a URL de conexão para o SQLite (necessário converter Path para string)\n",
    "        storage_url = f\"sqlite:///{db_file}\"\n",
    "\n",
    "        print(f\"Armazenando histórico do Optuna em: {storage_url}\")\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            study_name=study_name,\n",
    "            storage=storage_url,\n",
    "            direction=\"minimize\",\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        print(f\"Treinamento do Modelo: {treino_id}_({ano_teste})\")\n",
    "        # Enfileirar trials iniciais para serem avaliados primeiro\n",
    "        for params in trials_iniciais:\n",
    "            study.enqueue_trial(params)\n",
    "            \n",
    "        try:\n",
    "            study.optimize(\n",
    "                objective,\n",
    "                n_trials=num_trials, \n",
    "                n_jobs=1,\n",
    "                callbacks=[best_model_callback] \n",
    "            )\n",
    "\n",
    "            # --- Teste do melhor modelo salvo ---\n",
    "            teste_modelo(base_dir, csv_dir, dataset, train_ds, val_ds, test_ds, comentario, nome_dataset, h=h)\n",
    "        finally:\n",
    "            # --- LIMPEZA ---\n",
    "            # Garante que a pasta temporária dentro de Logs seja apagada ao final, mesmo que ocorra algum erro durante a otimização.\n",
    "            if temp_model_dir.exists():\n",
    "                print(f\"Limpando arquivos temporários em: {log_dir_base} e {temp_model_dir}\")\n",
    "                shutil.rmtree(temp_model_dir, ignore_errors=True)\n",
    "                shutil.rmtree(log_dir_base, ignore_errors=True)  \n",
    "\n",
    "            treino_path = Path(f\"./Treinos/{Local_treino}/Modelos\")\n",
    "            for modelo in os.listdir(treino_path):\n",
    "                print(f\"{modelo}\\n\\n)\")\n",
    "                path_para_hparams = f\"{treino_path}/{modelo}/hparams.yaml\" \n",
    "                params_extraidos = get_hyperparameters_from_yaml(path_para_hparams)\n",
    "                print(\"Hiperparâmetros do melhor modelo salvo:\")\n",
    "                for chave, valor in params_extraidos.items():\n",
    "                    print(f\" - {chave}: {valor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Educampo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
