{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l1FdbaKkRX7_",
        "4uO1xYpxROHM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Grafico de Produção em sacas"
      ],
      "metadata": {
        "id": "uh_y4Bg9ZcNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funções"
      ],
      "metadata": {
        "id": "l1FdbaKkRX7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import logging\n",
        "from functools import reduce\n",
        "import textwrap\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Get the current date\n",
        "current_date = datetime.now()\n",
        "\n",
        "\n",
        "# Configurando o logging para exibir mensagens de debug\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def _format_comment_with_breaks(text, line_length=77):\n",
        "    \"\"\"\n",
        "    Formata uma string de comentário com quebras de linha HTML\n",
        "    para largura consistente no tooltip.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"Nenhum comentário disponível.\"\n",
        "    # textwrap.wrap retorna uma lista de linhas, unidas pela tag <br>\n",
        "    return '<br>'.join(textwrap.wrap(text, width=line_length))"
      ],
      "metadata": {
        "id": "R31oQb8PRwuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Funções de Processamento de Dados (Parte 1) ---\n",
        "def _ler_e_processar_modelo_csv(arquivo, nome_legenda, base_dados):\n",
        "    \"\"\"\n",
        "    Lê um único arquivo CSV de modelo, calcula a produção total e retorna um\n",
        "    DataFrame agregado e o comentário do modelo.\n",
        "\n",
        "    Esta função foi otimizada para usar merges de Pandas em vez de iterrows.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(f\"Processando arquivo: {arquivo} como '{nome_legenda}'\")\n",
        "        df_modelo = pd.read_csv(arquivo)\n",
        "\n",
        "        # 1. Extrair e formatar comentário\n",
        "        comentario = \"Nenhum comentário disponível.\"\n",
        "        if 'comentario' in df_modelo.columns and not df_modelo['comentario'].dropna().empty:\n",
        "            comentario = df_modelo['comentario'].dropna().iloc[0]\n",
        "        comentario_formatado = _format_comment_with_breaks(comentario)\n",
        "\n",
        "        # 2. Preparar dados do modelo\n",
        "        df_modelo['ds'] = pd.to_datetime(df_modelo['ds']) # converte a data para o formato datetime\n",
        "        df_modelo['y_pred'] = pd.to_numeric(df_modelo['y_pred'], errors='coerce') # Converte o valores preditos para numerico, caso estejam em outro formato como string ou object\n",
        "        df_teste = df_modelo[df_modelo['flag'] == \"teste\"].copy() # Filtra para ter apenas os dados do periodo de teste\n",
        "\n",
        "        if df_teste.empty:\n",
        "            logging.warning(f\"Nenhum dado de 'teste' encontrado em {arquivo}.\")\n",
        "            return None, None, comentario_formatado\n",
        "\n",
        "        df_teste['Ano'] = df_teste['ds'].dt.year # Converte a data para ter apenas o ano.\n",
        "        df_teste.rename(columns={'unique_id': 'Municipio'}, inplace=True)\n",
        "\n",
        "        # 3. Merge com base_dados para obter área colhida\n",
        "        df_merged = pd.merge(\n",
        "            df_teste[['treino_id', 'Municipio', 'Ano', 'y', 'y_pred']],\n",
        "            base_dados,\n",
        "            on=['Municipio', 'Ano'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # 4. Calcular produção em sacas (evitando iteração)\n",
        "        saca_kg = 60\n",
        "        area_hectares = df_merged['Área colhida (Hectares)']\n",
        "\n",
        "        # Coluna do modelo (Estimado)\n",
        "        df_merged[nome_legenda] = (df_merged['y_pred'] * area_hectares * 1000) / saca_kg\n",
        "\n",
        "        # Coluna Real (IBGE)\n",
        "        nome_coluna_real = \"Safra Real (IBGE)\"\n",
        "        df_merged[nome_coluna_real] = (df_merged['y'] * area_hectares * 1000) / saca_kg\n",
        "\n",
        "        df_merged = df_merged.dropna(subset=[nome_legenda, nome_coluna_real, 'Área colhida (Hectares)'])\n",
        "\n",
        "        # 5. Agregar por Ano\n",
        "        df_agg = df_merged.groupby('Ano')[[nome_legenda, nome_coluna_real]].sum().reset_index()\n",
        "\n",
        "        # Arredondar valores\n",
        "        df_agg[nome_legenda] = df_agg[nome_legenda].round(2)\n",
        "        df_agg[nome_coluna_real] = df_agg[nome_coluna_real].round(2)\n",
        "\n",
        "        logging.info(f\"Arquivo {arquivo} processado com sucesso.\")\n",
        "\n",
        "        # Retorna o DataFrame agregado e o DataFrame real separado\n",
        "        df_estimado = df_agg[['Ano', nome_legenda]]\n",
        "        df_real = df_agg[['Ano', nome_coluna_real]]\n",
        "\n",
        "        return df_estimado, df_real, comentario_formatado\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"Erro: O arquivo {arquivo} não foi encontrado.\")\n",
        "        return None, None, None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Ocorreu um erro ao processar o arquivo {arquivo}: {e}\")\n",
        "        return None, None, None"
      ],
      "metadata": {
        "id": "xvIZK_v8R2lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _preparar_dados_conab(df_conab_raw):\n",
        "    \"\"\"\n",
        "    Formata o DataFrame da CONAB para o merge.\n",
        "    \"\"\"\n",
        "    logging.info(\"Preparando os dados da CONAB.\")\n",
        "    nome_conab = \"Estimativa CONAB\"\n",
        "    df_conab_est = df_conab_raw.T[['ESTIMADO']].rename(columns={'ESTIMADO': nome_conab})\n",
        "    df_conab_est.index.name = 'Ano'\n",
        "    df_conab_est.index = df_conab_est.index.astype(int)\n",
        "    df_conab_est[nome_conab] *= 1000 # Convertendo de mil para unidades\n",
        "    return df_conab_est, nome_conab\n",
        "\n",
        "def _calcular_diferencas_percentuais(df, colunas_modelos, col_base='Safra Real (IBGE)'):\n",
        "    \"\"\"\n",
        "    Calcula a diferença percentual de cada modelo em relação à coluna base (IBGE).\n",
        "    \"\"\"\n",
        "    logging.info(\"Calculando as diferenças percentuais.\")\n",
        "    for col_estimada in colunas_modelos:\n",
        "        if col_base in df and col_estimada in df:\n",
        "            nome_diff = f\"Diferença % ({col_estimada})\"\n",
        "            df[nome_diff] = ((df[col_estimada] - df[col_base]) / df[col_base]) * 100\n",
        "        else:\n",
        "            logging.warning(f\"Coluna '{col_estimada}' ou '{col_base}' não encontrada para cálculo de diferença.\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "o6Xwkn30R6me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def processar_dados_para_csv(lista_arquivos_modelos, df_conab_raw, base_dados,\n",
        "                           output_csv_path=\"dados_grafico_formatado.csv\",\n",
        "                           output_json_path=\"config_grafico.json\"):\n",
        "    \"\"\"\n",
        "    Orquestra a leitura, processamento, cálculo e unificação de todos os dados.\n",
        "    Salva o resultado em um CSV formatado e um JSON de configuração.\n",
        "    \"\"\"\n",
        "    logging.info(\"Iniciando o processamento e unificação dos dados...\")\n",
        "\n",
        "    lista_dfs_modelos = []\n",
        "    lista_dfs_real = []\n",
        "    ordem_modelos_estimados = []\n",
        "    model_comments = {}\n",
        "\n",
        "    # --- 1. Processar arquivos de modelo ---\n",
        "    for arquivo, nome_legenda in lista_arquivos_modelos:\n",
        "        df_estimado, df_real, comentario = _ler_e_processar_modelo_csv(arquivo, nome_legenda, base_dados)\n",
        "\n",
        "        if df_estimado is not None and df_real is not None:\n",
        "            lista_dfs_modelos.append(df_estimado)\n",
        "            lista_dfs_real.append(df_real)\n",
        "            ordem_modelos_estimados.append(nome_legenda)\n",
        "            model_comments[nome_legenda] = comentario\n",
        "\n",
        "    if not lista_dfs_modelos:\n",
        "        logging.error(\"Nenhum arquivo de modelo foi processado com sucesso.\")\n",
        "        return\n",
        "\n",
        "    # --- 2. Unificar dados Reais (IBGE) ---\n",
        "    # Assume que os dados reais (y) são os mesmos em todos os arquivos\n",
        "    df_real_unificado = pd.concat(lista_dfs_real).drop_duplicates().groupby('Ano').mean().reset_index()\n",
        "\n",
        "    # --- 3. Unificar dados dos Modelos ---\n",
        "    plot_df = reduce(lambda left, right: pd.merge(left, right, on='Ano', how='outer'), lista_dfs_modelos)\n",
        "    plot_df = pd.merge(df_real_unificado, plot_df, on='Ano', how='left')\n",
        "\n",
        "    # --- 4. Preparar e Unificar CONAB ---\n",
        "    df_conab_est, nome_conab = _preparar_dados_conab(df_conab_raw)\n",
        "    plot_df = pd.merge(plot_df, df_conab_est, on='Ano', how='left')\n",
        "    plot_df = plot_df.sort_values('Ano').reset_index(drop=True)\n",
        "\n",
        "    ordem_modelos_estimados.append(nome_conab)\n",
        "\n",
        "    # --- 5. Adicionar comentários padrão ---\n",
        "    ibge_comment = \"Dados oficiais de safra do Instituto Brasileiro de Geografia e Estatística.\"\n",
        "    conab_comment = \"Estimativa oficial da Companhia Nacional de Abastecimento.\"\n",
        "    model_comments['Safra Real (IBGE)'] = _format_comment_with_breaks(ibge_comment)\n",
        "    model_comments[nome_conab] = _format_comment_with_breaks(conab_comment)\n",
        "\n",
        "    # --- 6. Calcular Diferenças Percentuais ---\n",
        "    plot_df = _calcular_diferencas_percentuais(plot_df, ordem_modelos_estimados)\n",
        "\n",
        "    # --- 7. Salvar arquivos de saída ---\n",
        "    try:\n",
        "        plot_df.to_csv(output_csv_path, index=False, float_format='%.2f')\n",
        "        logging.info(f\"Dados formatados salvos com sucesso em: {output_csv_path}\")\n",
        "\n",
        "        config_data = {\n",
        "            \"comments\": model_comments,\n",
        "            \"order\": ordem_modelos_estimados,\n",
        "            \"conab_name\": nome_conab,\n",
        "            \"ibge_name\": \"Safra Real (IBGE)\"\n",
        "        }\n",
        "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(config_data, f, ensure_ascii=False, indent=4)\n",
        "        logging.info(f\"Configuração do gráfico salva com sucesso em: {output_json_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erro ao salvar arquivos de saída: {e}\")"
      ],
      "metadata": {
        "id": "TyhTntcAR-Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Função de Geração de Gráfico (Parte 2) ---\n",
        "\n",
        "def gerar_grafico_de_csv(input_csv_path, input_json_path, nome_arquivo_html=\"grafico_comparativo_cafe.html\"):\n",
        "    \"\"\"\n",
        "    Gera um gráfico comparativo usando Plotly a partir de um arquivo CSV\n",
        "    formatado e um arquivo JSON de configuração.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Carregar dados processados ---\n",
        "    logging.info(\"Iniciando a geração do gráfico a partir dos arquivos processados...\")\n",
        "    try:\n",
        "        plot_df = pd.read_csv(input_csv_path)\n",
        "        with open(input_json_path, 'r', encoding='utf-8') as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        model_comments = config.get('comments', {})\n",
        "        ordem_modelos_estimados = config.get('order', [])\n",
        "        nome_conab = config.get('conab_name', 'Estimativa CONAB')\n",
        "        nome_ibge = config.get('ibge_name', 'Safra Real (IBGE)')\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"Erro: Arquivo CSV '{input_csv_path}' ou JSON '{input_json_path}' não encontrado.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erro ao ler os arquivos de entrada: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 2. Gerando o Gráfico com Plotly ---\n",
        "    logging.info(\"Gerando o gráfico com Plotly.\")\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # --- DEFINIÇÃO DE CORES FIXAS E CUSTOMIZADAS ---\n",
        "    cor_ibge = 'darkblue'\n",
        "    cor_conab = 'firebrick'\n",
        "    cores_outros_modelos = [\n",
        "      '#8EB38E', '#B3A7CC', '#E6B36C', '#B89877',\n",
        "      '#D79FB5', '#9E9E9E', '#C6D07D', '#93C6CB',\n",
        "      '#E6C48F', '#A77FBA',\n",
        "      # Novas cores adicionadas:\n",
        "      '#7FB1B3', '#D4A59A', '#A2C68D', '#C79FE2',\n",
        "      '#E0A479', '#9AC1CF', '#B9D196', '#D9A8D0',\n",
        "      '#8FC2C1', '#E6D29E'\n",
        "    ]\n",
        "\n",
        "    outros_modelos_estimados = [m for m in ordem_modelos_estimados if m != nome_conab]\n",
        "\n",
        "    # --- BARRAS ---\n",
        "    # 1. Barra IBGE\n",
        "    fig.add_trace(go.Bar(\n",
        "        x=plot_df['Ano'], y=plot_df[nome_ibge],\n",
        "        name=nome_ibge, marker_color=cor_ibge,\n",
        "        text=plot_df[nome_ibge].apply(lambda x: f'{x:,.0f}'), textposition='outside',\n",
        "        customdata=[model_comments.get(nome_ibge)] * len(plot_df),\n",
        "        hovertemplate='<b>Ano: %{x}</b><br><br><b>Fonte</b>: %{data.name}<br><b>Produção</b>: %{y:,.0f} sacas<br><br><b>Descrição</b>:<br>%{customdata}<extra></extra>'\n",
        "    ))\n",
        "\n",
        "    # 2. Barra CONAB\n",
        "    fig.add_trace(go.Bar(\n",
        "        x=plot_df['Ano'], y=plot_df[nome_conab],\n",
        "        name=nome_conab, marker_color=cor_conab,\n",
        "        text=plot_df[nome_conab].apply(lambda x: f'{x:,.0f}' if pd.notna(x) else ''), textposition='outside',\n",
        "        customdata=[model_comments.get(nome_conab)] * len(plot_df),\n",
        "        hovertemplate='<b>Ano: %{x}</b><br><br><b>Modelo</b>: %{data.name}<br><b>Produção</b>: %{y:,.0f} sacas<br><br><b>Descrição</b>:<br>%{customdata}<extra></extra>'\n",
        "    ))\n",
        "\n",
        "    # 3. Barras para outros modelos\n",
        "    contador_cor = 0\n",
        "    for col in outros_modelos_estimados:\n",
        "        cor = cores_outros_modelos[contador_cor % len(cores_outros_modelos)]\n",
        "        fig.add_trace(go.Bar(\n",
        "            x=plot_df['Ano'], y=plot_df[col], name=col, marker_color=cor,\n",
        "            text=plot_df[col].apply(lambda x: f'{x:,.0f}' if pd.notna(x) else ''), textposition='outside',\n",
        "            customdata=[model_comments.get(col)] * len(plot_df),\n",
        "            hovertemplate='<b>Ano: %{x}</b><br><br><b>Modelo</b>: %{data.name}<br><b>Produção</b>: %{y:,.0f} sacas<br><br><b>Descrição</b>:<br>%{customdata}<extra></extra>'\n",
        "        ))\n",
        "        contador_cor += 1\n",
        "\n",
        "    # --- LINHAS DE DIFERENÇA PERCENTUAL ---\n",
        "    # 1. Linha CONAB\n",
        "    col_diff_conab = f\"Diferença % ({nome_conab})\"\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=plot_df['Ano'], y=plot_df[col_diff_conab], name=col_diff_conab,\n",
        "        mode='lines+markers+text', yaxis='y2', line=dict(color=cor_conab, dash='dot'),\n",
        "        text=plot_df[col_diff_conab].apply(lambda x: f'{x:.2f}%' if pd.notna(x) else ''), textposition='top center', textfont=dict(size=10, color=cor_conab),\n",
        "        customdata=[model_comments.get(nome_conab)] * len(plot_df),\n",
        "        hovertemplate='<b>Ano: %{x}</b><br><br><b>Modelo</b>: ' + nome_conab + '<br><b>Diferença</b>: %{y:.2f}%<br><br><b>Descrição</b>:<br>%{customdata}<extra></extra>'\n",
        "    ))\n",
        "\n",
        "    # 2. Linhas para outros modelos\n",
        "    contador_cor = 0\n",
        "    for col_original in outros_modelos_estimados:\n",
        "        cor = cores_outros_modelos[contador_cor % len(cores_outros_modelos)]\n",
        "        col_diff = f\"Diferença % ({col_original})\"\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=plot_df['Ano'], y=plot_df[col_diff], name=col_diff,\n",
        "            mode='lines+markers+text', yaxis='y2', line=dict(color=cor, dash='dot'),\n",
        "            text=plot_df[col_diff].apply(lambda x: f'{x:.2f}%' if pd.notna(x) else ''), textposition='top center', textfont=dict(size=10, color=cor),\n",
        "            customdata=[model_comments.get(col_original)] * len(plot_df),\n",
        "            hovertemplate='<b>Ano: %{x}</b><br><br><b>Modelo</b>: ' + col_original + '<br><b>Diferença</b>: %{y:.2f}%<br><br><b>Descrição</b>:<br>%{customdata}<extra></extra>'\n",
        "        ))\n",
        "        contador_cor += 1\n",
        "\n",
        "    # --- 3. Layout do Gráfico e da Legenda ---\n",
        "    fig.update_layout(\n",
        "        title=f'<b>Produção de Café (Sacas): Modelos vs. Real (IBGE) e CONAB | {plot_df[\"Ano\"].min()}-{plot_df[\"Ano\"].max()}</b>',\n",
        "        xaxis_title='<b>Ano</b>', yaxis_title='<b>Produção em Sacas</b>', barmode='group',\n",
        "        template='plotly_white', height=700,\n",
        "        legend=dict(\n",
        "            title=dict(text=\"<b>Legenda</b><br>\"),\n",
        "            orientation=\"h\",\n",
        "            yanchor=\"bottom\",\n",
        "            y=-0.8,\n",
        "            xanchor=\"center\",\n",
        "            x=0.5\n",
        "        ),\n",
        "        margin=dict(t=100, r=50, l=130, b=300), # Margem inferior aumentada para a legenda\n",
        "        yaxis2=dict(\n",
        "            title='<b>Diferença Percentual (%)</b>', overlaying='y', side='right',\n",
        "            showgrid=False, zeroline=True, zerolinecolor='lightgrey', zerolinewidth=1\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # --- 4. Salvando e Exibindo o Gráfico ---\n",
        "    try:\n",
        "        logging.info(f\"Salvando o gráfico como '{nome_arquivo_html}'...\")\n",
        "        fig.write_html(nome_arquivo_html)\n",
        "        logging.info(\"Gráfico salvo com sucesso.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Ocorreu um erro ao salvar o gráfico em HTML: {e}\")\n",
        "\n",
        "    logging.info(\"Exibindo o gráfico...\")\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "phg_xkyHRcAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grafico"
      ],
      "metadata": {
        "id": "qQca_9jbRaK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    data_conab = {\n",
        "        '2020': [31074.55, 34337.30442, 9.502069176], '2021': [20661.3, 21858.9, 5.478775236],\n",
        "        '2022': [26687.4, 21570.1, 23.72404393], '2023': [27101.9, 28650.4, 5.404811102],\n",
        "        '2024': [29836.4, 27708.3, 7.680370142], '2025': [24703.9, 24703.9, 0.0],\n",
        "        '2026': [0.0, 0.0, 0.0]\n",
        "    }\n",
        "    df_conab_raw = pd.DataFrame(data_conab, index=['ESTIMADO', 'REAL', 'Erro'])\n",
        "\n",
        "    try:\n",
        "        base_dados_raw = pd.read_csv(\"Area_colhida_V2.csv\")\n",
        "        base_dados_raw.rename(columns={'Area': 'Área colhida (Hectares)'}, inplace=True)\n",
        "        base_dados_raw = base_dados_raw[[\"Municipio\", \"Ano\", \"Área colhida (Hectares)\"]]\n",
        "    except FileNotFoundError:\n",
        "        logging.error(\"ERRO: Arquivo 'Area_colhida_V2.csv' não encontrado. Abortando.\")\n",
        "        base_dados_raw = pd.DataFrame()\n",
        "\n",
        "    # Modifique esta lista para alterar os arquivos, nomes e a ordem no gráfico\n",
        "    lista_arquivos_input = [\n",
        "      (\"26_11_2025_features_por_cluster.csv\", \"Feature por cluster (27/11/2025)\"),\n",
        "      (\"01_12_2025_teste_2016_2025_por_cluster.csv\", \"Feature por cluster 2016-2025\"),\n",
        "      (\"modelo_altitude_ajustado.csv\", \"Modelo Altitude\"),\n",
        "    ]\n",
        "\n",
        "    # Nomes dos arquivos intermediários\n",
        "    formatted_date = current_date.strftime(\"%d_%m_%Y\")\n",
        "    PATH_CSV_FORMATADO = \"dados_grafico_formatado.csv\"\n",
        "    PATH_JSON_CONFIG = \"config_grafico.json\"\n",
        "    PATH_HTML_OUTPUT = f\"grafico_sacas_cafe_{formatted_date}.html\"\n",
        "\n",
        "    # --- Bloco 2: Execução ---\n",
        "\n",
        "    if not base_dados_raw.empty and lista_arquivos_input:\n",
        "        # 1. Processa os dados e salva em arquivos CSV e JSON\n",
        "        processar_dados_para_csv(\n",
        "            lista_arquivos_modelos=lista_arquivos_input,\n",
        "            df_conab_raw=df_conab_raw,\n",
        "            base_dados=base_dados_raw,\n",
        "            output_csv_path=PATH_CSV_FORMATADO,\n",
        "            output_json_path=PATH_JSON_CONFIG\n",
        "        )\n",
        "\n",
        "        # 2. Gera o gráfico lendo os arquivos criados\n",
        "        gerar_grafico_de_csv(\n",
        "            input_csv_path=PATH_CSV_FORMATADO,\n",
        "            input_json_path=PATH_JSON_CONFIG,\n",
        "            nome_arquivo_html=PATH_HTML_OUTPUT\n",
        "        )\n",
        "    else:\n",
        "        logging.error(\"Não foi possível executar o script devido à falta do 'Area_colhida_V2.csv' ou lista de arquivos vazia.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "erGujnD4Qz8Q",
        "outputId": "a2a1d792-5ad4-4135-873c-a4d51b1c78eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"158b115d-1be7-4f67-9586-891f503a96e2\" class=\"plotly-graph-div\" style=\"height:700px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"158b115d-1be7-4f67-9586-891f503a96e2\")) {                    Plotly.newPlot(                        \"158b115d-1be7-4f67-9586-891f503a96e2\",                        [{\"customdata\":[\"Dados oficiais de safra do Instituto Brasileiro de Geografia e Estatística.\",\"Dados oficiais de safra do Instituto Brasileiro de Geografia e Estatística.\",\"Dados oficiais de safra do Instituto Brasileiro de Geografia e Estatística.\",\"Dados oficiais de safra do Instituto Brasileiro de Geografia e Estatística.\",\"Dados oficiais de safra do Instituto Brasileiro de Geografia e Estatística.\",\"Dados oficiais de safra do Instituto Brasileiro de Geografia e Estatística.\",\"Dados oficiais de safra do Instituto Brasileiro de Geografia e Estatística.\",\"Dados oficiais de safra do Instituto Brasileiro de Geografia e Estatística.\",\"Dados oficiais de safra do Instituto Brasileiro de Geografia e Estatística.\",\"Dados oficiais de safra do Instituto Brasileiro de Geografia e Estatística.\"],\"hovertemplate\":\"\\u003cb\\u003eAno: %{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eFonte\\u003c\\u002fb\\u003e: %{data.name}\\u003cbr\\u003e\\u003cb\\u003eProdução\\u003c\\u002fb\\u003e: %{y:,.0f} sacas\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eDescrição\\u003c\\u002fb\\u003e:\\u003cbr\\u003e%{customdata}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":\"darkblue\"},\"name\":\"Safra Real (IBGE)\",\"text\":[\"30,158,883\",\"34,126,583\",\"31,296,200\",\"24,574,517\",\"33,996,367\",\"22,345,000\",\"22,818,800\",\"28,483,900\",\"27,650,983\",\"0\"],\"textposition\":\"outside\",\"x\":[2016,2017,2018,2019,2020,2021,2022,2023,2024,2025],\"y\":[30158883.33,34126583.33,31296200.0,24574516.67,33996366.67,22345000.0,22818800.0,28483900.0,27650983.33,0.0],\"type\":\"bar\"},{\"customdata\":[\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\"],\"hovertemplate\":\"\\u003cb\\u003eAno: %{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eModelo\\u003c\\u002fb\\u003e: %{data.name}\\u003cbr\\u003e\\u003cb\\u003eProdução\\u003c\\u002fb\\u003e: %{y:,.0f} sacas\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eDescrição\\u003c\\u002fb\\u003e:\\u003cbr\\u003e%{customdata}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":\"firebrick\"},\"name\":\"Estimativa CONAB\",\"text\":[\"\",\"\",\"\",\"\",\"31,074,550\",\"20,661,300\",\"26,687,400\",\"27,101,900\",\"29,836,400\",\"24,703,900\"],\"textposition\":\"outside\",\"x\":[2016,2017,2018,2019,2020,2021,2022,2023,2024,2025],\"y\":[null,null,null,null,31074550.0,20661300.0,26687400.0,27101900.0,29836400.0,24703900.0],\"type\":\"bar\"},{\"customdata\":[\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\"],\"hovertemplate\":\"\\u003cb\\u003eAno: %{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eModelo\\u003c\\u002fb\\u003e: %{data.name}\\u003cbr\\u003e\\u003cb\\u003eProdução\\u003c\\u002fb\\u003e: %{y:,.0f} sacas\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eDescrição\\u003c\\u002fb\\u003e:\\u003cbr\\u003e%{customdata}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":\"#8EB38E\"},\"name\":\"Feature por cluster (27\\u002f11\\u002f2025)\",\"text\":[\"\",\"\",\"\",\"\",\"35,426,853\",\"27,933,555\",\"33,752,862\",\"27,825,387\",\"24,864,461\",\"22,058,228\"],\"textposition\":\"outside\",\"x\":[2016,2017,2018,2019,2020,2021,2022,2023,2024,2025],\"y\":[null,null,null,null,35426852.85,27933555.42,33752861.73,27825387.2,24864461.17,22058228.38],\"type\":\"bar\"},{\"customdata\":[\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\"],\"hovertemplate\":\"\\u003cb\\u003eAno: %{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eModelo\\u003c\\u002fb\\u003e: %{data.name}\\u003cbr\\u003e\\u003cb\\u003eProdução\\u003c\\u002fb\\u003e: %{y:,.0f} sacas\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eDescrição\\u003c\\u002fb\\u003e:\\u003cbr\\u003e%{customdata}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":\"#B3A7CC\"},\"name\":\"Feature por cluster 2016-2025\",\"text\":[\"21,600,735\",\"32,535,930\",\"31,623,025\",\"26,947,073\",\"28,620,113\",\"31,609,446\",\"26,194,223\",\"25,507,729\",\"27,300,044\",\"24,748,777\"],\"textposition\":\"outside\",\"x\":[2016,2017,2018,2019,2020,2021,2022,2023,2024,2025],\"y\":[21600734.87,32535929.68,31623024.74,26947072.58,28620112.7,31609446.44,26194223.21,25507728.77,27300044.29,24748777.29],\"type\":\"bar\"},{\"customdata\":[\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\"],\"hovertemplate\":\"\\u003cb\\u003eAno: %{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eModelo\\u003c\\u002fb\\u003e: %{data.name}\\u003cbr\\u003e\\u003cb\\u003eProdução\\u003c\\u002fb\\u003e: %{y:,.0f} sacas\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eDescrição\\u003c\\u002fb\\u003e:\\u003cbr\\u003e%{customdata}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":\"#E6B36C\"},\"name\":\"Modelo Altitude\",\"text\":[\"\",\"\",\"\",\"\",\"30,568,152\",\"31,813,823\",\"27,571,501\",\"19,462,849\",\"25,181,333\",\"25,560,650\"],\"textposition\":\"outside\",\"x\":[2016,2017,2018,2019,2020,2021,2022,2023,2024,2025],\"y\":[null,null,null,null,30568151.92,31813822.78,27571501.05,19462848.63,25181333.02,25560650.27],\"type\":\"bar\"},{\"customdata\":[\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\",\"Estimativa oficial da Companhia Nacional de Abastecimento.\"],\"hovertemplate\":\"\\u003cb\\u003eAno: %{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eModelo\\u003c\\u002fb\\u003e: Estimativa CONAB\\u003cbr\\u003e\\u003cb\\u003eDiferença\\u003c\\u002fb\\u003e: %{y:.2f}%\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eDescrição\\u003c\\u002fb\\u003e:\\u003cbr\\u003e%{customdata}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"line\":{\"color\":\"firebrick\",\"dash\":\"dot\"},\"mode\":\"lines+markers+text\",\"name\":\"Diferença % (Estimativa CONAB)\",\"text\":[\"\",\"\",\"\",\"\",\"-8.59%\",\"-7.54%\",\"16.95%\",\"-4.85%\",\"7.90%\",\"inf%\"],\"textfont\":{\"color\":\"firebrick\",\"size\":10},\"textposition\":\"top center\",\"x\":[2016,2017,2018,2019,2020,2021,2022,2023,2024,2025],\"y\":[null,null,null,null,-8.59,-7.54,16.95,-4.85,7.9,null],\"yaxis\":\"y2\",\"type\":\"scatter\"},{\"customdata\":[\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f26_11_2025_features_por_cluster\\u002fModelos\\u002fV43_cluster_0_2020_(2020)\\u003cbr\\u003eModelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019\\u003cbr\\u003ee testado com os dados de 2020.     Para esse treinamento foi utilizado o\\u003cbr\\u003edataset V43.     O dataset foi dividido em 5 Clusters. Nesta versão\\u003cbr\\u003ecadacluster tem seu proprio conjunto de features.      O modelo foi treinado\\u003cbr\\u003ecom os seguintes Hiperparâmetros:     input_size: 6  (-1 significa que usou a\\u003cbr\\u003ejanela de contexto maxima)     h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\"],\"hovertemplate\":\"\\u003cb\\u003eAno: %{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eModelo\\u003c\\u002fb\\u003e: Feature por cluster (27\\u002f11\\u002f2025)\\u003cbr\\u003e\\u003cb\\u003eDiferença\\u003c\\u002fb\\u003e: %{y:.2f}%\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eDescrição\\u003c\\u002fb\\u003e:\\u003cbr\\u003e%{customdata}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"line\":{\"color\":\"#8EB38E\",\"dash\":\"dot\"},\"mode\":\"lines+markers+text\",\"name\":\"Diferença % (Feature por cluster (27\\u002f11\\u002f2025))\",\"text\":[\"\",\"\",\"\",\"\",\"4.21%\",\"25.01%\",\"47.92%\",\"-2.31%\",\"-10.08%\",\"inf%\"],\"textfont\":{\"color\":\"#8EB38E\",\"size\":10},\"textposition\":\"top center\",\"x\":[2016,2017,2018,2019,2020,2021,2022,2023,2024,2025],\"y\":[null,null,null,null,4.21,25.01,47.92,-2.31,-10.08,null],\"yaxis\":\"y2\",\"type\":\"scatter\"},{\"customdata\":[\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\",\".\\u002fTreinos\\u002f01_12_2025_teste_2016_2025_por_cluster\\u002fModelos\\u002fV43_cluster_0_2017_(\\u003cbr\\u003e2017)     Modelo LSTM treinado com dados de 2012 a 2015, validado com os\\u003cbr\\u003edados de 2016 e testado com os dados de 2017.     Para esse treinamento foi\\u003cbr\\u003eutilizado o dataset V43.     O dataset foi dividido em 5 Clusters. Nesta\\u003cbr\\u003eversão cadacluster tem seu proprio conjunto de features. Nesta  versão usamos\\u003cbr\\u003eo dataset V43 para testar o modelo entre os anos de 2016-2025. Foi usado a\\u003cbr\\u003epartir de 2016, para que pudese usar pelo menos 2 ano de contexto durante o\\u003cbr\\u003etreinamento e 1 de validação.      O modelo foi treinado com os seguintes\\u003cbr\\u003eHiperparâmetros:     input_size: 3      h: 1 (horizonte de previsão)\\u003cbr\\u003eencoder_n_layers = 4     learning_rate: 0.00013034723280377263\\u003cbr\\u003eencoder_hidden_size: 192     decoder_layers: 1     decoder_hidden_size: 64\\u003cbr\\u003ebatch_size: 32     dropout: 0.3     weight_decay: 0.0001     steps: 1000\"],\"hovertemplate\":\"\\u003cb\\u003eAno: %{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eModelo\\u003c\\u002fb\\u003e: Feature por cluster 2016-2025\\u003cbr\\u003e\\u003cb\\u003eDiferença\\u003c\\u002fb\\u003e: %{y:.2f}%\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eDescrição\\u003c\\u002fb\\u003e:\\u003cbr\\u003e%{customdata}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"line\":{\"color\":\"#B3A7CC\",\"dash\":\"dot\"},\"mode\":\"lines+markers+text\",\"name\":\"Diferença % (Feature por cluster 2016-2025)\",\"text\":[\"-28.38%\",\"-4.66%\",\"1.04%\",\"9.65%\",\"-15.81%\",\"41.46%\",\"14.79%\",\"-10.45%\",\"-1.27%\",\"inf%\"],\"textfont\":{\"color\":\"#B3A7CC\",\"size\":10},\"textposition\":\"top center\",\"x\":[2016,2017,2018,2019,2020,2021,2022,2023,2024,2025],\"y\":[-28.38,-4.66,1.04,9.65,-15.81,41.46,14.79,-10.45,-1.27,null],\"yaxis\":\"y2\",\"type\":\"scatter\"},{\"customdata\":[\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\"],\"hovertemplate\":\"\\u003cb\\u003eAno: %{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eModelo\\u003c\\u002fb\\u003e: Modelo Altitude\\u003cbr\\u003e\\u003cb\\u003eDiferença\\u003c\\u002fb\\u003e: %{y:.2f}%\\u003cbr\\u003e\\u003cbr\\u003e\\u003cb\\u003eDescrição\\u003c\\u002fb\\u003e:\\u003cbr\\u003e%{customdata}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"line\":{\"color\":\"#E6B36C\",\"dash\":\"dot\"},\"mode\":\"lines+markers+text\",\"name\":\"Diferença % (Modelo Altitude)\",\"text\":[\"\",\"\",\"\",\"\",\"-10.08%\",\"42.38%\",\"20.83%\",\"-31.67%\",\"-8.93%\",\"inf%\"],\"textfont\":{\"color\":\"#E6B36C\",\"size\":10},\"textposition\":\"top center\",\"x\":[2016,2017,2018,2019,2020,2021,2022,2023,2024,2025],\"y\":[null,null,null,null,-10.08,42.38,20.83,-31.67,-8.93,null],\"yaxis\":\"y2\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"legend\":{\"title\":{\"text\":\"\\u003cb\\u003eLegenda\\u003c\\u002fb\\u003e\\u003cbr\\u003e\"},\"orientation\":\"h\",\"yanchor\":\"bottom\",\"y\":-0.8,\"xanchor\":\"center\",\"x\":0.5},\"margin\":{\"t\":100,\"r\":50,\"l\":130,\"b\":300},\"yaxis2\":{\"title\":{\"text\":\"\\u003cb\\u003eDiferença Percentual (%)\\u003c\\u002fb\\u003e\"},\"overlaying\":\"y\",\"side\":\"right\",\"showgrid\":false,\"zeroline\":true,\"zerolinecolor\":\"lightgrey\",\"zerolinewidth\":1},\"title\":{\"text\":\"\\u003cb\\u003eProdução de Café (Sacas): Modelos vs. Real (IBGE) e CONAB | 2016-2025\\u003c\\u002fb\\u003e\"},\"xaxis\":{\"title\":{\"text\":\"\\u003cb\\u003eAno\\u003c\\u002fb\\u003e\"}},\"yaxis\":{\"title\":{\"text\":\"\\u003cb\\u003eProdução em Sacas\\u003c\\u002fb\\u003e\"}},\"barmode\":\"group\",\"height\":700},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('158b115d-1be7-4f67-9586-891f503a96e2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "mensagem = \"\"\"\n",
        "Esse grafico foi gerado para a reunião do dia 04/12/2025, com base nos pedidos realizados na reuinião de 27/11/2025.\n",
        "Foi mostrado o resultado para 3 modelos diferentes:\n",
        "- 01_12_2025_teste_2016_2025_por_cluster.csv onde cada cluster tem um conjunto específico de features.\n",
        "  para cada ano de teste entre 2016 e 2025 foi treinado um modelo.\n",
        "  os modelos estão salvos no diretorio \"Treinos/01_12_2025_teste_2016_2025_por_cluster/Modelos\".\n",
        "\n",
        "- 26_11_2025_features_por_cluster.csv onde cada cluster tem um conjunto específico de features.\n",
        "  para cada ano de teste entre 2020 e 2025 foi treinado um modelo.\n",
        "  os modelos estão salvos no diretorio \"Treinos/26_11_2025_features_por_cluster/Modelos\".\n",
        "\n",
        "- modelo_altitude_ajustado.csv neste modelo tem o uso da variavel de altitude.\n",
        "  os modelos estão salvos no diretorio \"Treinos/Modelo_Altitude_Ajustado/Modelos\".\n",
        "\n",
        "Os valores para a área destinada a colheita foi usado o dataset: Area_colhida_V2.csv\n",
        "Todos os modelos foram treinados usando o mesmo conjunto de hiperparametros.\n",
        "\"\"\"\n",
        "\n",
        "# Obtém a data atual no formato dia_mês_ano\n",
        "data_atual = datetime.now().strftime(\"%d_%m_%Y\")\n",
        "\n",
        "# Monta o nome do arquivo\n",
        "nome_arquivo = f\"grafico_{data_atual}.txt\"\n",
        "\n",
        "# Salva a string no arquivo, mantendo a formatação\n",
        "with open(nome_arquivo, \"w\", encoding=\"utf-8\") as arquivo:\n",
        "    arquivo.write(mensagem)\n",
        "\n",
        "print(f\"Arquivo salvo como: {nome_arquivo}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBli2GY1eLxK",
        "outputId": "e54c9ffa-3d29-422b-9687-0a2adc42f649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo salvo como: grafico_04_12_2025.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grafico da produtividade"
      ],
      "metadata": {
        "id": "4uO1xYpxROHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from functools import reduce\n",
        "import logging\n",
        "import textwrap\n",
        "\n",
        "# Configuração do sistema de logs para monitorar a execução\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def gerar_grafico_produtividade_final(lista_arquivos_modelos, df_conab, base_dados_area, nome_arquivo_html=\"grafico_produtividade_teste.html\"):\n",
        "    \"\"\"\n",
        "    Gera um gráfico comparativo de produtividade (Ton/ha) focando apenas nos dados de TESTE.\n",
        "\n",
        "    Lógica de Cálculo:\n",
        "    1. IBGE e Modelos: Utiliza Média Ponderada pela Área (Município a Município).\n",
        "       Fórmula: Soma(Produtividade_Município * Área_Município) / Soma(Área_Total)\n",
        "    2. CONAB: Utiliza a produção total estimada dividida pela área total do arquivo CSV.\n",
        "\n",
        "    Parâmetros:\n",
        "    - lista_arquivos_modelos: Lista de tuplas (caminho_arquivo_csv, nome_legenda).\n",
        "    - df_conab: DataFrame com dados de sacas da CONAB.\n",
        "    - base_dados_area: DataFrame carregado do 'Area_colhida_V2.csv' contendo as áreas.\n",
        "    \"\"\"\n",
        "\n",
        "    # Função auxiliar para quebrar linhas em textos de tooltip muito longos\n",
        "    def format_comment_with_breaks(text, line_length=77):\n",
        "        if not isinstance(text, str):\n",
        "            return \"Nenhum comentário disponível.\"\n",
        "        return '<br>'.join(textwrap.wrap(text, width=line_length))\n",
        "\n",
        "    if not lista_arquivos_modelos:\n",
        "        logging.error(\"A lista de arquivos de modelos está vazia.\")\n",
        "        return\n",
        "\n",
        "    logging.info(\"Iniciando o processamento dos modelos...\")\n",
        "\n",
        "    lista_dfs_finais = [] # Armazena os DataFrames processados de cada modelo\n",
        "    ordem_modelos = []    # Mantém a ordem de plotagem\n",
        "    model_comments = {}   # Armazena os comentários/descrições para o tooltip\n",
        "\n",
        "    # ==============================================================================\n",
        "    # 1. PROCESSAMENTO DOS MODELOS (CÁLCULO DA MÉDIA PONDERADA)\n",
        "    # ==============================================================================\n",
        "    for arquivo, nome_legenda in lista_arquivos_modelos:\n",
        "        try:\n",
        "            logging.info(f\"Processando arquivo: {arquivo} ({nome_legenda})\")\n",
        "            df_modelo = pd.read_csv(arquivo)\n",
        "\n",
        "            # Extração do comentário do modelo (se existir)\n",
        "            comentario = \"Nenhum comentário disponível.\"\n",
        "            if 'comentario' in df_modelo.columns and not df_modelo['comentario'].dropna().empty:\n",
        "                comentario = df_modelo['comentario'].dropna().iloc[0]\n",
        "            model_comments[nome_legenda] = format_comment_with_breaks(comentario)\n",
        "\n",
        "            # Conversão de tipos de dados\n",
        "            df_modelo['ds'] = pd.to_datetime(df_modelo['ds'])\n",
        "            df_modelo['y_pred'] = pd.to_numeric(df_modelo['y_pred'], errors='coerce')\n",
        "\n",
        "            # --- FILTRO IMPORTANTE: Apenas dados de 'teste' ---\n",
        "            # Aqui garantimos que apenas os anos marcados como teste (ex: 2025) sejam processados.\n",
        "            cols_necessarias = [\"unique_id\", \"ds\", \"y\", \"y_pred\"]\n",
        "            df_processar = df_modelo[df_modelo['flag'] == \"teste\"][cols_necessarias].copy()\n",
        "\n",
        "            if df_processar.empty:\n",
        "                logging.warning(f\"O arquivo {arquivo} não possui dados com flag='teste'. Ignorando.\")\n",
        "                continue\n",
        "\n",
        "            # Lista temporária para armazenar o cálculo de volume (Ton) por município\n",
        "            temp_data = []\n",
        "\n",
        "            for _, row in df_processar.iterrows():\n",
        "                ano = row[\"ds\"].year\n",
        "                municipio_id = row[\"unique_id\"]\n",
        "\n",
        "                # Busca a área correspondente no arquivo único de áreas (Area_colhida_V2)\n",
        "                filtro_area = (base_dados_area['Municipio'] == municipio_id) & (base_dados_area['Ano'] == ano)\n",
        "                area_encontrada = base_dados_area.loc[filtro_area, 'Area_Hectares']\n",
        "\n",
        "                if not area_encontrada.empty:\n",
        "                    area = area_encontrada.values[0]\n",
        "\n",
        "                    # CÁLCULO DE VOLUME (TONELADAS):\n",
        "                    # Multiplicamos a produtividade (Ton/ha) pela área (ha) para obter o peso correto do município\n",
        "                    volume_real_ton = row[\"y\"] * area\n",
        "                    volume_pred_ton = row[\"y_pred\"] * area\n",
        "\n",
        "                    temp_data.append({\n",
        "                        \"Ano\": ano,\n",
        "                        \"Area\": area,\n",
        "                        \"Vol_Real_Ton\": volume_real_ton,\n",
        "                        \"Vol_Pred_Ton\": volume_pred_ton\n",
        "                    })\n",
        "\n",
        "            # Se nenhum município foi cruzado corretamente, pula para o próximo modelo\n",
        "            if not temp_data:\n",
        "                logging.warning(f\"Nenhum dado de área encontrado para cruzar com {nome_legenda}\")\n",
        "                continue\n",
        "\n",
        "            # Criação do DataFrame intermediário\n",
        "            df_calc = pd.DataFrame(temp_data)\n",
        "\n",
        "            # Agrupa por ANO somando Volumes e Áreas (Consolidação Estadual/Nacional)\n",
        "            df_agg = df_calc.groupby('Ano')[['Area', 'Vol_Real_Ton', 'Vol_Pred_Ton']].sum().reset_index()\n",
        "\n",
        "            # CÁLCULO FINAL DA PRODUTIVIDADE MÉDIA PONDERADA:\n",
        "            # Produtividade = Volume Total (Ton) / Área Total (ha)\n",
        "            nome_col_real = \"Produtividade Real (IBGE)\"\n",
        "\n",
        "            # Uso de lambda para evitar erro de divisão por zero\n",
        "            df_agg[nome_col_real] = df_agg.apply(lambda x: x['Vol_Real_Ton'] / x['Area'] if x['Area'] > 0 else 0, axis=1).round(2)\n",
        "            df_agg[nome_legenda] = df_agg.apply(lambda x: x['Vol_Pred_Ton'] / x['Area'] if x['Area'] > 0 else 0, axis=1).round(2)\n",
        "\n",
        "            # Tratamento visual: Se o Real for zero (comum em anos futuros de teste), transforma em NaN para não plotar barra vazia\n",
        "            # Isso assume que se é 'teste', provavelmente não temos o Real oficial fechado ainda\n",
        "            df_agg.loc[df_agg[nome_col_real] == 0, nome_col_real] = np.nan\n",
        "\n",
        "            # Adiciona comentário padrão para a coluna do IBGE se ainda não existir\n",
        "            if nome_col_real not in model_comments:\n",
        "                model_comments[nome_col_real] = format_comment_with_breaks(\"Produtividade calculada via média ponderada utilizando a área do arquivo 'Area_colhida_V2'.\")\n",
        "\n",
        "            # Armazena o resultado final (apenas colunas necessárias)\n",
        "            lista_dfs_finais.append(df_agg[['Ano', nome_col_real, nome_legenda]])\n",
        "            ordem_modelos.append(nome_legenda)\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Erro crítico ao processar {arquivo}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not lista_dfs_finais:\n",
        "        logging.error(\"Nenhum modelo foi processado com sucesso. Abortando.\")\n",
        "        return\n",
        "\n",
        "    # União (Merge) de todos os DataFrames gerados em um único DataFrame mestre para plotagem\n",
        "    # Usa o 'Ano' e 'Produtividade Real' como chaves para alinhar tudo\n",
        "    plot_df = reduce(lambda left, right: pd.merge(left, right, on=['Ano', 'Produtividade Real (IBGE)'], how='outer'), lista_dfs_finais)\n",
        "    plot_df = plot_df.sort_values('Ano').reset_index(drop=True)\n",
        "\n",
        "    # ==============================================================================\n",
        "    # 2. CÁLCULO DA PRODUTIVIDADE CONAB\n",
        "    # ==============================================================================\n",
        "    logging.info(\"Calculando produtividade estimada CONAB...\")\n",
        "    nome_conab = \"Produtividade CONAB (Est.)\"\n",
        "\n",
        "    # Calcula a área total por ano usando o MESMO arquivo CSV usado nos modelos\n",
        "    df_area_total_ano = base_dados_area.groupby('Ano')['Area_Hectares'].sum().reset_index()\n",
        "\n",
        "    # Prepara os dados de produção da CONAB (Originalmente em Sacas)\n",
        "    df_conab_sacas = df_conab.T[['ESTIMADO']].rename(columns={'ESTIMADO': 'Sacas_Conab'})\n",
        "    df_conab_sacas.index.name = 'Ano'\n",
        "    df_conab_sacas.index = df_conab_sacas.index.astype(int)\n",
        "    df_conab_sacas['Sacas_Conab'] = df_conab_sacas['Sacas_Conab'] * 1000 # Ajuste para unidade (milhares de sacas -> sacas)\n",
        "\n",
        "    # Cruza Produção Conab com Área Total IBGE (CSV)\n",
        "    df_conab_calc = pd.merge(df_conab_sacas, df_area_total_ano, on='Ano', how='inner')\n",
        "\n",
        "    # Fórmula CONAB:\n",
        "    # 1. Sacas * 60 = Kg\n",
        "    # 2. Kg / 1000 = Toneladas\n",
        "    # 3. Toneladas / Hectares = Ton/ha\n",
        "    df_conab_calc[nome_conab] = ((df_conab_calc['Sacas_Conab'] * 60) / 1000) / df_conab_calc['Area_Hectares']\n",
        "    df_conab_calc[nome_conab] = df_conab_calc[nome_conab].round(2)\n",
        "\n",
        "    # Adiciona coluna CONAB ao DataFrame principal\n",
        "    plot_df = pd.merge(plot_df, df_conab_calc[['Ano', nome_conab]], on='Ano', how='left')\n",
        "    ordem_modelos.append(nome_conab)\n",
        "\n",
        "    model_comments[nome_conab] = format_comment_with_breaks(\"Produtividade derivada: (Produção CONAB / Área Total do CSV Area_colhida_V2).\")\n",
        "\n",
        "    # ==============================================================================\n",
        "    # 3. CÁLCULO DAS DIFERENÇAS PERCENTUAIS\n",
        "    # ==============================================================================\n",
        "    col_real = \"Produtividade Real (IBGE)\"\n",
        "\n",
        "    for col in ordem_modelos:\n",
        "        if col == col_real: continue\n",
        "        if col in plot_df.columns and col_real in plot_df.columns:\n",
        "            nome_diff = f\"Diferença % ({col})\"\n",
        "            # A diferença só é calculada se houver um valor Real válido (o que pode não ocorrer no futuro/teste)\n",
        "            plot_df[nome_diff] = np.where(\n",
        "                (plot_df[col_real] > 0) & (plot_df[col_real].notna()),\n",
        "                ((plot_df[col] - plot_df[col_real]) / plot_df[col_real]) * 100,\n",
        "                np.nan\n",
        "            )\n",
        "\n",
        "    # ==============================================================================\n",
        "    # 4. GERAÇÃO DO GRÁFICO COM PLOTLY\n",
        "    # ==============================================================================\n",
        "    logging.info(\"Gerando visualização Plotly...\")\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Definição de paleta de cores\n",
        "    cor_ibge = 'darkblue'\n",
        "    cor_conab = 'firebrick'\n",
        "    cores_outros = ['#8EB38E', '#B3A7CC', '#E6B36C', '#B89877', '#D79FB5', '#93C6CB']\n",
        "\n",
        "    # --- BARRA: Real (IBGE) ---\n",
        "    if col_real in plot_df.columns:\n",
        "        df_plot_real = plot_df[plot_df[col_real] > 0] # Filtra valores vazios\n",
        "        fig.add_trace(go.Bar(\n",
        "            x=df_plot_real['Ano'], y=df_plot_real[col_real],\n",
        "            name=col_real, marker_color=cor_ibge,\n",
        "            text=df_plot_real[col_real], textposition='outside',\n",
        "            customdata=[model_comments.get(col_real)] * len(df_plot_real),\n",
        "            hovertemplate='<b>Ano: %{x}</b><br>Fonte: IBGE<br>Valor: %{y} Ton/ha<br><br>%{customdata}<extra></extra>'\n",
        "        ))\n",
        "\n",
        "    # --- BARRA: CONAB ---\n",
        "    if nome_conab in plot_df.columns:\n",
        "        # Filtra para exibir apenas anos relevantes que estão no plot_df (anos de teste)\n",
        "        df_c = plot_df[plot_df[nome_conab].notna() & (plot_df[nome_conab] > 0)]\n",
        "        fig.add_trace(go.Bar(\n",
        "            x=df_c['Ano'], y=df_c[nome_conab],\n",
        "            name=nome_conab, marker_color=cor_conab,\n",
        "            text=df_c[nome_conab], textposition='outside',\n",
        "            customdata=[model_comments.get(nome_conab)] * len(df_c),\n",
        "            hovertemplate='<b>Ano: %{x}</b><br>Fonte: CONAB<br>Valor: %{y} Ton/ha<br><br>%{customdata}<extra></extra>'\n",
        "        ))\n",
        "\n",
        "    # --- BARRAS: Outros Modelos ---\n",
        "    idx_cor = 0\n",
        "    modelos_extras = [m for m in ordem_modelos if m != nome_conab and m != col_real]\n",
        "\n",
        "    for col in modelos_extras:\n",
        "        if col in plot_df.columns:\n",
        "            cor = cores_outros[idx_cor % len(cores_outros)]\n",
        "            df_m = plot_df[plot_df[col] > 0] # Filtra zeros\n",
        "            fig.add_trace(go.Bar(\n",
        "                x=df_m['Ano'], y=df_m[col],\n",
        "                name=col, marker_color=cor,\n",
        "                text=df_m[col], textposition='outside',\n",
        "                customdata=[model_comments.get(col)] * len(df_m),\n",
        "                hovertemplate=f'<b>Ano: %{{x}}</b><br>Modelo: {col}<br>Valor: %{{y}} Ton/ha<br><br>%{{customdata}}<extra></extra>'\n",
        "            ))\n",
        "            idx_cor += 1\n",
        "\n",
        "    # --- LINHAS: Diferença Percentual ---\n",
        "    for col in ordem_modelos:\n",
        "        if col == col_real: continue\n",
        "        col_diff = f\"Diferença % ({col})\"\n",
        "\n",
        "        if col_diff in plot_df.columns:\n",
        "            df_l = plot_df.dropna(subset=[col_diff])\n",
        "            if df_l.empty: continue # Se não houver dados de diferença (ex: falta o Real), não plota linha\n",
        "\n",
        "            # Define a cor da linha combinando com a barra correspondente\n",
        "            cor_linha = cor_conab if col == nome_conab else cores_outros[ordem_modelos.index(col) % len(cores_outros)]\n",
        "            if col == nome_conab: cor_linha = cor_conab\n",
        "\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=df_l['Ano'], y=df_l[col_diff],\n",
        "                name=f\"Diferença % ({col})\",\n",
        "                mode='lines+markers+text', yaxis='y2', line=dict(color=cor_linha, dash='dot'),\n",
        "                text=df_l[col_diff].apply(lambda x: f'{x:.1f}%'),\n",
        "                textposition='top center', textfont=dict(size=10)\n",
        "            ))\n",
        "\n",
        "    # --- LAYOUT E SALVAMENTO ---\n",
        "    fig.update_layout(\n",
        "        title='<b>Produtividade de Café (Ton/ha) - Período de Teste</b>',\n",
        "        xaxis=dict(title='<b>Ano</b>', type='category'), # 'category' força mostrar apenas os anos existentes sem decimais\n",
        "        yaxis_title='<b>Produtividade (Ton/ha)</b>',\n",
        "        barmode='group',\n",
        "        template='plotly_white', height=700,\n",
        "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.6, xanchor=\"center\", x=0.5),\n",
        "        margin=dict(b=250),\n",
        "        yaxis2=dict(title='<b>Diferença (%)</b>', overlaying='y', side='right', showgrid=False)\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Salvando gráfico em: {nome_arquivo_html}\")\n",
        "    try:\n",
        "        fig.write_html(nome_arquivo_html)\n",
        "        fig.show()\n",
        "        logging.info(\"Processo concluído com sucesso.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erro ao salvar/exibir o gráfico: {e}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# EXECUÇÃO DO SCRIPT\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Definição dos dados brutos da CONAB (Sacas)\n",
        "data_conab = {\n",
        "    '2020': [31074.55, 34337.30442, 9.502069176], '2021': [20661.3, 21858.9, 5.478775236],\n",
        "    '2022': [26687.4, 21570.1, 23.72404393], '2023': [27101.9, 28650.4, 5.404811102],\n",
        "    '2024': [29836.4, 27708.3, 7.680370142], '2025': [24703.9, 24703.9, 0.0],\n",
        "    '2026': [0.0, 0.0, 0.0]\n",
        "}\n",
        "df_conab = pd.DataFrame(data_conab, index=['ESTIMADO', 'REAL', 'Erro'])\n",
        "\n",
        "# 2. Carregamento e preparação do arquivo de Áreas (FONTE ÚNICA)\n",
        "try:\n",
        "    logging.info(\"Carregando base de dados de áreas (Area_colhida_V2.csv)...\")\n",
        "    base_dados = pd.read_csv(\"Area_colhida_V2.csv\")\n",
        "\n",
        "    # Padronização dos nomes das colunas para evitar erros (Area vs Área colhida...)\n",
        "    if 'Area' in base_dados.columns:\n",
        "        base_dados.rename(columns={'Area': 'Area_Hectares'}, inplace=True)\n",
        "    elif 'Área colhida (Hectares)' in base_dados.columns:\n",
        "        base_dados.rename(columns={'Área colhida (Hectares)': 'Area_Hectares'}, inplace=True)\n",
        "\n",
        "    # Mantém apenas colunas essenciais\n",
        "    base_dados = base_dados[[\"Municipio\", \"Ano\", \"Area_Hectares\"]]\n",
        "\n",
        "    # Definição da lista de modelos a serem processados\n",
        "    # Formato: (Caminho do CSV, Nome para Legenda)\n",
        "    lista_arquivos = [\n",
        "      (\"modelo_altitude.csv\", \"Estimativa SEBRAE - Altitude\"),\n",
        "      (\"modelo_altitude_ajustado.csv\", \"Estimativa SEBRAE - Altitude Ajustado\"),\n",
        "      (\"modelo_altitude_ajustado.csv\", \"Estimativa SEBRAE - Altitude Ajustado\"),\n",
        "      # Adicione outros modelos aqui...\n",
        "    ]\n",
        "\n",
        "    # Executa a geração do gráfico se a base de dados for válida\n",
        "    if not base_dados.empty:\n",
        "        gerar_grafico_produtividade_final(lista_arquivos, df_conab, base_dados)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    logging.error(\"ERRO CRÍTICO: Arquivo 'Area_colhida_V2.csv' não encontrado. Verifique o diretório.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Erro não esperado na preparação dos dados: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "ksEO7yCYRRc7",
        "outputId": "a753bd34-366f-4593-810a-adfe6914257b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"13292d4c-2e1f-49c6-89e5-dc3d42606a39\" class=\"plotly-graph-div\" style=\"height:700px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"13292d4c-2e1f-49c6-89e5-dc3d42606a39\")) {                    Plotly.newPlot(                        \"13292d4c-2e1f-49c6-89e5-dc3d42606a39\",                        [{\"customdata\":[\"Produtividade calculada via média ponderada utilizando a área do arquivo\\u003cbr\\u003e'Area_colhida_V2'.\",\"Produtividade calculada via média ponderada utilizando a área do arquivo\\u003cbr\\u003e'Area_colhida_V2'.\",\"Produtividade calculada via média ponderada utilizando a área do arquivo\\u003cbr\\u003e'Area_colhida_V2'.\",\"Produtividade calculada via média ponderada utilizando a área do arquivo\\u003cbr\\u003e'Area_colhida_V2'.\",\"Produtividade calculada via média ponderada utilizando a área do arquivo\\u003cbr\\u003e'Area_colhida_V2'.\"],\"hovertemplate\":\"\\u003cb\\u003eAno: %{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003eFonte: IBGE\\u003cbr\\u003eValor: %{y} Ton\\u002fha\\u003cbr\\u003e\\u003cbr\\u003e%{customdata}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":\"darkblue\"},\"name\":\"Produtividade Real (IBGE)\",\"text\":[1.96,1.35,1.36,1.61,1.53],\"textposition\":\"outside\",\"x\":[2020,2021,2022,2023,2024],\"y\":[1.96,1.35,1.36,1.61,1.53],\"type\":\"bar\"},{\"customdata\":[\"Produtividade derivada: (Produção CONAB \\u002f Área Total do CSV Area_colhida_V2).\",\"Produtividade derivada: (Produção CONAB \\u002f Área Total do CSV Area_colhida_V2).\",\"Produtividade derivada: (Produção CONAB \\u002f Área Total do CSV Area_colhida_V2).\",\"Produtividade derivada: (Produção CONAB \\u002f Área Total do CSV Area_colhida_V2).\",\"Produtividade derivada: (Produção CONAB \\u002f Área Total do CSV Area_colhida_V2).\",\"Produtividade derivada: (Produção CONAB \\u002f Área Total do CSV Area_colhida_V2).\"],\"hovertemplate\":\"\\u003cb\\u003eAno: %{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003eFonte: CONAB\\u003cbr\\u003eValor: %{y} Ton\\u002fha\\u003cbr\\u003e\\u003cbr\\u003e%{customdata}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":\"firebrick\"},\"name\":\"Produtividade CONAB (Est.)\",\"text\":[1.79,1.25,1.59,1.54,1.65,1.45],\"textposition\":\"outside\",\"x\":[2020,2021,2022,2023,2024,2025],\"y\":[1.79,1.25,1.59,1.54,1.65,1.45],\"type\":\"bar\"},{\"customdata\":[\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.00013034723280377263     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.00013034723280377263     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.00013034723280377263     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.00013034723280377263     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.00013034723280377263     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.00013034723280377263     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\"],\"hovertemplate\":\"\\u003cb\\u003eAno: %{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003eModelo: Estimativa SEBRAE - Altitude\\u003cbr\\u003eValor: %{y} Ton\\u002fha\\u003cbr\\u003e\\u003cbr\\u003e%{customdata}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":\"#8EB38E\"},\"name\":\"Estimativa SEBRAE - Altitude\",\"text\":[2.32,1.78,2.2,1.26,1.55,1.62],\"textposition\":\"outside\",\"x\":[2020,2021,2022,2023,2024,2025],\"y\":[2.32,1.78,2.2,1.26,1.55,1.62],\"type\":\"bar\"},{\"customdata\":[\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\",\"cluster_0_(2020)     Modelo LSTM treinado com dados de 2012 a 2018, validado\\u003cbr\\u003ecom os dados de 2019 e testado com os dados de 2020.     Para esse\\u003cbr\\u003etreinamento foi utilizado o dataset V37, dividido em 5 Clusters.     Para\\u003cbr\\u003eesse treinamento, vamos usar o dataset novo onde incluimos a altitude que foi\\u003cbr\\u003ecategorizada em baixa, media, alta e muito alta. foi incluido também o preço\\u003cbr\\u003emedio do café.      O modelo foi treinado com as seguintes configurações:\\u003cbr\\u003einput_size: 6      encoder_n_layers = 4     learning_rate:\\u003cbr\\u003e0.0017184411036225246     encoder_hidden_size: 192     decoder_layers: 1\\u003cbr\\u003edecoder_hidden_size: 64     batch_size: 32     dropout: 0.3     weight_decay:\\u003cbr\\u003e0.0001     steps: 1000\"],\"hovertemplate\":\"\\u003cb\\u003eAno: %{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003eModelo: Estimativa SEBRAE - Altitude Ajustado\\u003cbr\\u003eValor: %{y} Ton\\u002fha\\u003cbr\\u003e\\u003cbr\\u003e%{customdata}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"marker\":{\"color\":\"#B3A7CC\"},\"name\":\"Estimativa SEBRAE - Altitude Ajustado\",\"text\":[1.76,1.93,1.64,1.1,1.39,1.5],\"textposition\":\"outside\",\"x\":[2020,2021,2022,2023,2024,2025],\"y\":[1.76,1.93,1.64,1.1,1.39,1.5],\"type\":\"bar\"},{\"line\":{\"color\":\"#8EB38E\",\"dash\":\"dot\"},\"mode\":\"lines+markers+text\",\"name\":\"Diferença % (Estimativa SEBRAE - Altitude)\",\"text\":[\"18.4%\",\"31.9%\",\"61.8%\",\"-21.7%\",\"1.3%\"],\"textfont\":{\"size\":10},\"textposition\":\"top center\",\"x\":[2020,2021,2022,2023,2024],\"y\":[18.3673469387755,31.851851851851848,61.76470588235294,-21.739130434782613,1.3071895424836613],\"yaxis\":\"y2\",\"type\":\"scatter\"},{\"line\":{\"color\":\"#B3A7CC\",\"dash\":\"dot\"},\"mode\":\"lines+markers+text\",\"name\":\"Diferença % (Estimativa SEBRAE - Altitude Ajustado)\",\"text\":[\"-10.2%\",\"43.0%\",\"20.6%\",\"-31.7%\",\"-9.2%\"],\"textfont\":{\"size\":10},\"textposition\":\"top center\",\"x\":[2020,2021,2022,2023,2024],\"y\":[-10.20408163265306,42.96296296296295,20.58823529411763,-31.67701863354037,-9.15032679738563],\"yaxis\":\"y2\",\"type\":\"scatter\"},{\"line\":{\"color\":\"firebrick\",\"dash\":\"dot\"},\"mode\":\"lines+markers+text\",\"name\":\"Diferença % (Produtividade CONAB (Est.))\",\"text\":[\"-8.7%\",\"-7.4%\",\"16.9%\",\"-4.3%\",\"7.8%\"],\"textfont\":{\"size\":10},\"textposition\":\"top center\",\"x\":[2020,2021,2022,2023,2024],\"y\":[-8.6734693877551,-7.407407407407414,16.91176470588235,-4.347826086956525,7.843137254901953],\"yaxis\":\"y2\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"title\":{\"text\":\"\\u003cb\\u003eAno\\u003c\\u002fb\\u003e\"},\"type\":\"category\"},\"legend\":{\"orientation\":\"h\",\"yanchor\":\"bottom\",\"y\":-0.6,\"xanchor\":\"center\",\"x\":0.5},\"margin\":{\"b\":250},\"yaxis2\":{\"title\":{\"text\":\"\\u003cb\\u003eDiferença (%)\\u003c\\u002fb\\u003e\"},\"overlaying\":\"y\",\"side\":\"right\",\"showgrid\":false},\"title\":{\"text\":\"\\u003cb\\u003eProdutividade de Café (Ton\\u002fha) - Período de Teste\\u003c\\u002fb\\u003e\"},\"yaxis\":{\"title\":{\"text\":\"\\u003cb\\u003eProdutividade (Ton\\u002fha)\\u003c\\u002fb\\u003e\"}},\"barmode\":\"group\",\"height\":700},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('13292d4c-2e1f-49c6-89e5-dc3d42606a39');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "\n",
        "# Dados de 2020 - 2026\n",
        "data = {\n",
        "    'Safra Real IBGE': [33996367, 22340050, 22818800, 28483900, 27641233, 0, 0],\n",
        "    'Estimativa CONAB': [31074550, 20661300, 26687400, 27101900, 29836400, 24703900, 0],\n",
        "    'Estimativa SEBRAE': [31329453, 27971152, 21549294, 27672381, 30735952, 25470456, 0],\n",
        "    'Estimativa SEBRAE - Predições Meteorologicas': [33911478, 27052299, 34348700, 18978972, 23485670, 25745075, 25626520] # pandemia_base_Hmax\n",
        "}\n",
        "\n",
        "# Anos correspondentes aos dados.\n",
        "index = [2020, 2021, 2022, 2023, 2024, 2025, 2026]\n",
        "\n",
        "colors = {\n",
        "    'Safra Real IBGE': 'darkblue',\n",
        "    'Estimativa CONAB': 'red',\n",
        "    'Estimativa SEBRAE': 'green',\n",
        "    #'Estimativa SEBRAE - Predições Meteorologicas': 'yellow'\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# --- Definição dos Dados ---\n",
        "# Os dados fornecidos são colocados num dicionário.\n",
        "data = {\n",
        "    'IBGE': [28483900, 27641233, 0, 0],\n",
        "    'CONAB': [27101900, 29836400, 24703900, 0],\n",
        "    'SEBRAE': [27672381, 30735952, 25745075, 25626520],\n",
        "}\n",
        "\n",
        "# Anos correspondentes aos dados.\n",
        "index = [2023, 2024, 2025, 2026]\n",
        "\n",
        "# Criação de um DataFrame do Pandas para facilitar a manipulação.\n",
        "df = pd.DataFrame(data, index=index)\n",
        "\n",
        "# --- Criação do Gráfico ---\n",
        "# Inicializa a figura do gráfico.\n",
        "fig = go.Figure()\n",
        "\n",
        "# Define as cores para cada estimativa, conforme solicitado.\n",
        "colors = {\n",
        "    'IBGE': 'darkblue',\n",
        "    'CONAB': 'red',\n",
        "    'SEBRAE': 'green',\n",
        "}\n",
        "\n",
        "# Adiciona uma barra ao gráfico para cada coluna de dados.\n",
        "for col in df.columns:\n",
        "    fig.add_trace(go.Bar(\n",
        "        x=df.index,\n",
        "        y=df[col],\n",
        "        name=col,\n",
        "        marker_color=colors[col],\n",
        "        text=df[col].apply(lambda x: f'{x:,.0f}'.replace(',', '.')), # Formata o texto para exibição\n",
        "        textposition='auto',\n",
        "    ))\n",
        "\n",
        "# --- Personalização do Layout ---\n",
        "# Atualiza o layout do gráfico para corresponder ao design de referência.\n",
        "fig.update_layout(\n",
        "    title_text='<b>Comparativo de Estimativas de Safra</b>',\n",
        "    xaxis_title='Ano',\n",
        "    yaxis_title='Produção em Sacas',\n",
        "    barmode='group',  # Agrupa as barras por ano.\n",
        "    plot_bgcolor='white',  # Fundo branco.\n",
        "    xaxis=dict(\n",
        "        showline=True,\n",
        "        linewidth=1,\n",
        "        linecolor='lightgray'\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        showgrid=True,\n",
        "        gridcolor='lightgray'\n",
        "    ),\n",
        "    legend=dict(\n",
        "        orientation=\"h\",  # Legenda horizontal.\n",
        "        yanchor=\"bottom\",\n",
        "        y=1.02,\n",
        "        xanchor=\"right\",\n",
        "        x=1\n",
        "    ),\n",
        "    font=dict(\n",
        "        family=\"Arial, sans-serif\",\n",
        "        size=12,\n",
        "        color=\"black\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# --- Exibição e Exportação ---\n",
        "# Mostra o gráfico interativo.\n",
        "fig.show()\n",
        "\n",
        "# Opcional: Salva o gráfico como um ficheiro HTML interativo.\n",
        "# fig.write_html('comparativo_estimativas_safra.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "O1M3UjUwZVpg",
        "outputId": "d586a5ce-af5a-42ff-cd53-c83663ffe3bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f6789ece-e9cf-4938-a52f-4c4550741c73\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f6789ece-e9cf-4938-a52f-4c4550741c73\")) {                    Plotly.newPlot(                        \"f6789ece-e9cf-4938-a52f-4c4550741c73\",                        [{\"marker\":{\"color\":\"darkblue\"},\"name\":\"IBGE\",\"text\":[\"28.483.900\",\"27.641.233\",\"0\",\"0\"],\"textposition\":\"auto\",\"x\":[2023,2024,2025,2026],\"y\":[28483900,27641233,0,0],\"type\":\"bar\"},{\"marker\":{\"color\":\"red\"},\"name\":\"CONAB\",\"text\":[\"27.101.900\",\"29.836.400\",\"24.703.900\",\"0\"],\"textposition\":\"auto\",\"x\":[2023,2024,2025,2026],\"y\":[27101900,29836400,24703900,0],\"type\":\"bar\"},{\"marker\":{\"color\":\"green\"},\"name\":\"SEBRAE\",\"text\":[\"27.672.381\",\"30.735.952\",\"25.745.075\",\"25.626.520\"],\"textposition\":\"auto\",\"x\":[2023,2024,2025,2026],\"y\":[27672381,30735952,25745075,25626520],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"\\u003cb\\u003eComparativo de Estimativas de Safra\\u003c\\u002fb\\u003e\"},\"xaxis\":{\"title\":{\"text\":\"Ano\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"lightgray\"},\"yaxis\":{\"title\":{\"text\":\"Produção em Sacas\"},\"showgrid\":true,\"gridcolor\":\"lightgray\"},\"legend\":{\"orientation\":\"h\",\"yanchor\":\"bottom\",\"y\":1.02,\"xanchor\":\"right\",\"x\":1},\"font\":{\"family\":\"Arial, sans-serif\",\"size\":12,\"color\":\"black\"},\"barmode\":\"group\",\"plot_bgcolor\":\"white\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f6789ece-e9cf-4938-a52f-4c4550741c73');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}