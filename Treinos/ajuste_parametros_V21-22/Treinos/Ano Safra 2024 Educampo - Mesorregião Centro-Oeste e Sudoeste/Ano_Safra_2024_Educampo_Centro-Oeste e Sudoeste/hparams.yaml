alias: null
batch_size: 32
context_size: null
dataloader_kwargs: null
decoder_hidden_size: 256
decoder_layers: 3
drop_last_loader: false
early_stop_patience_steps: -1
encoder_bias: true
encoder_dropout: 0.4
encoder_hidden_size: 192
encoder_n_layers: 6
exclude_insample_y: false
futr_exog_list:
- altitude
- "\xC1rea destinada \xE0 colheita (Hectares)"
- precipitacao (mm) 01
- precipitacao (mm) 02
- precipitacao (mm) 03
- precipitacao (mm) 04
- precipitacao (mm) 05
- precipitacao (mm) 06
- precipitacao (mm) 07
- precipitacao (mm) 08
- precipitacao (mm) 09
- precipitacao (mm) 10
- precipitacao (mm) 11
- precipitacao (mm) 12
- "temperatura maxima (\xBAC) 01"
- "temperatura maxima (\xBAC) 02"
- "temperatura maxima (\xBAC) 03"
- "temperatura maxima (\xBAC) 04"
- "temperatura maxima (\xBAC) 05"
- "temperatura maxima (\xBAC) 06"
- "temperatura maxima (\xBAC) 07"
- "temperatura maxima (\xBAC) 08"
- "temperatura maxima (\xBAC) 09"
- "temperatura maxima (\xBAC) 10"
- "temperatura maxima (\xBAC) 11"
- "temperatura maxima (\xBAC) 12"
- "temperatura minima (\xBAC) 01"
- "temperatura minima (\xBAC) 02"
- "temperatura minima (\xBAC) 03"
- "temperatura minima (\xBAC) 04"
- "temperatura minima (\xBAC) 05"
- "temperatura minima (\xBAC) 06"
- "temperatura minima (\xBAC) 07"
- "temperatura minima (\xBAC) 08"
- "temperatura minima (\xBAC) 09"
- "temperatura minima (\xBAC) 10"
- "temperatura minima (\xBAC) 11"
- "temperatura minima (\xBAC) 12"
h: 1
h_train: 1
hist_exog_list: null
inference_input_size: null
inference_windows_batch_size: 1024
input_size: 3
learning_rate: 0.00015738037481650337
logger: !!python/object:pytorch_lightning.loggers.csv_logs.CSVLogger
  _experiment: null
  _flush_logs_every_n_steps: 100
  _fs: !!python/object/apply:fsspec.spec.make_instance
  - !!python/name:fsspec.implementations.local.LocalFileSystem ''
  - !!python/tuple []
  - {}
  _name: logs
  _prefix: ''
  _root_dir: ./Logs
  _save_dir: ./Logs
  _version: null
loss: !!python/object:neuralforecast.losses.pytorch.HuberLoss
  _backward_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _backward_pre_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _buffers: {}
  _forward_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _forward_hooks_always_called: !!python/object/apply:collections.OrderedDict
  - []
  _forward_hooks_with_kwargs: !!python/object/apply:collections.OrderedDict
  - []
  _forward_pre_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _forward_pre_hooks_with_kwargs: !!python/object/apply:collections.OrderedDict
  - []
  _is_full_backward_hook: null
  _load_state_dict_post_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _load_state_dict_pre_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _modules: {}
  _non_persistent_buffers_set: !!set {}
  _parameters: {}
  _state_dict_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _state_dict_pre_hooks: !!python/object/apply:collections.OrderedDict
  - []
  delta: 1.0
  horizon_weight: null
  is_distribution_output: false
  output_names:
  - ''
  outputsize_multiplier: 1
  training: true
lr_scheduler: !!python/name:torch.optim.lr_scheduler.StepLR ''
lr_scheduler_kwargs:
  gamma: 0.1
  step_size: 500
max_steps: 1000
n_samples: 100
n_series: 1
num_lr_decays: -1
optimizer: !!python/name:torch.optim.adamw.AdamW ''
optimizer_kwargs:
  weight_decay: 1.0e-05
random_seed: 1
recurrent: false
scaler_type: revin
start_padding_enabled: false
stat_exog_list: null
step_size: 1
val_check_steps: 100
valid_batch_size: null
valid_loss: null
windows_batch_size: 128
