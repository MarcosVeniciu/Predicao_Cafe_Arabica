treino_id,unique_id,ds,y,y_pred,diferença_%,flag,dataset,modelo,comentario,data_treino
V42_cluster_0_2020,Aguanil,2019-12-31T00:00:00,1.8000000000000005,1.447041,-19.60883140563966,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Alfenas,2019-12-31T00:00:00,1.7572173913043478,2.471499,40.6484467116784,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Alto_Caparao,2019-12-31T00:00:00,1.32,1.3779229,4.388097922007238,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Alto_Jequitiba,2019-12-31T00:00:00,1.5,1.439138,-4.057463010152182,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Amparo_do_Serra,2019-12-31T00:00:00,1.260869565217391,1.3657398,8.31729625833449,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Araponga,2019-12-31T00:00:00,0.9,1.3032448,44.80498101976183,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Astolfo_Dutra,2019-12-31T00:00:00,1.2,1.1632987,-3.0584394931793177,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Bambui,2019-12-31T00:00:00,1.26,1.8269261,44.99413588690379,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Boa_Esperanca,2019-12-31T00:00:00,1.92,1.8325983,-4.552170385917025,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Bom_Jesus_do_Galho,2019-12-31T00:00:00,1.080232558139535,1.5950699,47.65986020649724,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Bom_Sucesso,2019-12-31T00:00:00,1.5,1.4206924,-5.28717041015625,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Caete,2019-12-31T00:00:00,1.3125,1.5100865,15.054212297712052,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Caiana,2019-12-31T00:00:00,1.080049875311721,1.2081497,11.860544514749629,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Cajuri,2019-12-31T00:00:00,1.184993531694696,1.4826568,25.11940334301328,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Camacho,2019-12-31T00:00:00,1.5,1.5000942,0.0062783559163411,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Campo_do_Meio,2019-12-31T00:00:00,1.619877049180328,1.9280387,19.02376895158973,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Campos_Altos,2019-12-31T00:00:00,1.199978657560559,2.0148456,67.90678708910572,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Campos_Gerais,2019-12-31T00:00:00,1.5913557213930347,1.9208721,20.706644430544028,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Cana_Verde,2019-12-31T00:00:00,1.2,1.3449138,12.076153357823694,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Canaa,2019-12-31T00:00:00,1.26,1.3668227,8.477993616982111,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Candeias,2019-12-31T00:00:00,1.5,1.6798002,11.986676851908369,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Caparao,2019-12-31T00:00:00,1.32,1.3442112,1.8341833894902964,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Capitolio,2019-12-31T00:00:00,1.500213219616205,1.7239523,14.913818306242463,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Caputira,2019-12-31T00:00:00,1.2,1.1938028,-0.5164305369059208,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Carangola,2019-12-31T00:00:00,1.26,1.2130389,-3.727069733634828,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Caratinga,2019-12-31T00:00:00,0.9960199004975124,2.0077724,101.57955123947096,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Carmo_da_Mata,2019-12-31T00:00:00,1.080487804878049,1.3199835,22.16551416883616,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Carmo_do_Rio_Claro,2019-12-31T00:00:00,1.937142857142857,2.0922024,8.004549963284388,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Carmopolis_de_Minas,2019-12-31T00:00:00,1.5,1.6336269,8.908462524414062,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Cassia,2019-12-31T00:00:00,1.563673469387755,1.9825202,26.786075320451708,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Catas_Altas_da_Noruega,2019-12-31T00:00:00,0.75,0.9232316,23.09754689534505,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Chale,2019-12-31T00:00:00,1.2,1.4059854,17.165446281433113,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Claudio,2019-12-31T00:00:00,1.44,1.2943718,-10.113066434860226,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Conceicao_da_Barra_de_Minas,2019-12-31T00:00:00,1.6656441717791413,1.8231664,9.457133839363102,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Conceicao_de_Ipanema,2019-12-31T00:00:00,1.185699039487727,1.5398061,29.864837219863176,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Coqueiral,2019-12-31T00:00:00,1.5,1.4905038,-0.6330808003743489,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Corrego_Danta,2019-12-31T00:00:00,1.5,1.785437,19.029132525126137,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Cristais,2019-12-31T00:00:00,1.739953810623557,1.7807122,2.342501049305014,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Desterro_de_Entre_Rios,2019-12-31T00:00:00,1.8000000000000005,1.7867442,-0.736431280771906,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Divinesia,2019-12-31T00:00:00,2.0,2.101852,5.092597007751489,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Divino,2019-12-31T00:00:00,1.079956453368816,1.2042326,11.507512155025644,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Doresopolis,2019-12-31T00:00:00,1.7400000000000002,1.6229808,-6.725239479678812,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Durande,2019-12-31T00:00:00,1.32,1.7713704,34.19472809993859,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Eloi_Mendes,2019-12-31T00:00:00,1.5,2.2548044,50.32029151916504,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Entre_Folhas,2019-12-31T00:00:00,0.8400000000000001,1.0629221,26.53834763027372,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Entre_Rios_de_Minas,2019-12-31T00:00:00,1.921428571428572,2.072692,7.872441798780939,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ervalia,2019-12-31T00:00:00,1.199920191540303,1.5129092,26.08414998195639,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Espera_Feliz,2019-12-31T00:00:00,0.9,1.5180949,68.6772108078003,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Faria_Lemos,2019-12-31T00:00:00,1.0796875,1.1724591,8.592451567587744,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ferros,2019-12-31T00:00:00,0.8363636363636363,1.0239642,22.430498185365103,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Fervedouro,2019-12-31T00:00:00,1.079947575360419,1.2047783,11.558962779716412,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Guape,2019-12-31T00:00:00,1.6177358490566045,2.1120088,30.55337874072396,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ibituruna,2019-12-31T00:00:00,1.6599999999999997,1.4549732,-12.351010793662915,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ilicinea,2019-12-31T00:00:00,1.8601303639326447,2.2114637,18.88756469351168,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Itapecerica,2019-12-31T00:00:00,1.2,1.4289267,19.077225526173912,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ituiutaba,2019-12-31T00:00:00,1.076923076923077,1.199423,11.374988726207173,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Itumirim,2019-12-31T00:00:00,1.601593625498008,1.4427269,-9.919293662208831,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Lajinha,2019-12-31T00:00:00,1.055939660590823,1.4872656,40.84759218352188,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Lavras,2019-12-31T00:00:00,1.5,1.5860438,5.736255645751953,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Luisburgo,2019-12-31T00:00:00,1.6399999999999997,1.4577966,-11.1099650220173,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Machado,2019-12-31T00:00:00,1.5270920664493093,2.0753393,35.90138819509522,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Manhuacu,2019-12-31T00:00:00,1.019980879541109,1.3003576,27.48842711345428,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Manhumirim,2019-12-31T00:00:00,1.2,1.5489911,29.082590341568,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Martins_Soares,2019-12-31T00:00:00,1.6199186991869925,1.6736015,3.313919950458879,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Matipo,2019-12-31T00:00:00,1.32,1.2535645,-5.032994169177436,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Medeiros,2019-12-31T00:00:00,1.199936224489796,2.1596227,79.9781209320709,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Miradouro,2019-12-31T00:00:00,1.32,1.4057631,6.497208277384435,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Mirai,2019-12-31T00:00:00,1.2,1.0269035,-14.424707492192583,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Moeda,2019-12-31T00:00:00,1.0,1.0564775,5.647754669189453,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Muriae,2019-12-31T00:00:00,1.02054794520548,1.2273456,20.26339297326613,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Mutum,2019-12-31T00:00:00,1.4430666666666667,1.5543562,7.712017281553519,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Nazareno,2019-12-31T00:00:00,1.680203045685279,1.671465,-0.5200565761669778,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Nepomuceno,2019-12-31T00:00:00,1.5,1.517892,1.192800203959147,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Oliveira,2019-12-31T00:00:00,1.56,1.542738,-1.1065409733698952,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Orizania,2019-12-31T00:00:00,1.379912663755459,1.432512,3.81179059608069,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Paraguacu,2019-12-31T00:00:00,1.5599709934735322,2.1627927,38.64313450033396,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Passa_Tempo,2019-12-31T00:00:00,1.5,1.5899558,5.997053782145182,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Perdoes,2019-12-31T00:00:00,1.7399328859060397,1.5662113,-9.98438184033706,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Pimenta,2019-12-31T00:00:00,1.5,2.1406355,42.7090326944987,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Piranga,2019-12-31T00:00:00,2.1,1.7005962,-19.019227936154323,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Piumhi,2019-12-31T00:00:00,1.3199752300070773,1.7963605,36.0904698716708,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ponte_Nova,2019-12-31T00:00:00,1.166666666666667,1.3441513,15.21296501159665,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Porto_Firme,2019-12-31T00:00:00,1.119607843137255,1.2360793,10.402882829856535,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Raul_Soares,2019-12-31T00:00:00,1.2,1.0501761,-12.48532136281331,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Reduto,2019-12-31T00:00:00,1.259851301115242,1.4147612,12.29588637910572,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ribeirao_Vermelho,2019-12-31T00:00:00,1.5,1.4892764,-0.7149060567220051,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ritapolis,2019-12-31T00:00:00,1.8008849557522122,1.7459483,-3.0505358440577144,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Rosario_da_Limeira,2019-12-31T00:00:00,1.2,1.1668166,-2.7652839819590214,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Santa_Barbara_do_Leste,2019-12-31T00:00:00,1.139892904953146,1.2936662,13.490156656983157,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Santa_Margarida,2019-12-31T00:00:00,1.2,1.3858502,15.48751592636109,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Santa_Rita_de_Minas,2019-12-31T00:00:00,1.26,1.5324887,21.626087597438268,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Santa_Rita_do_Itueto,2019-12-31T00:00:00,1.649769585253456,1.2114129,-26.57078191554745,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Santana_da_Vargem,2019-12-31T00:00:00,1.7999440402909912,1.6730006,-7.052634045141214,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Santana_do_Jacare,2019-12-31T00:00:00,1.2,1.3700814,14.173452059427902,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Santana_do_Manhuacu,2019-12-31T00:00:00,1.5,1.4045318,-6.36454423268636,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Domingos_das_Dores,2019-12-31T00:00:00,1.296118830857691,1.6621358,28.23946384317108,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Francisco_de_Paula,2019-12-31T00:00:00,1.6399999999999997,1.3995134,-14.663819278158776,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Francisco_do_Gloria,2019-12-31T00:00:00,0.9,1.2021735,33.57483016120062,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Joao_do_Manhuacu,2019-12-31T00:00:00,1.320060560181681,1.4149909,7.19136262599006,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Jose_do_Mantimento,2019-12-31T00:00:00,1.56039603960396,1.3610551,-12.775019855063558,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Roque_de_Minas,2019-12-31T00:00:00,1.5000955109837628,1.5893157,5.947630748192603,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Tiago,2019-12-31T00:00:00,1.679886685552408,1.8730749,11.50007350448814,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Tomas_de_Aquino,2019-12-31T00:00:00,1.560034623469766,2.1365666,36.95636025016342,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Senador_Firmino,2019-12-31T00:00:00,1.32258064516129,1.735506,31.22118973150489,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Senhora_de_Oliveira,2019-12-31T00:00:00,1.6625,1.5380814,-7.4838251099550686,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sericita,2019-12-31T00:00:00,0.9,1.2718241,41.31379127502441,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Simonesia,2019-12-31T00:00:00,1.32,1.3108493,-0.6932341691219492,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Tapirai,2019-12-31T00:00:00,1.679835390946502,1.8651762,11.033272124107771,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Tombos,2019-12-31T00:00:00,0.96,1.1091955,15.541194876035073,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Tres_Pontas,2019-12-31T00:00:00,1.7400000000000002,2.0352988,16.971196799442673,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ubaporanga,2019-12-31T00:00:00,1.320030895983522,1.2401513,-6.051343957280839,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Vargem_Bonita,2019-12-31T00:00:00,2.1,2.0977654,-0.1064073471795987,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Vermelho_Novo,2019-12-31T00:00:00,1.079912663755459,1.1890302,10.10428994375826,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Vicosa,2019-12-31T00:00:00,0.8395833333333333,1.5523217,84.89191135756727,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Vieiras,2019-12-31T00:00:00,1.019736842105263,1.3514502,32.52931041102257,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Aguanil,2020-12-31T00:00:00,2.1644859813084114,2.79361,29.06575141175422,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Alfenas,2020-12-31T00:00:00,2.364130434782609,2.889643,22.22857552013177,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Alto_Caparao,2020-12-31T00:00:00,1.92,1.5906849,-17.15182860692342,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Alto_Jequitiba,2020-12-31T00:00:00,1.8000000000000005,1.6568484,-7.952864964803073,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Amparo_do_Serra,2020-12-31T00:00:00,1.4392156862745098,1.6915522,17.532915900123864,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Araponga,2020-12-31T00:00:00,1.8000000000000005,1.5233366,-15.370186169942231,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Astolfo_Dutra,2020-12-31T00:00:00,1.75,1.5504589,-11.402348109654016,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Bambui,2020-12-31T00:00:00,1.7998829724985372,1.892804,5.162616432565701,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Boa_Esperanca,2020-12-31T00:00:00,2.063248407643312,2.966232,43.76514482685117,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Bom_Jesus_do_Galho,2020-12-31T00:00:00,1.44,1.7659206,22.633377710978195,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Bom_Sucesso,2020-12-31T00:00:00,2.1,1.9934901,-5.071900004432319,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Caete,2020-12-31T00:00:00,1.2,1.823717,51.976416508356735,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Caiana,2020-12-31T00:00:00,1.5,1.4457159,-3.618939717610677,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Cajuri,2020-12-31T00:00:00,1.8000000000000005,1.8506383,2.8132372432284733,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Camacho,2020-12-31T00:00:00,2.1,2.843749,35.41662125360398,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Campo_do_Meio,2020-12-31T00:00:00,2.4,2.9354732,22.311383485794074,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Campos_Altos,2020-12-31T00:00:00,2.1000529941706416,2.303435,9.684617178608775,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Campos_Gerais,2020-12-31T00:00:00,2.690649114843395,2.653056,-1.3971798976027594,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Cana_Verde,2020-12-31T00:00:00,1.5,1.5473067,3.1537771224975586,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Canaa,2020-12-31T00:00:00,1.8000000000000005,1.7499598,-2.780009640587715,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Candeias,2020-12-31T00:00:00,1.859937013094646,2.874329,54.53905534875998,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Caparao,2020-12-31T00:00:00,1.9199600798403191,1.680766,-12.458284727339056,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Capitolio,2020-12-31T00:00:00,2.1,2.0439792,-2.667658669607984,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Caputira,2020-12-31T00:00:00,1.8000000000000005,1.299598,-27.80011230044896,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Carangola,2020-12-31T00:00:00,1.5,1.6854967,12.366445859273274,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Caratinga,2020-12-31T00:00:00,2.040022111663903,2.0848231,2.196104622652116,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Carmo_da_Mata,2020-12-31T00:00:00,1.68,1.500217,-10.701371374584376,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Carmo_do_Rio_Claro,2020-12-31T00:00:00,2.7847803881511743,2.7389328,-1.6463610692449655,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Carmopolis_de_Minas,2020-12-31T00:00:00,1.981818181818182,2.1223292,7.090007274522689,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Cassia,2020-12-31T00:00:00,2.1679245283018864,2.4563856,13.305863761403872,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Catas_Altas_da_Noruega,2020-12-31T00:00:00,1.0,1.1691443,16.914427280426025,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Chale,2020-12-31T00:00:00,1.71015625,1.4240701,-16.728654480677537,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Claudio,2020-12-31T00:00:00,2.76,1.8615702,-32.55180293235225,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Conceicao_da_Barra_de_Minas,2020-12-31T00:00:00,1.911042944785276,2.857356,49.518150770453744,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Conceicao_de_Ipanema,2020-12-31T00:00:00,1.500533617929563,1.6953822,12.985288511807983,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Coqueiral,2020-12-31T00:00:00,2.160028248587571,2.0229788,-6.344799704512116,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Corrego_Danta,2020-12-31T00:00:00,1.8000000000000005,1.9178239,6.54577281739975,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Cristais,2020-12-31T00:00:00,1.92,3.0088303,56.70991192261378,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Desterro_de_Entre_Rios,2020-12-31T00:00:00,2.4,2.860474,19.186421235402427,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Divinesia,2020-12-31T00:00:00,2.1,3.5998383,71.42086937313988,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Divino,2020-12-31T00:00:00,1.8000483851457605,1.4742286,-18.100611478296567,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Doresopolis,2020-12-31T00:00:00,1.8000000000000005,2.1333578,18.519875738355832,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Durande,2020-12-31T00:00:00,2.4,1.8298868,-23.754716912905373,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Eloi_Mendes,2020-12-31T00:00:00,2.109128416709644,2.586514,22.634258570152475,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Entre_Folhas,2020-12-31T00:00:00,1.26046511627907,1.2596046,-0.0682718639444422,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Entre_Rios_de_Minas,2020-12-31T00:00:00,1.8000000000000005,2.396493,33.13849767049152,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ervalia,2020-12-31T00:00:00,1.7999614197530858,1.8317658,1.7669462695176812,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Espera_Feliz,2020-12-31T00:00:00,2.04,1.7716995,-13.15198295256671,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Faria_Lemos,2020-12-31T00:00:00,1.44031007751938,1.4375727,-0.1900535096906726,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ferros,2020-12-31T00:00:00,1.2,1.1201239,-6.65634473164876,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Fervedouro,2020-12-31T00:00:00,1.5,1.2399123,-17.339181900024414,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Guape,2020-12-31T00:00:00,1.847986942328618,2.4419246,32.13970921861265,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ibituruna,2020-12-31T00:00:00,1.8000000000000005,1.9014001,5.633338292439763,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ilicinea,2020-12-31T00:00:00,2.09995817649519,3.093599,47.31717591645639,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Itapecerica,2020-12-31T00:00:00,1.8000000000000005,2.4795296,37.75164551205103,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ituiutaba,2020-12-31T00:00:00,0.75,1.8377373,145.03164291381836,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Itumirim,2020-12-31T00:00:00,2.050724637681159,1.9649942,-4.180495225077372,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Lajinha,2020-12-31T00:00:00,1.9200446677833607,1.5188698,-20.894040480538585,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Lavras,2020-12-31T00:00:00,1.8000000000000005,2.174867,20.825939708285844,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Luisburgo,2020-12-31T00:00:00,2.1,1.8894964,-10.023978778294158,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Machado,2020-12-31T00:00:00,2.211839166046165,2.0893478,-5.5379852464435375,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Manhuacu,2020-12-31T00:00:00,1.6800182481751822,1.4578741,-13.22272473762406,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Manhumirim,2020-12-31T00:00:00,1.68,1.6464643,-1.9961697714669329,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Martins_Soares,2020-12-31T00:00:00,1.8000000000000005,1.7441769,-3.101285298665379,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Matipo,2020-12-31T00:00:00,1.56,1.4204122,-8.947936999492159,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Medeiros,2020-12-31T00:00:00,1.980113636363636,2.4678493,24.631698367585447,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Miradouro,2020-12-31T00:00:00,1.5,1.4498186,-3.3454259236653647,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Mirai,2020-12-31T00:00:00,1.5011494252873558,1.1970944,-20.254811393539143,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Moeda,2020-12-31T00:00:00,1.2,1.8754843,56.29036227862041,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Muriae,2020-12-31T00:00:00,1.56,1.2236873,-21.558506977863804,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Mutum,2020-12-31T00:00:00,1.65,1.6588291,0.535096544207954,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Nazareno,2020-12-31T00:00:00,2.1,2.090819,-0.4371960957845094,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Nepomuceno,2020-12-31T00:00:00,1.920040743570155,1.9881859,3.5491506743494527,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Oliveira,2020-12-31T00:00:00,1.947151898734177,2.1722102,11.558333889325633,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Orizania,2020-12-31T00:00:00,1.679901960784314,1.7322408,3.1155886787835163,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Paraguacu,2020-12-31T00:00:00,2.3413259668508286,2.480291,5.935308660847198,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Passa_Tempo,2020-12-31T00:00:00,1.511111111111111,2.021536,33.77812504768372,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Perdoes,2020-12-31T00:00:00,1.92,2.0146668,4.930562277634943,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Pimenta,2020-12-31T00:00:00,1.8000000000000005,2.8288677,57.15931521521672,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Piranga,2020-12-31T00:00:00,2.4,3.084988,28.54117155075074,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Piumhi,2020-12-31T00:00:00,1.7399966931216928,2.1852934,25.591815472351527,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ponte_Nova,2020-12-31T00:00:00,1.333333333333333,1.4813958,11.104688048362757,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Porto_Firme,2020-12-31T00:00:00,1.56,1.3093526,-16.06713869632819,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Raul_Soares,2020-12-31T00:00:00,1.320083682008368,1.2996101,-1.550927744047744,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Reduto,2020-12-31T00:00:00,1.8000000000000005,1.6978059,-5.677451027764229,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ribeirao_Vermelho,2020-12-31T00:00:00,1.92,1.7505643,-8.824774126211798,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ritapolis,2020-12-31T00:00:00,2.101769911504425,1.9692702,-6.304195404052737,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Rosario_da_Limeira,2020-12-31T00:00:00,1.5,1.20159,-19.894003868103027,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Santa_Barbara_do_Leste,2020-12-31T00:00:00,1.4400953029271608,1.6314402,13.286958116077455,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Santa_Margarida,2020-12-31T00:00:00,1.8000000000000005,1.4861325,-17.437083191341838,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Santa_Rita_de_Minas,2020-12-31T00:00:00,1.5,1.6583109,10.554059346516926,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Santa_Rita_do_Itueto,2020-12-31T00:00:00,1.77,1.6186275,-8.552115919899807,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Santana_da_Vargem,2020-12-31T00:00:00,2.22,2.520966,13.557029414821328,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Santana_do_Jacare,2020-12-31T00:00:00,1.8000000000000005,1.8228184,1.2676888042025949,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Santana_do_Manhuacu,2020-12-31T00:00:00,1.8000000000000005,1.5603776,-13.312355677286796,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Domingos_das_Dores,2020-12-31T00:00:00,1.56,1.7005022,9.006548539186133,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Francisco_de_Paula,2020-12-31T00:00:00,1.62,2.5013266,54.40287413420499,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Francisco_do_Gloria,2020-12-31T00:00:00,1.679693486590038,1.4644822,-12.81253395833236,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Joao_do_Manhuacu,2020-12-31T00:00:00,1.8000000000000005,1.6529706,-8.168302641974568,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Jose_do_Mantimento,2020-12-31T00:00:00,1.8000000000000005,1.6025536,-10.969244109259725,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Roque_de_Minas,2020-12-31T00:00:00,1.8000000000000005,1.9188651,6.6036158137851135,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Tiago,2020-12-31T00:00:00,2.69971671388102,2.8520057,5.640925415415319,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sao_Tomas_de_Aquino,2020-12-31T00:00:00,2.099966151415999,2.841271,35.300796238960515,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Senador_Firmino,2020-12-31T00:00:00,1.376237623762376,2.2599914,64.21520298333479,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Senhora_de_Oliveira,2020-12-31T00:00:00,1.8000000000000005,2.4315283,35.08490721384682,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Sericita,2020-12-31T00:00:00,2.5200000000000005,1.6312792,-35.26669721754772,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Simonesia,2020-12-31T00:00:00,1.8000000000000005,1.353894,-24.783666928609225,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Tapirai,2020-12-31T00:00:00,1.799716914366596,1.8938371,5.2297213627886014,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Tombos,2020-12-31T00:00:00,1.5,1.2267395,-18.217364947001137,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Tres_Pontas,2020-12-31T00:00:00,2.26,2.983074,31.99442255813465,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Ubaporanga,2020-12-31T00:00:00,1.6200873362445412,1.3662423,-15.668602613104628,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Vargem_Bonita,2020-12-31T00:00:00,1.8031222896790973,3.214006,78.24669826942436,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Vermelho_Novo,2020-12-31T00:00:00,1.320080321285141,1.4618539,10.739766227680343,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Vicosa,2020-12-31T00:00:00,1.8000000000000005,1.8983573,5.464292897118447,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_0_2020,Vieiras,2020-12-31T00:00:00,1.8000000000000005,1.334677,-25.851278834872787,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:32:32
V42_cluster_1_2020,Almenara,2019-12-31T00:00:00,0.7857142857142858,1.2506096363067627,59.16849916631524,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Alpinopolis,2019-12-31T00:00:00,2.1000392310710083,3.087964534759521,47.04318324494714,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Alterosa,2019-12-31T00:00:00,1.739944346066279,2.2614338397979736,29.971619202113832,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Andradas,2019-12-31T00:00:00,1.62,1.7342135906219482,7.050221643330132,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Araguari,2019-12-31T00:00:00,1.680014528284754,3.687469005584717,119.49030460763429,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Arapua,2019-12-31T00:00:00,1.2,2.0330731868743896,69.42276557286581,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Araxa,2019-12-31T00:00:00,1.387894736842105,2.155083417892456,55.27715183904691,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Arceburgo,2019-12-31T00:00:00,1.5,2.4501328468322754,63.34218978881836,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Areado,2019-12-31T00:00:00,1.9231597845601445,1.837530255317688,-4.452543669534003,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Bom_Jesus_da_Penha,2019-12-31T00:00:00,2.219967266775777,2.266843318939209,2.111565015619059,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Botelhos,2019-12-31T00:00:00,1.92,2.4556691646575928,27.899435659249622,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Botumirim,2019-12-31T00:00:00,0.7199999999999999,1.028072953224182,42.78791017002532,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Cabo_Verde,2019-12-31T00:00:00,2.040029112081514,1.8908597230911253,-7.312120601954825,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Campestre,2019-12-31T00:00:00,1.5499999999999998,2.2185394763946533,43.13157912223571,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Capetinga,2019-12-31T00:00:00,2.04,3.725447654724121,82.61998307471181,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Carmo_do_Paranaiba,2019-12-31T00:00:00,1.793633952254642,2.4994468688964844,39.35100112007905,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Cascalho_Rico,2019-12-31T00:00:00,1.8000000000000005,2.611269950866699,45.07055282592771,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Claraval,2019-12-31T00:00:00,1.680106729914023,4.9739670753479,196.0506607578696,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Conceicao_da_Aparecida,2019-12-31T00:00:00,1.968025889967637,2.500786304473877,27.070803144515583,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Coromandel,2019-12-31T00:00:00,1.6463414634146338,2.8128662109375,70.85557725694449,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Corrego_Fundo,2019-12-31T00:00:00,1.804347826086957,4.318127632141113,139.31791696203754,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Cruzeiro_da_Fortaleza,2019-12-31T00:00:00,1.5,2.5776171684265137,71.84114456176758,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Delfinopolis,2019-12-31T00:00:00,1.558823529411765,2.1448543071746826,37.59442725271547,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Divisa_Nova,2019-12-31T00:00:00,1.799757281553398,2.420584917068481,34.49507563482325,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Dom_Cavati,2019-12-31T00:00:00,0.7826086956521741,2.220339298248291,183.7100214428372,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Dores_do_Turvo,2019-12-31T00:00:00,1.5,0.8388686776161194,-44.07542149225871,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Estrela_do_Indaia,2019-12-31T00:00:00,1.62,2.062781572341919,27.33219582357524,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Estrela_do_Sul,2019-12-31T00:00:00,1.679876706296786,3.058664083480835,82.07670074927847,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Guaranesia,2019-12-31T00:00:00,1.260049474335189,3.096252202987671,145.72465336103366,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Guarani,2019-12-31T00:00:00,1.8000000000000005,1.933730959892273,7.429497771792926,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Guarda-Mor,2019-12-31T00:00:00,1.7992125984251972,2.911417007446289,61.816175030931575,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Guaxupe,2019-12-31T00:00:00,1.32,2.2401514053344727,69.7084397980661,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Guimarania,2019-12-31T00:00:00,1.560154738878143,2.609669685363769,67.26992652282036,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Ibia,2019-12-31T00:00:00,1.199972848221559,1.898355960845948,58.19990957790747,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Ibiraci,2019-12-31T00:00:00,1.283972495924009,4.485022068023682,249.3082665136096,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Indianopolis,2019-12-31T00:00:00,1.8000000000000005,3.885324239730835,115.85134665171302,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Itamogi,2019-12-31T00:00:00,1.849947396107312,2.8338770866394043,53.18690102229353,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Itanhomi,2019-12-31T00:00:00,0.96,2.014352321624756,109.82836683591208,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Jacui,2019-12-31T00:00:00,1.68,2.182372570037842,29.90312916891916,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Jacutinga,2019-12-31T00:00:00,1.6801232665639447,1.57761812210083,-6.10104904223784,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Juruaia,2019-12-31T00:00:00,1.68,2.3286571502685547,38.61054465884254,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Lagoa_Dourada,2019-12-31T00:00:00,2.7000000000000006,2.892162561416626,7.117131904319455,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Lagoa_Formosa,2019-12-31T00:00:00,2.25,2.281911373138428,1.4182832505967882,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Lagoa_Grande,2019-12-31T00:00:00,1.2,2.458577871322632,104.881489276886,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Matutina,2019-12-31T00:00:00,1.8000000000000005,2.256274461746216,25.348581208123083,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Monte_Belo,2019-12-31T00:00:00,1.640077821011673,2.522862434387207,53.8257759519602,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Monte_Carmelo,2019-12-31T00:00:00,1.6200304645849195,3.570505142211914,120.3974073491723,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Monte_Santo_de_Minas,2019-12-31T00:00:00,1.589970501474926,2.686048746109009,68.9370176124219,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Muzambinho,2019-12-31T00:00:00,2.1,2.0016462802886963,-4.683510462443038,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Nova_Resende,2019-12-31T00:00:00,2.1150040551500404,2.0242903232574463,-4.2890571141793306,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Passos,2019-12-31T00:00:00,1.56,3.414781332015991,118.89623923179433,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Patis,2019-12-31T00:00:00,3.0,4.900295734405518,63.343191146850586,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Patos_de_Minas,2019-12-31T00:00:00,1.62,2.0416316986083984,26.02664806224681,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Patrocinio,2019-12-31T00:00:00,1.384841075794621,3.549644947052002,156.32146598592317,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Pedrinopolis,2019-12-31T00:00:00,1.5,4.0469841957092285,169.79894638061523,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Perdizes,2019-12-31T00:00:00,1.199035812672176,2.729163408279419,127.61316880078788,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Piracema,2019-12-31T00:00:00,1.166666666666667,1.763498306274414,51.15699768066403,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Ponto_dos_Volantes,2019-12-31T00:00:00,0.6000000000000001,0.6643970012664795,10.732833544413232,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Pratapolis,2019-12-31T00:00:00,1.738461538461539,3.834264039993286,120.55501115005625,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Pratinha,2019-12-31T00:00:00,1.7400000000000002,1.9724242687225344,13.357716593249078,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Presidente_Kubitschek,2019-12-31T00:00:00,1.315789473684211,0.7276228666305542,-44.7006621360779,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Presidente_Olegario,2019-12-31T00:00:00,1.8693227091633469,2.8321874141693115,51.50874700692607,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Quartel_Geral,2019-12-31T00:00:00,3.0,6.648229122161865,121.60763740539552,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Rio_Paranaiba,2019-12-31T00:00:00,1.142998097117485,2.620924711227417,129.30263119747093,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Rio_Vermelho,2019-12-31T00:00:00,1.2,1.3982102870941162,16.51752392450969,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Romaria,2019-12-31T00:00:00,2.2,3.3554811477661133,52.52187035300514,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Sacramento,2019-12-31T00:00:00,1.231428571428572,2.37937068939209,93.2203576072462,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Santa_Rosa_da_Serra,2019-12-31T00:00:00,1.44,1.6484678983688354,14.476937386724687,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Santo_Antonio_do_Amparo,2019-12-31T00:00:00,1.639983909895414,2.9714841842651367,81.18983767679987,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Sao_Goncalo_do_Abaete,2019-12-31T00:00:00,1.92,2.492040634155273,29.79378302892049,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Sao_Gotardo,2019-12-31T00:00:00,1.379944289693593,2.339017868041992,69.50089112375362,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Sao_Joao_Batista_do_Gloria,2019-12-31T00:00:00,1.68,2.4225783348083496,44.20109135763987,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Sao_Pedro_da_Uniao,2019-12-31T00:00:00,1.62,2.387834310531616,47.39717966244544,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Sao_Sebastiao_do_Paraiso,2019-12-31T00:00:00,1.470008496176721,2.139244079589844,45.5259670371775,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Serra_do_Salitre,2019-12-31T00:00:00,1.4904255319148938,4.039882659912109,171.05565312757906,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Tapira,2019-12-31T00:00:00,1.8000000000000005,3.262977600097656,81.27653333875865,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Tiros,2019-12-31T00:00:00,1.5,2.9371371269226074,95.80914179484049,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Tupaciguara,2019-12-31T00:00:00,2.4,2.59368109703064,8.070045709609989,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Uberaba,2019-12-31T00:00:00,2.476744186046512,3.6024868488311768,45.4525206570334,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Uberlandia,2019-12-31T00:00:00,2.015625,3.893788099288941,93.1801847709242,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Unai,2019-12-31T00:00:00,2.819938392607113,2.7182908058166504,-3.604603102569437,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Varginha,2019-12-31T00:00:00,1.5600442722744878,2.2742350101470947,45.78015833046474,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Varjao_de_Minas,2019-12-31T00:00:00,2.494512195121952,2.738291025161743,9.772605261922724,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Virginopolis,2019-12-31T00:00:00,1.5066666666666668,1.6540558338165283,9.782466846229743,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Almenara,2020-12-31T00:00:00,0.78,0.9550843834877014,22.446715831756585,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Alpinopolis,2020-12-31T00:00:00,2.765998457979954,2.8013699054718018,1.2787949100188531,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Alterosa,2020-12-31T00:00:00,1.980023501762632,2.7923169136047363,41.02443284733716,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Andradas,2020-12-31T00:00:00,2.568,1.858696699142456,-27.620845048969784,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Araguari,2020-12-31T00:00:00,1.979964695498676,3.0216381549835205,52.6107087592533,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Arapua,2020-12-31T00:00:00,1.5,1.8913094997406008,26.087299982706707,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Araxa,2020-12-31T00:00:00,1.786008230452675,1.8463722467422483,3.37982856183557,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Arceburgo,2020-12-31T00:00:00,1.8387096774193543,2.4839119911193848,35.089950394212174,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Areado,2020-12-31T00:00:00,2.078746484531941,2.2494399547576904,8.21136543084442,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Bom_Jesus_da_Penha,2020-12-31T00:00:00,1.986087924318308,2.318862199783325,16.755264023834023,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Botelhos,2020-12-31T00:00:00,1.716062736614386,2.48394775390625,44.7469081932763,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Botumirim,2020-12-31T00:00:00,0.9200000000000002,0.8578317761421204,-6.757415636726064,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Cabo_Verde,2020-12-31T00:00:00,2.249052581714827,1.871918797492981,-16.768562339893986,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Campestre,2020-12-31T00:00:00,2.138000770416025,2.060056209564209,-3.645675059165165,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Capetinga,2020-12-31T00:00:00,2.213808463251671,3.23384952545166,46.07630150179024,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Carmo_do_Paranaiba,2020-12-31T00:00:00,1.947480785653288,2.3967082500457764,23.067106371567814,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Cascalho_Rico,2020-12-31T00:00:00,2.478504672897196,2.1162362098693848,-14.616412346898864,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Claraval,2020-12-31T00:00:00,2.796008294453085,4.105695724487305,46.841328497932864,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Conceicao_da_Aparecida,2020-12-31T00:00:00,2.243948871362524,2.3101162910461426,2.94870442584736,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Coromandel,2020-12-31T00:00:00,1.818934240362812,2.2722675800323486,24.923019733748788,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Corrego_Fundo,2020-12-31T00:00:00,1.5,4.048981666564941,169.9321111043294,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Cruzeiro_da_Fortaleza,2020-12-31T00:00:00,2.1,2.434987545013428,15.951787857782268,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Delfinopolis,2020-12-31T00:00:00,1.8000000000000005,2.1244664192199707,18.025912178887243,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Divisa_Nova,2020-12-31T00:00:00,1.8000000000000005,2.85205602645874,58.44755702548554,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Dom_Cavati,2020-12-31T00:00:00,0.7894736842105263,1.4003490209579468,77.37754265467326,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Dores_do_Turvo,2020-12-31T00:00:00,1.625,1.2620474100112915,-22.335543999305138,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Estrela_do_Indaia,2020-12-31T00:00:00,1.8000000000000005,1.9311846494674685,7.288036081525999,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Estrela_do_Sul,2020-12-31T00:00:00,2.0627027027027025,2.866674900054932,38.97663954668826,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Guaranesia,2020-12-31T00:00:00,1.564686285397002,2.673547983169556,70.86798856239774,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Guarani,2020-12-31T00:00:00,1.127272727272727,2.1673736572265625,92.26701797977576,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Guarda-Mor,2020-12-31T00:00:00,1.7994350282485878,2.3664679527282715,31.51171982194788,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Guaxupe,2020-12-31T00:00:00,1.7400000000000002,1.8268539905548096,4.991608652575249,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Guimarania,2020-12-31T00:00:00,2.149930843706777,2.087679147720337,-2.89552085680623,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Ibia,2020-12-31T00:00:00,1.746,1.776843547821045,1.7665262211365935,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Ibiraci,2020-12-31T00:00:00,2.6315194346289745,4.178248405456543,58.77703012463772,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Indianopolis,2020-12-31T00:00:00,1.8000000000000005,3.7688469886779785,109.38038825988768,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Itamogi,2020-12-31T00:00:00,2.400049176297025,2.3226511478424072,-3.224851774663772,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Itanhomi,2020-12-31T00:00:00,1.26,1.6932122707366943,34.381926248944,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Jacui,2020-12-31T00:00:00,1.68,1.936169981956482,15.248213211695358,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Jacutinga,2020-12-31T00:00:00,1.8000000000000005,1.7538161277770996,-2.5657706790500363,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Juruaia,2020-12-31T00:00:00,1.92,1.9756211042404173,2.896932512521748,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Lagoa_Dourada,2020-12-31T00:00:00,2.1,3.154207706451416,50.20036697387694,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Lagoa_Formosa,2020-12-31T00:00:00,2.5494505494505484,2.139230728149414,-16.090518852760017,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Lagoa_Grande,2020-12-31T00:00:00,2.4,2.0425009727478027,-14.895792802174885,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Matutina,2020-12-31T00:00:00,1.82995951417004,2.2779507637023926,24.4809377509936,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Monte_Belo,2020-12-31T00:00:00,1.8000000000000005,2.519591808319092,39.97732268439397,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Monte_Carmelo,2020-12-31T00:00:00,1.98,2.799468755722046,41.38731089505284,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Monte_Santo_de_Minas,2020-12-31T00:00:00,2.123987903619159,2.646800756454468,24.61468127687847,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Muzambinho,2020-12-31T00:00:00,1.764006791171477,2.180347442626953,23.601986882317178,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Nova_Resende,2020-12-31T00:00:00,2.353040067245727,2.2219314575195312,-5.571881735089211,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Passos,2020-12-31T00:00:00,1.62,3.1402792930603027,93.84440080619152,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Patis,2020-12-31T00:00:00,3.6000000000000005,3.266187906265259,-9.27255815929838,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Patos_de_Minas,2020-12-31T00:00:00,1.941759465478842,1.908966302871704,-1.6888375306078895,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Patrocinio,2020-12-31T00:00:00,1.7479986236953782,3.653821468353272,109.02885270177528,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Pedrinopolis,2020-12-31T00:00:00,2.158490566037736,3.802670478820801,76.17267078453011,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Perdizes,2020-12-31T00:00:00,2.050845253576073,2.846409320831299,38.79200860562227,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Piracema,2020-12-31T00:00:00,1.333333333333333,1.4880402088165283,11.603015661239647,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Ponto_dos_Volantes,2020-12-31T00:00:00,0.631578947368421,0.4764811396598816,-24.55715288718541,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Pratapolis,2020-12-31T00:00:00,2.402298850574712,2.1508538722991943,-10.46684837797608,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Pratinha,2020-12-31T00:00:00,2.1,1.9740331172943115,-5.99842298598517,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Presidente_Kubitschek,2020-12-31T00:00:00,1.789473684210526,1.1504719257354736,-35.70892179713529,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Presidente_Olegario,2020-12-31T00:00:00,1.9814229249011863,2.364176034927368,19.31708295165049,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Quartel_Geral,2020-12-31T00:00:00,3.6000000000000005,6.527731895446777,81.32588598463268,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Rio_Paranaiba,2020-12-31T00:00:00,1.818897637795276,2.4270267486572266,33.43393812963971,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Rio_Vermelho,2020-12-31T00:00:00,0.9333333333333332,1.2060151100158691,29.21590464455744,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Romaria,2020-12-31T00:00:00,2.330769230769231,3.4340341091156006,47.33479676073533,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Sacramento,2020-12-31T00:00:00,1.8321428571428573,2.245534896850586,22.563308210168422,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Santa_Rosa_da_Serra,2020-12-31T00:00:00,2.1,1.6841249465942385,-19.803573971702946,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Santo_Antonio_do_Amparo,2020-12-31T00:00:00,2.1,3.054864883422852,45.46975635346912,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Sao_Goncalo_do_Abaete,2020-12-31T00:00:00,2.44,2.2405998706817627,-8.172136447468741,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Sao_Gotardo,2020-12-31T00:00:00,1.38027397260274,2.0918073654174805,51.550156486181066,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Sao_Joao_Batista_do_Gloria,2020-12-31T00:00:00,1.5,2.297928810119629,53.19525400797526,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Sao_Pedro_da_Uniao,2020-12-31T00:00:00,2.1,2.0467324256896973,-2.536551157633468,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Sao_Sebastiao_do_Paraiso,2020-12-31T00:00:00,1.929071661237785,2.0352797508239746,5.5056580696458655,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Serra_do_Salitre,2020-12-31T00:00:00,1.8000000000000005,3.4276249408721924,90.42360782623288,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Tapira,2020-12-31T00:00:00,2.1013698630137,2.532593011856079,20.521049455993275,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Tiros,2020-12-31T00:00:00,1.740235294117647,2.630652904510498,51.16650681678766,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Tupaciguara,2020-12-31T00:00:00,2.1,2.5651841163635254,22.1516245887393,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Uberaba,2020-12-31T00:00:00,2.5325581395348844,4.451510906219482,75.77132136587483,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Uberlandia,2020-12-31T00:00:00,1.920754716981132,4.191476821899414,118.2203060517377,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Unai,2020-12-31T00:00:00,2.3398907103825146,2.8141002655029297,20.26631214083044,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Varginha,2020-12-31T00:00:00,1.899977968715576,2.9996814727783203,57.87980293298701,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Varjao_de_Minas,2020-12-31T00:00:00,2.519786096256685,2.534832000732422,0.5971103856033038,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_1_2020,Virginopolis,2020-12-31T00:00:00,1.626666666666667,1.6212903261184692,-0.3305127386187103,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:07
V42_cluster_2_2020,Abre_Campo,2019-12-31T00:00:00,1.5,1.181108474731445,-21.259435017903648,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Aimores,2019-12-31T00:00:00,1.49025069637883,1.1717039346694946,-21.37538083245821,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Alvarenga,2019-12-31T00:00:00,0.6601705237515224,0.7628461718559265,15.55289798777044,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Andrelandia,2019-12-31T00:00:00,1.5,2.6050591468811035,73.67060979207358,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Antonio_Dias,2019-12-31T00:00:00,1.0,1.0818731784820557,8.187317848205566,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Antonio_Prado_de_Minas,2019-12-31T00:00:00,0.597560975609756,1.5454981327056885,158.63438139156426,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Bocaiuva,2019-12-31T00:00:00,1.6230769230769229,1.873615026473999,15.435996891762988,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Bom_Jesus_do_Amparo,2019-12-31T00:00:00,1.8000000000000005,4.850529670715332,169.4738705952962,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Campo_Belo,2019-12-31T00:00:00,1.26,3.0361878871917725,140.96729263426766,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Capela_Nova,2019-12-31T00:00:00,1.8000000000000005,2.0610740184783936,14.504112137688512,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Capitao_Eneas,2019-12-31T00:00:00,0.5333333333333333,1.1906219720840454,123.24161976575851,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Casa_Grande,2019-12-31T00:00:00,1.5,1.350110411643982,-9.99263922373454,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Cataguases,2019-12-31T00:00:00,0.5,0.8556411862373352,71.12823724746704,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Caxambu,2019-12-31T00:00:00,0.8979591836734695,1.13973867893219,26.925443790175684,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Coimbra,2019-12-31T00:00:00,1.5,1.661892056465149,10.792803764343262,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Conselheiro_Pena,2019-12-31T00:00:00,1.3,1.9930062294006348,53.308171492356514,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Corrego_Novo,2019-12-31T00:00:00,1.145454545454546,0.7856148481369019,-31.41457674995304,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Cruzilia,2019-12-31T00:00:00,1.2,1.3580217361450195,13.168478012084966,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Divisopolis,2019-12-31T00:00:00,1.080402010050251,2.375197410583496,119.8438533516818,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Esmeraldas,2019-12-31T00:00:00,2.1500000000000004,2.1951870918273926,2.1017252012740566,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Espirito_Santo_do_Dourado,2019-12-31T00:00:00,1.2,2.986161947250366,148.84682893753052,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Eugenopolis,2019-12-31T00:00:00,0.7199999999999999,1.243713140487671,72.73793617884321,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Felicio_dos_Santos,2019-12-31T00:00:00,1.8000000000000005,1.5304226875305176,-14.976517359415704,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Formiga,2019-12-31T00:00:00,1.2400000000000002,4.0845947265625,229.40280052923384,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Guaraciaba,2019-12-31T00:00:00,1.197368421052632,1.2689032554626465,5.974337818858343,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Guiricema,2019-12-31T00:00:00,1.5,1.6840674877166748,12.27116584777832,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Iapu,2019-12-31T00:00:00,1.5625,2.1519670486450195,37.72589111328125,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Ijaci,2019-12-31T00:00:00,1.416666666666667,3.9895856380462646,181.61780974444216,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Imbe_de_Minas,2019-12-31T00:00:00,0.9601626016260164,1.7609344720840454,83.39961055574727,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Inhapim,2019-12-31T00:00:00,1.319886363636364,1.6386024951934814,24.14724027294561,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Irai_de_Minas,2019-12-31T00:00:00,1.67972027972028,1.7793551683425903,5.931635750620477,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Itamarati_de_Minas,2019-12-31T00:00:00,0.9,0.8646855354309082,-3.9238293965657576,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Jequeri,2019-12-31T00:00:00,1.32,1.322700023651123,0.2045472462971958,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Lamim,2019-12-31T00:00:00,1.5200000000000002,1.2388653755187988,-18.495698979026407,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Mar_de_Espanha,2019-12-31T00:00:00,1.8000000000000005,1.8072763681411743,0.4042426745096693,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Mata_Verde,2019-12-31T00:00:00,1.020366598778004,2.8907744884490967,183.30743988592948,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Monte_Alegre_de_Minas,2019-12-31T00:00:00,1.8000000000000005,4.91839599609375,173.24422200520831,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Nova_Era,2019-12-31T00:00:00,1.5,2.0475668907165527,36.50445938110352,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Nova_Ponte,2019-12-31T00:00:00,1.679452054794521,2.335292100906372,39.0508347195474,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Paula_Candido,2019-12-31T00:00:00,1.32,1.4724794626235962,11.551474441181524,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Pecanha,2019-12-31T00:00:00,1.44,1.3230907917022705,-8.118695020675656,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Pedra_Bonita,2019-12-31T00:00:00,1.2,1.059457778930664,-11.71185175577799,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Pedra_Dourada,2019-12-31T00:00:00,0.96,1.133743405342102,18.09827138980232,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Pedra_do_Anta,2019-12-31T00:00:00,1.318181818181818,1.191153883934021,-9.63660190845356,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Piedade_de_Caratinga,2019-12-31T00:00:00,1.44,1.7431433200836182,21.051619450251263,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Pocrane,2019-12-31T00:00:00,1.07741935483871,1.531274914741516,42.12431843409276,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Presidente_Bernardes,2019-12-31T00:00:00,1.2571428571428571,1.521501541137695,21.028531681407586,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sabinopolis,2019-12-31T00:00:00,1.2,0.9840052723884584,-17.999560634295143,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Santa_Barbara,2019-12-31T00:00:00,1.071428571428571,1.593142867088318,48.6933342615764,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Santo_Antonio_do_Grama,2019-12-31T00:00:00,1.2,1.8644030094146729,55.36691745122274,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sao_Domingos_do_Prata,2019-12-31T00:00:00,0.7211538461538461,1.9280405044555664,167.35494995117188,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sao_Geraldo,2019-12-31T00:00:00,2.0,0.873839259147644,-56.30803704261778,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sao_Jose_da_Barra,2019-12-31T00:00:00,1.702112676056338,2.4210469722747803,42.237761714116175,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sao_Jose_do_Alegre,2019-12-31T00:00:00,1.5,2.0910067558288574,39.4004503885905,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sao_Miguel_do_Anta,2019-12-31T00:00:00,1.303164556962025,1.2175374031066897,-6.570709232220986,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sao_Sebastiao_da_Vargem_Alegre,2019-12-31T00:00:00,1.02,1.0254970788955688,0.5389293034871402,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sao_Sebastiao_do_Anta,2019-12-31T00:00:00,1.5,2.1167564392089844,41.117095947265625,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Senhora_dos_Remedios,2019-12-31T00:00:00,1.8000000000000005,1.4851492643356323,-17.491707536909328,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Teixeiras,2019-12-31T00:00:00,0.96,1.1772220134735107,22.627293070157386,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Visconde_do_Rio_Branco,2019-12-31T00:00:00,0.5,0.9244025945663452,84.88051891326904,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Abre_Campo,2020-12-31T00:00:00,1.5,1.2243927717208862,-18.373815218607582,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Aimores,2020-12-31T00:00:00,2.063157894736842,1.4960469007492063,-27.487522667768044,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Alvarenga,2020-12-31T00:00:00,0.9,0.7514234781265259,-16.508502430386017,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Andrelandia,2020-12-31T00:00:00,1.8000000000000005,2.68260145187378,49.03341399298772,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Antonio_Dias,2020-12-31T00:00:00,1.333333333333333,0.9592487812042236,-28.05634140968321,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Antonio_Prado_de_Minas,2020-12-31T00:00:00,1.5,1.3451533317565918,-10.323111216227217,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Bocaiuva,2020-12-31T00:00:00,1.622950819672131,1.8497685194015503,13.97563604393392,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Bom_Jesus_do_Amparo,2020-12-31T00:00:00,1.5,1.372506856918335,-8.499542872111004,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Campo_Belo,2020-12-31T00:00:00,1.8000000000000005,3.130887746810913,73.9382081561618,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Capela_Nova,2020-12-31T00:00:00,1.8000000000000005,2.029937028884888,12.774279382493743,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Capitao_Eneas,2020-12-31T00:00:00,3.0,1.4368709325790403,-52.10430224736532,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Casa_Grande,2020-12-31T00:00:00,1.553846153846154,1.6481013298034668,6.0659271655696285,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Cataguases,2020-12-31T00:00:00,1.25,0.9556453227996826,-23.54837417602539,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Caxambu,2020-12-31T00:00:00,1.3265306122448983,1.064715027809143,-19.736867134387698,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Coimbra,2020-12-31T00:00:00,1.8000000000000005,1.6261720657348633,-9.657107459174275,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Conselheiro_Pena,2020-12-31T00:00:00,1.55981308411215,1.679175615310669,7.652361197268735,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Corrego_Novo,2020-12-31T00:00:00,1.2545454545454553,0.922649383544922,-26.455483920332355,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Cruzilia,2020-12-31T00:00:00,1.2,1.3499151468276978,12.492928902308153,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Divisopolis,2020-12-31T00:00:00,1.34029484029484,2.310750961303711,72.4061670486912,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Esmeraldas,2020-12-31T00:00:00,2.096153846153846,1.7485889196395874,-16.58107906306554,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Espirito_Santo_do_Dourado,2020-12-31T00:00:00,1.619148936170213,2.6937031745910645,66.36537346357426,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Eugenopolis,2020-12-31T00:00:00,1.8000000000000005,1.2076630592346191,-32.90760782029894,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Felicio_dos_Santos,2020-12-31T00:00:00,3.548821548821549,1.6756690740585327,-52.78237998146261,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Formiga,2020-12-31T00:00:00,1.441791044776119,2.62886118888855,82.3330224177359,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Guaraciaba,2020-12-31T00:00:00,1.2,1.373799443244934,14.483286937077844,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Guiricema,2020-12-31T00:00:00,2.4,1.7744452953338623,-26.06477936108907,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Iapu,2020-12-31T00:00:00,1.0,2.066589832305908,106.65898323059082,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Ijaci,2020-12-31T00:00:00,2.458064516129032,3.573573112487793,45.381583316432526,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Imbe_de_Minas,2020-12-31T00:00:00,1.439877300613497,1.1184171438217163,-22.32552431063496,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Inhapim,2020-12-31T00:00:00,1.8000000000000005,1.5171550512313845,-15.713608264923106,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Irai_de_Minas,2020-12-31T00:00:00,3.3595092024539883,1.866690993309021,-44.4356041071274,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Itamarati_de_Minas,2020-12-31T00:00:00,1.2,0.8598512411117554,-28.34572990735372,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Jequeri,2020-12-31T00:00:00,2.1,1.4287683963775637,-31.963409696306503,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Lamim,2020-12-31T00:00:00,2.387096774193548,1.3632912635803225,-42.88914976893244,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Mar_de_Espanha,2020-12-31T00:00:00,1.8000000000000005,1.9390864372253416,7.727024290296751,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Mata_Verde,2020-12-31T00:00:00,1.319688109161793,1.8847368955612185,42.8168430462194,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Monte_Alegre_de_Minas,2020-12-31T00:00:00,3.0,2.195362329483032,-26.82125568389893,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Nova_Era,2020-12-31T00:00:00,1.565217391304348,1.5605480670928955,-0.2983179357316802,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Nova_Ponte,2020-12-31T00:00:00,1.8591549295774648,2.135127782821656,14.843994379043584,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Paula_Candido,2020-12-31T00:00:00,1.62,1.4237614870071411,-12.11348845634932,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Pecanha,2020-12-31T00:00:00,2.0,1.2159790992736816,-39.201045036315904,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Pedra_Bonita,2020-12-31T00:00:00,1.7400000000000002,1.00850510597229,-42.03993643837414,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Pedra_Dourada,2020-12-31T00:00:00,1.44,1.2047802209854126,-16.334706876013012,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Pedra_do_Anta,2020-12-31T00:00:00,1.5,1.2484720945358276,-16.768527030944824,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Piedade_de_Caratinga,2020-12-31T00:00:00,1.8000000000000005,1.3700255155563354,-23.887471357981376,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Pocrane,2020-12-31T00:00:00,1.258064516129032,1.2156933546066284,-3.36796412101157,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Presidente_Bernardes,2020-12-31T00:00:00,1.5,1.4972162246704102,-0.1855850219726562,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sabinopolis,2020-12-31T00:00:00,1.0,1.1243473291397097,12.434732913970947,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Santa_Barbara,2020-12-31T00:00:00,1.142857142857143,1.1281182765960691,-1.2896507978439469,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Santo_Antonio_do_Grama,2020-12-31T00:00:00,1.695652173913043,2.028885126113892,19.65219974517825,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sao_Domingos_do_Prata,2020-12-31T00:00:00,0.8846153846153846,0.9137312769889832,3.291361746580709,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sao_Geraldo,2020-12-31T00:00:00,1.8000000000000005,1.5891225337982178,-11.715414788987914,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sao_Jose_da_Barra,2020-12-31T00:00:00,2.537024221453287,2.8010928630828857,10.408597576507644,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sao_Jose_do_Alegre,2020-12-31T00:00:00,1.359090909090909,1.9729444980621336,45.16648480724732,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sao_Miguel_do_Anta,2020-12-31T00:00:00,1.5,1.2486575841903689,-16.756161053975426,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sao_Sebastiao_da_Vargem_Alegre,2020-12-31T00:00:00,1.8000000000000005,1.07702898979187,-40.16505612267389,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Sao_Sebastiao_do_Anta,2020-12-31T00:00:00,1.8000000000000005,1.8959476947784424,5.330427487691228,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Senhora_dos_Remedios,2020-12-31T00:00:00,1.8000000000000005,1.756993293762207,-2.3892614576551794,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Teixeiras,2020-12-31T00:00:00,1.5,1.1155027151107788,-25.63315232594808,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_2_2020,Visconde_do_Rio_Branco,2020-12-31T00:00:00,1.0,0.9234800338745116,-7.651996612548828,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:33:42
V42_cluster_3_2020,Aiuruoca,2019-12-31T00:00:00,1.538461538461539,1.7291336059570312,12.393684387206992,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Albertina,2019-12-31T00:00:00,1.56,1.8000441789627075,15.387447369404326,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Baependi,2019-12-31T00:00:00,1.439956331877729,1.803998351097107,25.281462441618665,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Bandeira_do_Sul,2019-12-31T00:00:00,1.5,2.025362014770508,35.02413431803386,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Borda_da_Mata,2019-12-31T00:00:00,1.5606557377049175,2.12469482421875,36.14115995519304,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Brazopolis,2019-12-31T00:00:00,1.559426229508197,1.4061684608459473,-9.827830631692231,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Bueno_Brandao,2019-12-31T00:00:00,1.56,2.105031490325928,34.93791604653382,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Cachoeira_de_Minas,2019-12-31T00:00:00,1.68,2.2351651191711426,33.04554280780611,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Caldas,2019-12-31T00:00:00,1.44,1.7452363967895508,21.196971999274364,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Cambuquira,2019-12-31T00:00:00,1.679869186046512,2.169668912887573,29.15701596942678,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Campanha,2019-12-31T00:00:00,1.506616257088847,2.302133560180664,52.80158762052336,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Careacu,2019-12-31T00:00:00,1.2,1.5502028465270996,29.183570543924976,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Carmo_da_Cachoeira,2019-12-31T00:00:00,1.6199809705042818,2.1734957695007324,34.1679815426565,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Carmo_de_Minas,2019-12-31T00:00:00,1.5,1.8028069734573364,20.187131563822422,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Carrancas,2019-12-31T00:00:00,1.875,1.766198992729187,-5.802720387776682,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Carvalhopolis,2019-12-31T00:00:00,1.560199625701809,1.8357455730438232,17.660941766863214,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Conceicao_das_Pedras,2019-12-31T00:00:00,1.679708222811671,1.6962717771530151,0.9860971159394718,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Conceicao_do_Rio_Verde,2019-12-31T00:00:00,1.8000000000000005,2.313779830932617,28.543323940700937,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Conceicao_dos_Ouros,2019-12-31T00:00:00,1.5028571428571431,1.5839828252792358,5.398096739112633,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Congonhal,2019-12-31T00:00:00,1.501818181818182,2.1001744270324707,39.842122865358206,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Cordislandia,2019-12-31T00:00:00,1.5,2.548660516738892,69.91070111592612,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Cristina,2019-12-31T00:00:00,1.500597371565114,1.6291420459747314,8.566233477774665,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Datas,2019-12-31T00:00:00,2.0,2.880226850509644,44.01134252548221,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Dom_Vicoso,2019-12-31T00:00:00,1.65,1.6122851371765137,-2.2857492620294693,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Fama,2019-12-31T00:00:00,1.619932432432432,1.814002275466919,11.98011961172224,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Fortaleza_de_Minas,2019-12-31T00:00:00,1.4403669724770642,1.7205562591552734,19.452632005047644,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Heliodora,2019-12-31T00:00:00,1.5599383667180282,1.8360247611999512,17.698545043339386,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Ibitiura_de_Minas,2019-12-31T00:00:00,1.32,1.8860015869140625,42.87890809955018,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Inconfidentes,2019-12-31T00:00:00,1.5,3.0373456478118896,102.48970985412598,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Ingai,2019-12-31T00:00:00,1.5,2.036619186401367,35.77461242675781,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Itajuba,2019-12-31T00:00:00,1.8235294117647065,2.211150646209717,21.25664834053283,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Itutinga,2019-12-31T00:00:00,1.521739130434783,1.6047024726867676,5.45187677655899,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Jesuania,2019-12-31T00:00:00,1.5,1.9286248683929443,28.57499122619629,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Lambari,2019-12-31T00:00:00,1.559769167353669,1.823552131652832,16.911666791484368,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Luminarias,2019-12-31T00:00:00,1.720238095238095,2.6564180850982666,54.42153574273664,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Maria_da_Fe,2019-12-31T00:00:00,1.5,2.0220425128936768,34.80283419291179,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Monsenhor_Paulo,2019-12-31T00:00:00,1.5,2.2925796508789062,52.838643391927086,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Monte_Siao,2019-12-31T00:00:00,1.5601626016260162,1.919675350189209,23.04328716689563,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Natercia,2019-12-31T00:00:00,1.4399193548387097,1.948483109474182,35.31890539053407,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Olimpio_Noronha,2019-12-31T00:00:00,1.5,1.8020119667053225,20.13413111368815,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Ouro_Fino,2019-12-31T00:00:00,1.439922480620155,1.7925150394439695,24.486912564345687,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Paraisopolis,2019-12-31T00:00:00,1.2,1.9236629009246824,60.30524174372356,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Pedralva,2019-12-31T00:00:00,1.5,1.9413660764694207,29.424405097961422,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Pirangucu,2019-12-31T00:00:00,1.166666666666667,1.3384429216384888,14.723678997584722,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Piranguinho,2019-12-31T00:00:00,1.5,1.524836540222168,1.6557693481445312,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Poco_Fundo,2019-12-31T00:00:00,1.199923204914885,1.9099221229553225,59.17036316426603,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Pocos_de_Caldas,2019-12-31T00:00:00,1.08,2.0029845237731934,85.46152997899938,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Pouso_Alegre,2019-12-31T00:00:00,1.5,1.5900315046310425,6.002100308736166,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Pouso_Alto,2019-12-31T00:00:00,1.65,1.7055808305740356,3.368535186305196,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Santa_Rita_de_Caldas,2019-12-31T00:00:00,1.9210526315789471,1.966059684753418,2.342832904972455,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Santa_Rita_do_Sapucai,2019-12-31T00:00:00,1.5,1.6995630264282229,13.30420176188151,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Sao_Bento_Abade,2019-12-31T00:00:00,1.44,2.8343420028686523,96.82930575476752,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Sao_Goncalo_do_Sapucai,2019-12-31T00:00:00,1.5,2.599847316741944,73.32315444946289,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Sao_Joao_da_Mata,2019-12-31T00:00:00,1.3799999999999997,1.586712121963501,14.97913927271749,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Sao_Joao_del_Rei,2019-12-31T00:00:00,1.982758620689655,2.0803771018981934,4.923366878343673,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Sao_Lourenco,2019-12-31T00:00:00,1.6000000000000003,1.672325611114502,4.520350694656352,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Sao_Sebastiao_da_Bela_Vista,2019-12-31T00:00:00,1.44,1.4660310745239258,1.8077135086059608,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Sao_Tome_das_Letras,2019-12-31T00:00:00,1.440140845070423,1.4907057285308838,3.5111068221933683,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Senador_Jose_Bento,2019-12-31T00:00:00,1.2,1.4644241333007812,22.035344441731773,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Serrania,2019-12-31T00:00:00,1.68,2.074258327484131,23.46775758834113,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Silvianopolis,2019-12-31T00:00:00,1.2,1.2871248722076416,7.260406017303471,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Soledade_de_Minas,2019-12-31T00:00:00,1.8000000000000005,1.7332301139831543,-3.709438112046998,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Tocos_do_Moji,2019-12-31T00:00:00,1.2,1.8575831651687624,54.79859709739687,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Tres_Coracoes,2019-12-31T00:00:00,1.56,3.065091609954834,96.48023140736116,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Turvolandia,2019-12-31T00:00:00,2.1,2.366483211517334,12.68967673892066,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Virginia,2019-12-31T00:00:00,1.36,1.9873560667037964,46.12912255174975,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Aiuruoca,2020-12-31T00:00:00,1.8000000000000005,1.8466041088104248,2.589117156134696,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Albertina,2020-12-31T00:00:00,1.68,2.029411792755127,20.79832099732899,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Baependi,2020-12-31T00:00:00,1.67948717948718,2.0995023250579834,25.008535385131804,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Bandeira_do_Sul,2020-12-31T00:00:00,1.5,1.7492046356201172,16.613642374674477,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Borda_da_Mata,2020-12-31T00:00:00,1.62,2.523030996322632,55.74265409398961,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Brazopolis,2020-12-31T00:00:00,1.680327868852459,2.0481977462768555,21.89274392476896,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Bueno_Brandao,2020-12-31T00:00:00,1.8000000000000005,2.238006114959717,24.333673053317582,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Cachoeira_de_Minas,2020-12-31T00:00:00,1.8596638655462183,2.8813509941101074,54.93934401224707,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Caldas,2020-12-31T00:00:00,1.68,1.7105579376220703,1.81892485845657,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Cambuquira,2020-12-31T00:00:00,1.800145348837209,2.254194498062134,25.22291600054488,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Campanha,2020-12-31T00:00:00,1.6833930704898452,2.262268781661988,34.387435787869606,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Careacu,2020-12-31T00:00:00,1.320454545454546,1.6837186813354492,27.5105369686054,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Carmo_da_Cachoeira,2020-12-31T00:00:00,1.8000000000000005,2.158050060272217,19.891670015123136,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Carmo_de_Minas,2020-12-31T00:00:00,1.7400000000000002,2.019425868988037,16.05895798781821,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Carrancas,2020-12-31T00:00:00,1.785714285714286,2.166245937347412,21.309772491455057,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Carvalhopolis,2020-12-31T00:00:00,1.679765395894428,1.6332573890686035,-2.768720378422866,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Conceicao_das_Pedras,2020-12-31T00:00:00,1.739921976592978,1.875096321105957,7.768988858780328,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Conceicao_do_Rio_Verde,2020-12-31T00:00:00,1.8000000000000005,2.5828328132629395,43.49071184794106,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Conceicao_dos_Ouros,2020-12-31T00:00:00,1.5028571428571431,1.7066490650177002,13.56029900307889,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Congonhal,2020-12-31T00:00:00,1.619354838709677,2.2832140922546387,40.99529254958928,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Cordislandia,2020-12-31T00:00:00,1.62,2.1143345832824707,30.51448044953522,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Cristina,2020-12-31T00:00:00,2.100263852242744,1.9352271556854248,-7.85790301447537,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Datas,2020-12-31T00:00:00,2.0,3.382956027984619,69.14780139923099,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Dom_Vicoso,2020-12-31T00:00:00,1.4285714285714288,1.860007286071777,30.20051002502439,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Fama,2020-12-31T00:00:00,1.8000000000000005,1.926502346992493,7.027908166249578,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Fortaleza_de_Minas,2020-12-31T00:00:00,1.68,1.983593463897705,18.071039517720543,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Heliodora,2020-12-31T00:00:00,1.8000000000000005,2.175880193710327,20.882232983907045,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Ibitiura_de_Minas,2020-12-31T00:00:00,1.68,2.052046060562134,22.14559884298416,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Inconfidentes,2020-12-31T00:00:00,2.46,3.243680715560913,31.856939656947684,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Ingai,2020-12-31T00:00:00,1.6791666666666667,2.247556686401367,33.84952971124767,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Itajuba,2020-12-31T00:00:00,1.5882352941176472,2.592184543609619,63.21161941245749,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Itutinga,2020-12-31T00:00:00,1.8000000000000005,1.693758487701416,-5.902306238810235,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Jesuania,2020-12-31T00:00:00,1.5,2.076314926147461,38.42099507649739,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Lambari,2020-12-31T00:00:00,1.8000970402717127,1.886346340179444,4.791369463876898,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Luminarias,2020-12-31T00:00:00,1.860103626943005,2.693312644958496,44.793688155150384,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Maria_da_Fe,2020-12-31T00:00:00,1.7333333333333332,1.9315837621688845,11.437524740512565,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Monsenhor_Paulo,2020-12-31T00:00:00,2.1,2.0748751163482666,-1.1964230310349278,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Monte_Siao,2020-12-31T00:00:00,1.8000000000000005,2.350325584411621,30.573643578423376,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Natercia,2020-12-31T00:00:00,1.4399193548387097,2.1028852462768555,46.04187652664804,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Olimpio_Noronha,2020-12-31T00:00:00,1.5595854922279788,2.0183324813842773,29.41467405553676,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Ouro_Fino,2020-12-31T00:00:00,2.099920063948841,1.838497877120972,-12.4491494374444,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Paraisopolis,2020-12-31T00:00:00,2.4,2.316222906112671,-3.490712245305376,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Pedralva,2020-12-31T00:00:00,1.68,2.4629440307617188,46.60381135486421,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Pirangucu,2020-12-31T00:00:00,1.333333333333333,1.4854341745376587,11.407563090324429,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Piranguinho,2020-12-31T00:00:00,1.458181818181818,1.6380013227462769,12.331761534969123,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Poco_Fundo,2020-12-31T00:00:00,1.740033329060377,1.5787092447280884,-9.271321510801416,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Pocos_de_Caldas,2020-12-31T00:00:00,1.8000000000000005,1.9090006351470947,6.055590841505247,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Pouso_Alegre,2020-12-31T00:00:00,1.6375,2.1728968620300293,32.69599157435294,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Pouso_Alto,2020-12-31T00:00:00,1.45,1.927051424980164,32.90009827449404,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Santa_Rita_de_Caldas,2020-12-31T00:00:00,1.8000000000000005,2.1950371265411377,21.946507030063188,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Santa_Rita_do_Sapucai,2020-12-31T00:00:00,1.68,2.095179319381714,24.713054725102023,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Sao_Bento_Abade,2020-12-31T00:00:00,1.8000000000000005,2.640507459640503,46.6948588689168,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Sao_Goncalo_do_Sapucai,2020-12-31T00:00:00,2.4,2.4833149909973145,3.4714579582214395,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Sao_Joao_da_Mata,2020-12-31T00:00:00,1.6399999999999997,1.55021071434021,-5.474956442670106,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Sao_Joao_del_Rei,2020-12-31T00:00:00,1.9709821428571432,2.124648094177246,7.79641519721473,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Sao_Lourenco,2020-12-31T00:00:00,3.3928571428571423,1.971990942955017,-41.87816168132581,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Sao_Sebastiao_da_Bela_Vista,2020-12-31T00:00:00,1.5,1.7899428606033323,19.329524040222168,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Sao_Tome_das_Letras,2020-12-31T00:00:00,2.133763094278808,1.7511025667190552,-17.933599497796564,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Senador_Jose_Bento,2020-12-31T00:00:00,1.620618556701031,1.5071237087249756,-7.003180822949981,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Serrania,2020-12-31T00:00:00,1.8000000000000005,1.87046217918396,3.914565510219983,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Silvianopolis,2020-12-31T00:00:00,1.50032154340836,1.5149290561676023,0.9736254753802824,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Soledade_de_Minas,2020-12-31T00:00:00,3.2000000000000006,2.188800811767578,-31.599974632263194,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Tocos_do_Moji,2020-12-31T00:00:00,1.441340782122905,2.003626823425293,39.01131836942925,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Tres_Coracoes,2020-12-31T00:00:00,2.1,3.052109479904175,45.33854666210356,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Turvolandia,2020-12-31T00:00:00,1.6202531645569618,2.399522066116333,48.09550251811745,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_3_2020,Virginia,2020-12-31T00:00:00,1.5333333333333332,1.972079157829285,28.613858119301185,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:16
V42_cluster_4_2020,Abadia_dos_Dourados,2019-12-31T00:00:00,2.4,1.5567141771316528,-35.13690928618113,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Acucena,2019-12-31T00:00:00,0.8571428571428571,0.9524257183074952,11.116333802541105,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Agua_Boa,2019-12-31T00:00:00,1.092380952380952,1.2232178449630735,11.977222075957092,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Aguas_Vermelhas,2019-12-31T00:00:00,2.7000000000000006,3.147379875183105,16.569625006781656,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Angelandia,2019-12-31T00:00:00,1.219512195121951,1.424787163734436,16.83254742622378,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Aricanduva,2019-12-31T00:00:00,0.9608695652173912,1.1249103546142578,17.072118353520057,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Ataleia,2019-12-31T00:00:00,0.9,0.9730463624000548,8.116262488894991,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Bandeira,2019-12-31T00:00:00,0.9,0.9324477910995485,3.6053101221720354,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Berilo,2019-12-31T00:00:00,0.7142857142857143,0.8043954372406006,12.615361213684078,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Berizal,2019-12-31T00:00:00,1.2,1.5614545345306396,30.12121121088665,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Bonfinopolis_de_Minas,2019-12-31T00:00:00,2.4,2.316690444946289,-3.471231460571285,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Buritis,2019-12-31T00:00:00,3.12,2.326669454574585,-25.427261071327408,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Buritizeiro,2019-12-31T00:00:00,2.880232558139535,2.4066619873046875,-16.442094909889736,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Capelinha,2019-12-31T00:00:00,1.140074211502783,1.4913582801818848,30.812386170550983,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Carai,2019-12-31T00:00:00,1.2,0.8931866884231567,-25.56777596473694,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Caranaiba,2019-12-31T00:00:00,1.444444444444444,1.303351879119873,-9.767946830162606,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Catuji,2019-12-31T00:00:00,1.25974025974026,1.0506030321121216,-16.601614976666657,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Congonhas_do_Norte,2019-12-31T00:00:00,0.9,1.0654101371765137,18.37890413072374,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Coroaci,2019-12-31T00:00:00,1.456,1.7262741327285769,18.5627838412484,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Cuparaque,2019-12-31T00:00:00,1.5,0.9415485858917236,-37.230094273885086,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Diamantina,2019-12-31T00:00:00,1.7228915662650603,1.984229564666748,15.168569138000054,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Formoso,2019-12-31T00:00:00,3.0,2.717339277267456,-9.422024091084795,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Franciscopolis,2019-12-31T00:00:00,3.0,1.5430396795272827,-48.56534401575725,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Frei_Gaspar,2019-12-31T00:00:00,1.2,0.9123401641845704,-23.971652984619137,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Grao_Mogol,2019-12-31T00:00:00,0.7250000000000001,1.6853501796722412,132.4620937478953,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Indaiabira,2019-12-31T00:00:00,2.4,2.968076467514038,23.669852813084923,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Itabirinha,2019-12-31T00:00:00,1.098484848484848,0.9346833825111388,-14.911581730020416,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Itacambira,2019-12-31T00:00:00,0.6000000000000001,2.405561208724976,300.9268681208292,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Itaipe,2019-12-31T00:00:00,0.9610983981693364,1.0069262981414795,4.768283878053948,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Itamarandiba,2019-12-31T00:00:00,1.214953271028037,1.7650296688079834,45.27551889419559,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Itambacuri,2019-12-31T00:00:00,0.9,1.261352777481079,40.15030860900878,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Jequitinhonha,2019-12-31T00:00:00,2.0375,1.7616043090820312,-13.54089280578986,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Joao_Pinheiro,2019-12-31T00:00:00,2.4,2.4782156944274902,3.258987267812097,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Jose_Goncalves_de_Minas,2019-12-31T00:00:00,0.958823529411765,1.142062783241272,19.110842423936308,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Juiz_de_Fora,2019-12-31T00:00:00,0.9,1.0692788362503052,18.80875958336724,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Ladainha,2019-12-31T00:00:00,1.118556701030928,1.2028238773345947,7.533563227148077,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Malacacheta,2019-12-31T00:00:00,1.5,1.6692509651184082,11.283397674560549,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Mantena,2019-12-31T00:00:00,0.279,0.8572927713394165,207.27339474531053,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Minas_Novas,2019-12-31T00:00:00,0.9,1.0132004022598269,12.577822473314072,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Monte_Formoso,2019-12-31T00:00:00,1.2,0.9922081828117372,-17.315984765688576,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Ninheira,2019-12-31T00:00:00,2.7000871839581526,2.8802855014801025,6.673796260822619,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Nova_Belem,2019-12-31T00:00:00,0.9,1.169630765914917,29.95897399054633,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Novo_Cruzeiro,2019-12-31T00:00:00,1.079404466501241,1.0810788869857788,0.1551244724755716,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Novorizonte,2019-12-31T00:00:00,1.6000000000000003,0.8977977633476257,-43.887639790773406,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Ouro_Verde_de_Minas,2019-12-31T00:00:00,1.5,1.21954607963562,-18.696928024291992,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Padre_Paraiso,2019-12-31T00:00:00,0.6000000000000001,0.7056907415390015,17.61512358983356,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Paracatu,2019-12-31T00:00:00,2.4,2.7139434814453125,13.080978393554693,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Pedra_Azul,2019-12-31T00:00:00,1.2,1.2830983400344849,6.92486166954041,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Pirapora,2019-12-31T00:00:00,3.0,3.00951886177063,0.3172953923543294,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Pote,2019-12-31T00:00:00,0.7250000000000001,0.9003701210021973,24.188982207199608,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Rio_Pardo_de_Minas,2019-12-31T00:00:00,2.75,2.813617467880249,2.313362468372692,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Santa_Barbara_do_Monte_Verde,2019-12-31T00:00:00,0.6666666666666667,1.010444164276123,51.56662464141844,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Santa_Maria_do_Suacui,2019-12-31T00:00:00,0.8,0.8649828433990479,8.122855424880976,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Santo_Antonio_do_Retiro,2019-12-31T00:00:00,2.1,1.0915743112564087,-48.02027089255197,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Sao_Goncalo_do_Rio_Preto,2019-12-31T00:00:00,1.6000000000000003,3.188661575317383,99.2913484573364,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Sao_Joao_do_Manteninha,2019-12-31T00:00:00,0.9,0.7254899740219116,-19.390002886454266,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Sao_Joao_do_Paraiso,2019-12-31T00:00:00,1.5857142857142863,2.378511905670166,49.996246303523925,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Sao_Sebastiao_do_Maranhao,2019-12-31T00:00:00,0.925925925925926,0.9793714880943298,5.772120714187621,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Senador_Modestino_Goncalves,2019-12-31T00:00:00,1.833333333333333,1.866851568222046,1.8282673575661563,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Setubinha,2019-12-31T00:00:00,0.9,1.2776648998260498,41.96276664733886,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Taiobeiras,2019-12-31T00:00:00,2.579831932773109,2.0745530128479004,-19.585730120879425,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Teofilo_Otoni,2019-12-31T00:00:00,0.90625,0.9880985617637634,9.031565436001484,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Turmalina,2019-12-31T00:00:00,1.199233716475096,1.4462050199508667,20.594092717947657,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Urucuia,2019-12-31T00:00:00,2.400355871886121,2.08461594581604,-13.153879796247994,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Vargem_Grande_do_Rio_Pardo,2019-12-31T00:00:00,2.1,1.7416516542434692,-17.064206940787184,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Varzea_da_Palma,2019-12-31T00:00:00,3.6004056795131847,3.514038562774658,-2.398816256455975,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Abadia_dos_Dourados,2020-12-31T00:00:00,2.584210526315789,1.8331204652786253,-29.064584846651947,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Acucena,2020-12-31T00:00:00,1.166666666666667,0.9229884743690492,-20.88670219693867,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Agua_Boa,2020-12-31T00:00:00,1.33,1.2160547971725464,-8.567308483267194,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Aguas_Vermelhas,2020-12-31T00:00:00,3.6000000000000005,3.687669038772583,2.43525107701618,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Angelandia,2020-12-31T00:00:00,1.8400349650349648,1.458198070526123,-20.75161079896035,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Aricanduva,2020-12-31T00:00:00,1.3196428571428571,1.1317949295043943,-14.23475500372653,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Ataleia,2020-12-31T00:00:00,1.204819277108434,0.9990534782409668,-17.078561305999777,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Bandeira,2020-12-31T00:00:00,1.083333333333333,0.9257537126541138,-14.545811139620245,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Berilo,2020-12-31T00:00:00,0.8428571428571429,0.8086740970611572,-4.05561560291355,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Berizal,2020-12-31T00:00:00,3.340236686390532,1.9312108755111688,-42.18341223004647,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Bonfinopolis_de_Minas,2020-12-31T00:00:00,3.3,2.563779592514038,-22.30970931775642,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Buritis,2020-12-31T00:00:00,3.3,2.439783096313477,-26.06717889959161,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Buritizeiro,2020-12-31T00:00:00,3.0,2.429139852523804,-19.02867158253988,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Capelinha,2020-12-31T00:00:00,1.434123222748815,1.2811672687530518,-10.665468041343702,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Carai,2020-12-31T00:00:00,1.5225,1.2117292881011963,-20.411869418640634,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Caranaiba,2020-12-31T00:00:00,1.645161290322581,1.5414233207702637,-6.305641286513403,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Catuji,2020-12-31T00:00:00,1.197183098591549,1.3236136436462402,10.560669057509504,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Congonhas_do_Norte,2020-12-31T00:00:00,1.2,1.035309910774231,-13.72417410214742,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Coroaci,2020-12-31T00:00:00,1.8000000000000005,1.86934232711792,3.852351506551092,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Cuparaque,2020-12-31T00:00:00,1.018518518518519,1.296867609024048,27.328819795088265,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Diamantina,2020-12-31T00:00:00,2.3453815261044184,1.998220682144165,-14.801892148305315,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Formoso,2020-12-31T00:00:00,3.0,2.970684289932251,-0.9771903355916342,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Franciscopolis,2020-12-31T00:00:00,1.8000000000000005,2.5767996311187744,43.15553506215412,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Frei_Gaspar,2020-12-31T00:00:00,1.08,1.3798657655715942,27.76534866403649,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Grao_Mogol,2020-12-31T00:00:00,0.7749999999999999,1.4833838939666748,91.40437341505483,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Indaiabira,2020-12-31T00:00:00,3.0,3.1620147228240967,5.400490760803223,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Itabirinha,2020-12-31T00:00:00,0.7992424242424243,1.123647689819336,40.58909483995482,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Itacambira,2020-12-31T00:00:00,0.75,1.93844211101532,158.45894813537598,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Itaipe,2020-12-31T00:00:00,0.9594594594594597,1.0060298442840576,4.85381475636656,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Itamarandiba,2020-12-31T00:00:00,1.7196261682242993,1.812594413757324,5.406305582626994,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Itambacuri,2020-12-31T00:00:00,1.091304347826087,1.1994297504425049,9.907905419034298,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Jequitinhonha,2020-12-31T00:00:00,2.4,2.218266010284424,-7.572249571482336,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Joao_Pinheiro,2020-12-31T00:00:00,3.0,2.460658073425293,-17.9780642191569,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Jose_Goncalves_de_Minas,2020-12-31T00:00:00,1.5,1.253626823425293,-16.424878438313804,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Juiz_de_Fora,2020-12-31T00:00:00,1.222222222222222,1.327109456062317,8.58168276873503,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Ladainha,2020-12-31T00:00:00,1.242268041237113,1.1653521060943604,-6.191573202362663,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Malacacheta,2020-12-31T00:00:00,1.8000000000000005,1.7105345726013184,-4.970301522148994,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Mantena,2020-12-31T00:00:00,0.7,0.8750823140144348,25.011759144919267,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Minas_Novas,2020-12-31T00:00:00,1.0,1.1340138912200928,13.401389122009276,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Monte_Formoso,2020-12-31T00:00:00,0.6000000000000001,1.411569118499756,135.2615197499593,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Ninheira,2020-12-31T00:00:00,3.0,2.934469223022461,-2.184359232584636,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Nova_Belem,2020-12-31T00:00:00,1.1,1.2848219871520996,16.801998832009048,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Novo_Cruzeiro,2020-12-31T00:00:00,1.19727047146402,1.1587620973587036,-3.216347101438869,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Novorizonte,2020-12-31T00:00:00,1.235294117647059,1.3010222911834717,5.320852143423874,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Ouro_Verde_de_Minas,2020-12-31T00:00:00,1.2,1.4462380409240725,20.51983674367269,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Padre_Paraiso,2020-12-31T00:00:00,0.7219512195121951,0.6665312051773071,-7.676420904494617,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Paracatu,2020-12-31T00:00:00,2.7000000000000006,2.7991888523101807,3.673661196673334,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Pedra_Azul,2020-12-31T00:00:00,1.0,1.4036332368850708,40.36332368850708,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Pirapora,2020-12-31T00:00:00,3.6000000000000005,2.8004279136657715,-22.21033573150636,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Pote,2020-12-31T00:00:00,0.6571428571428573,0.9209535717964172,40.14510875162869,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Rio_Pardo_de_Minas,2020-12-31T00:00:00,3.4487179487179493,2.740549325942993,-20.53425746336304,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Santa_Barbara_do_Monte_Verde,2020-12-31T00:00:00,1.6000000000000003,1.1657955646514893,-27.13777720928194,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Santa_Maria_do_Suacui,2020-12-31T00:00:00,0.8,0.8870542049407959,10.881775617599482,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Santo_Antonio_do_Retiro,2020-12-31T00:00:00,0.975,1.701857089996338,74.54944512782956,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Sao_Goncalo_do_Rio_Preto,2020-12-31T00:00:00,3.6000000000000005,3.3728840351104736,-6.3087768024868565,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Sao_Joao_do_Manteninha,2020-12-31T00:00:00,0.9166666666666664,0.8979202508926392,-2.045063538984803,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Sao_Joao_do_Paraiso,2020-12-31T00:00:00,3.146938775510204,2.727969169616699,-13.31356075796481,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Sao_Sebastiao_do_Maranhao,2020-12-31T00:00:00,0.8148148148148149,0.9956505298614502,22.193474119359788,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Senador_Modestino_Goncalves,2020-12-31T00:00:00,3.6000000000000005,1.904953360557556,-47.08462887340122,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Setubinha,2020-12-31T00:00:00,1.0,1.269983172416687,26.9983172416687,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Taiobeiras,2020-12-31T00:00:00,3.240083507306889,2.2532455921173096,-30.457175346379422,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Teofilo_Otoni,2020-12-31T00:00:00,1.09375,1.005064606666565,-8.10837881905692,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Turmalina,2020-12-31T00:00:00,1.320338983050847,2.05833387374878,55.89434987314253,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Urucuia,2020-12-31T00:00:00,2.4,2.438699960708618,1.612498362859094,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Vargem_Grande_do_Rio_Pardo,2020-12-31T00:00:00,1.8000000000000005,2.2336180210113525,24.089890056186235,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_4_2020,Varzea_da_Palma,2020-12-31T00:00:00,3.0,3.672782421112061,22.42608070373535,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2020_(2020)
    Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:34:51
V42_cluster_0_2021,Aguanil,2020-12-31T00:00:00,2.1644859813084114,2.0127973556518555,-7.008066902094769,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Alfenas,2020-12-31T00:00:00,2.364130434782609,2.355599880218506,-0.3608326528264262,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Alto_Caparao,2020-12-31T00:00:00,1.92,1.4923042058944702,-22.27582260966301,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Alto_Jequitiba,2020-12-31T00:00:00,1.8000000000000005,1.4775166511535645,-17.915741602579764,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Amparo_do_Serra,2020-12-31T00:00:00,1.4392156862745098,1.3730084896087646,-4.600227561243873,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Araponga,2020-12-31T00:00:00,1.8000000000000005,1.236244797706604,-31.31973346074423,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Astolfo_Dutra,2020-12-31T00:00:00,1.75,1.4348920583724976,-18.006168093000138,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Bambui,2020-12-31T00:00:00,1.7998829724985372,1.5397062301635742,-14.45520327212132,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Boa_Esperanca,2020-12-31T00:00:00,2.063248407643312,2.3782618045806885,15.267836668159193,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Bom_Jesus_do_Galho,2020-12-31T00:00:00,1.44,1.115467190742493,-22.53700064288245,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Bom_Sucesso,2020-12-31T00:00:00,2.1,1.6719412803649902,-20.383748554048086,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Caete,2020-12-31T00:00:00,1.2,1.296789526939392,8.065793911616012,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Caiana,2020-12-31T00:00:00,1.5,1.220267653465271,-18.64882310231527,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Cajuri,2020-12-31T00:00:00,1.8000000000000005,1.3846778869628906,-23.073450724283862,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Camacho,2020-12-31T00:00:00,2.1,1.941046953201294,-7.569192704700292,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Campo_do_Meio,2020-12-31T00:00:00,2.4,2.2545583248138428,-6.060069799423214,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Campos_Altos,2020-12-31T00:00:00,2.1000529941706416,1.7894669771194458,-14.789437119602464,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Campos_Gerais,2020-12-31T00:00:00,2.690649114843395,2.1667604446411133,-19.47071683602913,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Cana_Verde,2020-12-31T00:00:00,1.5,1.3949708938598633,-7.001940409342448,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Canaa,2020-12-31T00:00:00,1.8000000000000005,1.3882852792739868,-22.87304004033408,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Candeias,2020-12-31T00:00:00,1.859937013094646,1.822766900062561,-1.9984608495015463,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Caparao,2020-12-31T00:00:00,1.9199600798403191,1.5109827518463137,-21.301345391932305,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Capitolio,2020-12-31T00:00:00,2.1,1.6869863271713257,-19.6673177537464,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Caputira,2020-12-31T00:00:00,1.8000000000000005,1.1577130556106567,-35.68260802163019,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Carangola,2020-12-31T00:00:00,1.5,1.3453865051269531,-10.307566324869793,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Caratinga,2020-12-31T00:00:00,2.040022111663903,1.3235845565795898,-35.11910733653594,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Carmo_da_Mata,2020-12-31T00:00:00,1.68,1.278373122215271,-23.90636177290053,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Carmo_do_Rio_Claro,2020-12-31T00:00:00,2.7847803881511743,2.2335329055786133,-19.795007352035263,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Carmopolis_de_Minas,2020-12-31T00:00:00,1.981818181818182,1.6924684047698977,-14.600218107940956,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Cassia,2020-12-31T00:00:00,2.1679245283018864,2.0229597091674805,-6.686801927000449,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Catas_Altas_da_Noruega,2020-12-31T00:00:00,1.0,0.8968635201454163,-10.313647985458374,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Chale,2020-12-31T00:00:00,1.71015625,1.2561249732971191,-26.54911074370432,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Claudio,2020-12-31T00:00:00,2.76,1.4616931676864624,-47.040102620055706,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Conceicao_da_Barra_de_Minas,2020-12-31T00:00:00,1.911042944785276,2.144713878631592,12.227403600946866,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Conceicao_de_Ipanema,2020-12-31T00:00:00,1.500533617929563,1.3423161506652832,-10.544080144141542,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Coqueiral,2020-12-31T00:00:00,2.160028248587571,1.8148152828216555,-15.981872736694456,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Corrego_Danta,2020-12-31T00:00:00,1.8000000000000005,1.5834364891052246,-12.031306160820868,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Cristais,2020-12-31T00:00:00,1.92,1.9782909154891968,3.0359851817290027,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Desterro_de_Entre_Rios,2020-12-31T00:00:00,2.4,2.177901029586792,-9.254123767216996,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Divinesia,2020-12-31T00:00:00,2.1,2.423059225082397,15.383772622971303,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Divino,2020-12-31T00:00:00,1.8000483851457605,1.1913154125213623,-33.817589440803026,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Doresopolis,2020-12-31T00:00:00,1.8000000000000005,1.6799159049987793,-6.671338611178943,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Durande,2020-12-31T00:00:00,2.4,1.5917210578918457,-33.678289254506424,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Eloi_Mendes,2020-12-31T00:00:00,2.109128416709644,2.180802345275879,3.398272385806189,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Entre_Folhas,2020-12-31T00:00:00,1.26046511627907,1.0173006057739258,-19.2916493574192,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Entre_Rios_de_Minas,2020-12-31T00:00:00,1.8000000000000005,2.1827502250671387,21.2639013926188,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ervalia,2020-12-31T00:00:00,1.7999614197530858,1.398492693901062,-22.30429616136418,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Espera_Feliz,2020-12-31T00:00:00,2.04,1.3767622709274292,-32.511653385910336,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Faria_Lemos,2020-12-31T00:00:00,1.44031007751938,1.1682963371276855,-18.88577637811011,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ferros,2020-12-31T00:00:00,1.2,0.9423642754554749,-21.46964371204376,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Fervedouro,2020-12-31T00:00:00,1.5,1.1444194316864014,-23.70537122090657,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Guape,2020-12-31T00:00:00,1.847986942328618,1.930883526802063,4.485777608849792,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ibituruna,2020-12-31T00:00:00,1.8000000000000005,1.7742726802825928,-1.4292955398559717,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ilicinea,2020-12-31T00:00:00,2.09995817649519,2.2972636222839355,9.395684542539149,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Itapecerica,2020-12-31T00:00:00,1.8000000000000005,1.6220906972885132,-9.88385015063817,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ituiutaba,2020-12-31T00:00:00,0.75,1.636192798614502,118.15903981526692,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Itumirim,2020-12-31T00:00:00,2.050724637681159,1.7819855213165283,-13.104592953469634,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Lajinha,2020-12-31T00:00:00,1.9200446677833607,1.277147889137268,-33.48342824692195,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Lavras,2020-12-31T00:00:00,1.8000000000000005,1.943631649017334,7.979536056518538,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Luisburgo,2020-12-31T00:00:00,2.1,1.579090595245361,-24.80520975022089,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Machado,2020-12-31T00:00:00,2.211839166046165,1.776973009109497,-19.66083988439472,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Manhuacu,2020-12-31T00:00:00,1.6800182481751822,1.2447326183319092,-25.90957748917761,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Manhumirim,2020-12-31T00:00:00,1.68,1.4307482242584229,-14.836415222712922,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Martins_Soares,2020-12-31T00:00:00,1.8000000000000005,1.611425280570984,-10.47637330161202,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Matipo,2020-12-31T00:00:00,1.56,1.2357808351516724,-20.783279797969723,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Medeiros,2020-12-31T00:00:00,1.980113636363636,1.745084524154663,-11.869475968085856,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Miradouro,2020-12-31T00:00:00,1.5,1.3137171268463137,-12.41885821024577,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Mirai,2020-12-31T00:00:00,1.5011494252873558,1.1562544107437134,-22.97539530267757,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Moeda,2020-12-31T00:00:00,1.2,1.1816391944885254,-1.5300671259562135,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Muriae,2020-12-31T00:00:00,1.56,1.1955482959747314,-23.3622887195685,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Mutum,2020-12-31T00:00:00,1.65,1.3789148330688477,-16.429404056433473,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Nazareno,2020-12-31T00:00:00,2.1,1.7870153188705444,-14.904032434735983,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Nepomuceno,2020-12-31T00:00:00,1.920040743570155,1.7793339490890503,-7.328323367736051,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Oliveira,2020-12-31T00:00:00,1.947151898734177,1.6642091274261477,-14.531109334200767,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Orizania,2020-12-31T00:00:00,1.679901960784314,1.4337272644042969,-14.654110902107814,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Paraguacu,2020-12-31T00:00:00,2.3413259668508286,1.9456058740615845,-16.901537777821794,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Passa_Tempo,2020-12-31T00:00:00,1.511111111111111,1.620272159576416,7.22389291314518,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Perdoes,2020-12-31T00:00:00,1.92,1.8673382997512813,-2.7427968879540727,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Pimenta,2020-12-31T00:00:00,1.8000000000000005,1.953990459442139,8.555025524563243,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Piranga,2020-12-31T00:00:00,2.4,2.053150415420532,-14.45206602414449,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Piumhi,2020-12-31T00:00:00,1.7399966931216928,1.5909554958343506,-8.5656023299648,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ponte_Nova,2020-12-31T00:00:00,1.333333333333333,1.2483712434768677,-6.372156739234904,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Porto_Firme,2020-12-31T00:00:00,1.56,1.1784613132476809,-24.45760812514868,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Raul_Soares,2020-12-31T00:00:00,1.320083682008368,1.074129343032837,-18.631723301157503,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Reduto,2020-12-31T00:00:00,1.8000000000000005,1.386003017425537,-22.99983236524795,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ribeirao_Vermelho,2020-12-31T00:00:00,1.92,1.644438862800598,-14.352142562468844,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ritapolis,2020-12-31T00:00:00,2.101769911504425,1.821012020111084,-13.35816493787264,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Rosario_da_Limeira,2020-12-31T00:00:00,1.5,1.1489291191101074,-23.404725392659508,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Santa_Barbara_do_Leste,2020-12-31T00:00:00,1.4400953029271608,1.162652850151062,-19.26556195358493,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Santa_Margarida,2020-12-31T00:00:00,1.8000000000000005,1.3348780870437622,-25.84010627534656,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Santa_Rita_de_Minas,2020-12-31T00:00:00,1.5,1.2393651008605957,-17.375659942626953,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Santa_Rita_do_Itueto,2020-12-31T00:00:00,1.77,1.7018533945083618,-3.850090705742272,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Santana_da_Vargem,2020-12-31T00:00:00,2.22,2.179849863052368,-1.8085647273707923,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Santana_do_Jacare,2020-12-31T00:00:00,1.8000000000000005,1.454501986503601,-19.194334083133285,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Santana_do_Manhuacu,2020-12-31T00:00:00,1.8000000000000005,1.4046341180801392,-21.964771217770057,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Domingos_das_Dores,2020-12-31T00:00:00,1.56,1.661757230758667,6.522899407606856,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Francisco_de_Paula,2020-12-31T00:00:00,1.62,1.8496122360229488,14.173594816231423,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Francisco_do_Gloria,2020-12-31T00:00:00,1.679693486590038,1.1353881359100342,-32.40504026630497,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Joao_do_Manhuacu,2020-12-31T00:00:00,1.8000000000000005,1.4182426929473877,-21.20873928070069,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Jose_do_Mantimento,2020-12-31T00:00:00,1.8000000000000005,1.3662385940551758,-24.09785588582358,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Roque_de_Minas,2020-12-31T00:00:00,1.8000000000000005,1.630128264427185,-9.437318642934176,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Tiago,2020-12-31T00:00:00,2.69971671388102,2.0982449054718018,-22.279071182419106,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Tomas_de_Aquino,2020-12-31T00:00:00,2.099966151415999,2.3480663299560547,11.814484646467402,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Senador_Firmino,2020-12-31T00:00:00,1.376237623762376,1.7346224784851074,26.040913904313573,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Senhora_de_Oliveira,2020-12-31T00:00:00,1.8000000000000005,1.8472627401351929,2.6257077852884776,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sericita,2020-12-31T00:00:00,2.5200000000000005,1.2326399087905884,-51.08571790513538,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Simonesia,2020-12-31T00:00:00,1.8000000000000005,1.258967638015747,-30.057353443569617,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Tapirai,2020-12-31T00:00:00,1.799716914366596,1.7084777355194092,-5.069640570628199,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Tombos,2020-12-31T00:00:00,1.5,1.0982437133789062,-26.78375244140625,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Tres_Pontas,2020-12-31T00:00:00,2.26,2.5517492294311523,12.909257939431532,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ubaporanga,2020-12-31T00:00:00,1.6200873362445412,1.2386136054992676,-23.546491736029022,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Vargem_Bonita,2020-12-31T00:00:00,1.8031222896790973,2.290733814239502,27.0426208666737,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Vermelho_Novo,2020-12-31T00:00:00,1.320080321285141,1.1249785423278809,-14.779538472880368,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Vicosa,2020-12-31T00:00:00,1.8000000000000005,1.2894940376281738,-28.361442353990352,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Vieiras,2020-12-31T00:00:00,1.8000000000000005,1.240276575088501,-31.09574582841662,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Aguanil,2021-12-31T00:00:00,1.7382352941176469,1.7624281644821167,1.391806416906893,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Alfenas,2021-12-31T00:00:00,1.6209774981853378,2.054320335388184,26.73342706409973,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Alto_Caparao,2021-12-31T00:00:00,1.08,1.5096445083618164,39.7818989223904,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Alto_Jequitiba,2021-12-31T00:00:00,0.9,1.5213241577148438,69.03601752387154,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Amparo_do_Serra,2021-12-31T00:00:00,1.2,1.2949739694595337,7.914497454961145,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Araponga,2021-12-31T00:00:00,1.2,1.297385334968567,8.115444580713913,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Astolfo_Dutra,2021-12-31T00:00:00,1.75,1.2653586864471436,-27.693789345877512,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Bambui,2021-12-31T00:00:00,1.439910025706941,1.487810492515564,3.3266291610898127,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Boa_Esperanca,2021-12-31T00:00:00,1.570339108544351,1.923084020614624,22.46297695516576,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Bom_Jesus_do_Galho,2021-12-31T00:00:00,1.079802955665025,1.0762622356414795,-0.3279042722535341,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Bom_Sucesso,2021-12-31T00:00:00,1.37989417989418,1.598495364189148,15.841880303584722,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Caete,2021-12-31T00:00:00,1.222222222222222,1.2320467233657837,0.8038228208368577,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Caiana,2021-12-31T00:00:00,0.9,1.221745491027832,35.749499003092446,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Cajuri,2021-12-31T00:00:00,1.2602564102564102,1.3691084384918213,8.637292169239128,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Camacho,2021-12-31T00:00:00,1.5,1.5443140268325806,2.954268455505371,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Campo_do_Meio,2021-12-31T00:00:00,1.255813953488372,1.7958645820617676,43.00403153454816,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Campos_Altos,2021-12-31T00:00:00,1.2,1.7414382696151731,45.11985580126445,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Campos_Gerais,2021-12-31T00:00:00,1.589070422535211,2.013036012649536,26.68010077476165,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Cana_Verde,2021-12-31T00:00:00,1.32,1.3623037338256836,3.2048283201275405,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Canaa,2021-12-31T00:00:00,1.2,1.3364139795303345,11.367831627527876,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Candeias,2021-12-31T00:00:00,1.199963309484498,1.64497709274292,37.085615846837776,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Caparao,2021-12-31T00:00:00,0.9,1.5105640888214111,67.84045431349011,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Capitolio,2021-12-31T00:00:00,1.2,1.5793125629425049,31.609380245208747,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Caputira,2021-12-31T00:00:00,1.38008658008658,1.1599701642990112,-15.949464255623711,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Carangola,2021-12-31T00:00:00,1.02,1.2526357173919678,22.80742327372233,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Caratinga,2021-12-31T00:00:00,1.26,1.3191907405853271,4.697677824232312,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Carmo_da_Mata,2021-12-31T00:00:00,1.32,1.32110857963562,0.083983305728792,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Carmo_do_Rio_Claro,2021-12-31T00:00:00,1.643702081051479,2.1758852005004883,32.37710322229261,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Carmopolis_de_Minas,2021-12-31T00:00:00,1.98,1.6311839818954468,-17.616970611341063,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Cassia,2021-12-31T00:00:00,1.314977578475336,1.7295467853546145,31.526712977110595,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Catas_Altas_da_Noruega,2021-12-31T00:00:00,1.0,0.8756178617477417,-12.43821382522583,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Chale,2021-12-31T00:00:00,1.26016713091922,1.250935673713684,-0.7325581646302776,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Claudio,2021-12-31T00:00:00,1.800653594771242,1.5747945308685305,-12.543171243961854,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Conceicao_da_Barra_de_Minas,2021-12-31T00:00:00,1.462068965517241,1.8273365497589111,24.982924393887824,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Conceicao_de_Ipanema,2021-12-31T00:00:00,1.29,1.3227391242980957,2.537916612255478,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Coqueiral,2021-12-31T00:00:00,1.5,1.7270278930664062,15.13519287109375,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Corrego_Danta,2021-12-31T00:00:00,1.08029197080292,1.460891604423523,35.23118230136662,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Cristais,2021-12-31T00:00:00,1.318776371308017,1.7982609272003174,36.358291392249306,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Desterro_de_Entre_Rios,2021-12-31T00:00:00,1.5,2.005257129669189,33.683808644612625,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Divinesia,2021-12-31T00:00:00,2.1,2.046135902404785,-2.564957028343568,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Divino,2021-12-31T00:00:00,1.08,1.2644065618515017,17.074681652916794,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Doresopolis,2021-12-31T00:00:00,1.2,1.4648160934448242,22.068007787068687,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Durande,2021-12-31T00:00:00,1.32,1.600224494934082,21.22912840409712,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Eloi_Mendes,2021-12-31T00:00:00,1.037885462555066,1.7560687065124512,69.1967726563355,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Entre_Folhas,2021-12-31T00:00:00,0.8400000000000001,1.0210589170455933,21.55463298161824,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Entre_Rios_de_Minas,2021-12-31T00:00:00,1.8000000000000005,2.005397319793701,11.41096221076116,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ervalia,2021-12-31T00:00:00,1.08,1.4380388259887695,33.151743147108284,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Espera_Feliz,2021-12-31T00:00:00,0.78,1.4426484107971191,84.95492446116911,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Faria_Lemos,2021-12-31T00:00:00,0.7199999999999999,1.2166966199874878,68.98564166492889,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ferros,2021-12-31T00:00:00,1.205128205128205,0.8802629113197327,-26.95690735857537,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Fervedouro,2021-12-31T00:00:00,1.15006090133983,1.144633173942566,-0.4719513019650266,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Guape,2021-12-31T00:00:00,1.376842105263158,1.8515617847442627,34.47887580329123,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ibituruna,2021-12-31T00:00:00,1.6000000000000003,1.5952776670455933,-0.2951458096504405,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ilicinea,2021-12-31T00:00:00,1.560035211267606,2.0321474075317383,30.26291925042627,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Itapecerica,2021-12-31T00:00:00,1.222784810126582,1.5129637718200684,23.730991691289248,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ituiutaba,2021-12-31T00:00:00,2.25,1.300914645195007,-42.181571324666336,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Itumirim,2021-12-31T00:00:00,1.2595155709342565,1.5080897808074951,19.73569963004561,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Lajinha,2021-12-31T00:00:00,1.260014255167498,1.3204116821289062,4.793392353595192,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Lavras,2021-12-31T00:00:00,1.259900454447089,1.5433552265167236,22.498187937715223,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Luisburgo,2021-12-31T00:00:00,1.3799999999999997,1.6380103826522827,18.69640454002052,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Machado,2021-12-31T00:00:00,1.444286871961102,1.703050136566162,17.916334325860248,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Manhuacu,2021-12-31T00:00:00,0.9,1.3061832189559937,45.13146877288818,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Manhumirim,2021-12-31T00:00:00,1.2,1.4811643362045288,23.430361350377403,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Martins_Soares,2021-12-31T00:00:00,1.2,1.62553071975708,35.46089331309001,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Matipo,2021-12-31T00:00:00,1.07995337995338,1.2549071311950684,16.2001207171777,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Medeiros,2021-12-31T00:00:00,1.500161864681127,1.5816199779510498,5.429954939378322,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Miradouro,2021-12-31T00:00:00,1.2,1.35516619682312,12.93051640192668,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Mirai,2021-12-31T00:00:00,1.019512195121951,1.1655875444412231,14.327964885383157,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Moeda,2021-12-31T00:00:00,1.8000000000000005,1.1320196390151978,-37.11002005471124,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Muriae,2021-12-31T00:00:00,0.9,1.2596311569213867,39.95901743570963,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Mutum,2021-12-31T00:00:00,1.319934249850568,1.355337619781494,2.6822070822796107,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Nazareno,2021-12-31T00:00:00,1.5,1.804083585739136,20.27223904927572,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Nepomuceno,2021-12-31T00:00:00,1.379962721342032,1.6543742418289185,19.885429964370168,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Oliveira,2021-12-31T00:00:00,1.537805840568272,1.6640393733978271,8.208678202373557,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Orizania,2021-12-31T00:00:00,1.08,1.4826604127883911,37.28337155448065,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Paraguacu,2021-12-31T00:00:00,0.9712280701754386,1.8162815570831297,87.00875858695521,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Passa_Tempo,2021-12-31T00:00:00,1.511627906976744,1.4985519647598269,-0.8650238697345307,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Perdoes,2021-12-31T00:00:00,1.44017094017094,1.6555848121643066,14.957521082031992,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Pimenta,2021-12-31T00:00:00,1.3502325581395351,1.7777953147888184,31.66586037877916,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Piranga,2021-12-31T00:00:00,2.1,1.9125107526779173,-8.928059396289648,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Piumhi,2021-12-31T00:00:00,1.5,1.5246824026107788,1.645493507385254,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ponte_Nova,2021-12-31T00:00:00,1.2,1.2897672653198242,7.480605443318689,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Porto_Firme,2021-12-31T00:00:00,1.319607843137255,1.2438870668411257,-5.738127178458559,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Raul_Soares,2021-12-31T00:00:00,1.320079522862823,1.089897871017456,-17.436953445514998,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Reduto,2021-12-31T00:00:00,1.2,1.41581928730011,17.98494060834249,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ribeirao_Vermelho,2021-12-31T00:00:00,1.258899676375405,1.5712478160858154,24.81120184332051,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ritapolis,2021-12-31T00:00:00,1.5,1.751885175704956,16.79234504699707,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Rosario_da_Limeira,2021-12-31T00:00:00,0.9,1.1556823253631592,28.409147262573235,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Santa_Barbara_do_Leste,2021-12-31T00:00:00,1.319917440660475,1.1190974712371826,-15.2145856427811,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Santa_Margarida,2021-12-31T00:00:00,0.9,1.4351909160614014,59.46565734015571,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Santa_Rita_de_Minas,2021-12-31T00:00:00,1.2,1.230767011642456,2.563917636871342,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Santa_Rita_do_Itueto,2021-12-31T00:00:00,1.3799999999999997,1.7916991710662842,29.8332732656728,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Santana_da_Vargem,2021-12-31T00:00:00,1.44,1.8708871603012085,29.92271946536171,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Santana_do_Jacare,2021-12-31T00:00:00,1.2,1.489365816116333,24.11381800969442,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Santana_do_Manhuacu,2021-12-31T00:00:00,1.32,1.428101658821106,8.189519607659538,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Domingos_das_Dores,2021-12-31T00:00:00,1.2,1.6338200569152832,36.15167140960694,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Francisco_de_Paula,2021-12-31T00:00:00,1.3200867052023115,1.488976001739502,12.793803306375004,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Francisco_do_Gloria,2021-12-31T00:00:00,0.78,1.1910498142242432,52.69869413131322,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Joao_do_Manhuacu,2021-12-31T00:00:00,0.9,1.4933600425720217,65.9288936191135,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Jose_do_Mantimento,2021-12-31T00:00:00,1.500917431192661,1.3890588283538818,-7.452681973977336,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Roque_de_Minas,2021-12-31T00:00:00,1.2,1.4200572967529297,18.338108062744148,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Tiago,2021-12-31T00:00:00,1.5,1.990339636802673,32.68930912017822,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sao_Tomas_de_Aquino,2021-12-31T00:00:00,1.2,1.7496943473815918,45.80786228179932,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Senador_Firmino,2021-12-31T00:00:00,1.563636363636364,1.5386826992034912,-1.5958738881488368,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Senhora_de_Oliveira,2021-12-31T00:00:00,1.62037037037037,1.5810086727142334,-2.429179055350146,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Sericita,2021-12-31T00:00:00,0.9,1.3683927059173584,52.0436339908176,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Simonesia,2021-12-31T00:00:00,1.08,1.2873413562774658,19.198273729394977,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Tapirai,2021-12-31T00:00:00,1.396563119629874,1.6298494338989258,16.70431583005563,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Tombos,2021-12-31T00:00:00,0.8497959183673469,1.1578011512756348,36.24461194165731,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Tres_Pontas,2021-12-31T00:00:00,1.3500296384113808,1.8953930139541624,40.39640019937095,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Ubaporanga,2021-12-31T00:00:00,1.319867549668874,1.3304098844528198,0.7987418727425012,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Vargem_Bonita,2021-12-31T00:00:00,1.519565217391304,1.6947247982025146,11.526953815902283,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Vermelho_Novo,2021-12-31T00:00:00,0.96,1.1657397747039795,21.43122653166455,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Vicosa,2021-12-31T00:00:00,0.9006711409395973,1.310544490814209,45.50754778786672,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_0_2021,Vieiras,2021-12-31T00:00:00,0.78,1.236966609954834,58.5854628147223,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:25
V42_cluster_1_2021,Almenara,2020-12-31T00:00:00,0.78,0.8422830700874329,7.985008985568312,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Alpinopolis,2020-12-31T00:00:00,2.765998457979954,2.364647626876831,-14.510161057581897,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Alterosa,2020-12-31T00:00:00,1.980023501762632,1.7063602209091189,-13.821213768922238,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Andradas,2020-12-31T00:00:00,2.568,1.5615314245224,-39.19270153728973,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Araguari,2020-12-31T00:00:00,1.979964695498676,2.289541959762573,15.635494156421156,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Arapua,2020-12-31T00:00:00,1.5,1.5455825328826904,3.038835525512696,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Araxa,2020-12-31T00:00:00,1.786008230452675,1.7196674346923828,-3.714473126670737,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Arceburgo,2020-12-31T00:00:00,1.8387096774193543,1.5778790712356567,-14.185524195955496,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Areado,2020-12-31T00:00:00,2.078746484531941,1.776653528213501,-14.53245783294544,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Bom_Jesus_da_Penha,2020-12-31T00:00:00,1.986087924318308,2.1554088592529297,8.525349399762273,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Botelhos,2020-12-31T00:00:00,1.716062736614386,1.672345757484436,-2.5475163697219454,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Botumirim,2020-12-31T00:00:00,0.9200000000000002,0.6968790292739868,-24.25227942674057,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Cabo_Verde,2020-12-31T00:00:00,2.249052581714827,1.7777345180511477,-20.9562936631884,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Campestre,2020-12-31T00:00:00,2.138000770416025,1.6845982074737549,-21.20684750052941,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Capetinga,2020-12-31T00:00:00,2.213808463251671,1.7396831512451172,-21.41672687031616,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Carmo_do_Paranaiba,2020-12-31T00:00:00,1.947480785653288,1.9733881950378416,1.3303037223991518,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Cascalho_Rico,2020-12-31T00:00:00,2.478504672897196,1.9689887762069704,-20.55739100522404,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Claraval,2020-12-31T00:00:00,2.796008294453085,1.556173324584961,-44.34303618940597,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Conceicao_da_Aparecida,2020-12-31T00:00:00,2.243948871362524,2.1214427947998047,-5.459396964260317,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Coromandel,2020-12-31T00:00:00,1.818934240362812,1.9879240989685056,9.290597474925043,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Corrego_Fundo,2020-12-31T00:00:00,1.5,1.3278189897537231,-11.478734016418455,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Cruzeiro_da_Fortaleza,2020-12-31T00:00:00,2.1,1.9056041240692136,-9.25694647289458,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Delfinopolis,2020-12-31T00:00:00,1.8000000000000005,1.442164421081543,-19.879754384358737,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Divisa_Nova,2020-12-31T00:00:00,1.8000000000000005,1.7261528968811035,-4.102616839938708,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Dom_Cavati,2020-12-31T00:00:00,0.7894736842105263,0.9887135028839112,25.237043698628742,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Dores_do_Turvo,2020-12-31T00:00:00,1.625,1.919588327407837,18.12851245586689,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Estrela_do_Indaia,2020-12-31T00:00:00,1.8000000000000005,1.5892155170440674,-11.71024905310738,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Estrela_do_Sul,2020-12-31T00:00:00,2.0627027027027025,1.8065024614334104,-12.420609181032232,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Guaranesia,2020-12-31T00:00:00,1.564686285397002,1.6954994201660156,8.36034264439295,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Guarani,2020-12-31T00:00:00,1.127272727272727,2.957077741622925,162.32141256332403,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Guarda-Mor,2020-12-31T00:00:00,1.7994350282485878,2.013373613357544,11.889208654406666,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Guaxupe,2020-12-31T00:00:00,1.7400000000000002,1.5567140579223633,-10.533674832048098,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Guimarania,2020-12-31T00:00:00,2.149930843706777,1.7350103855133057,-19.299246736610893,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Ibia,2020-12-31T00:00:00,1.746,1.569389581680298,-10.115144233659915,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Ibiraci,2020-12-31T00:00:00,2.6315194346289745,1.4001809358596802,-46.79192114508948,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Indianopolis,2020-12-31T00:00:00,1.8000000000000005,2.2045223712921143,22.47346507178411,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Itamogi,2020-12-31T00:00:00,2.400049176297025,1.738595962524414,-27.55998586633756,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Itanhomi,2020-12-31T00:00:00,1.26,1.158897876739502,-8.023978036547465,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Jacui,2020-12-31T00:00:00,1.68,1.7467130422592163,3.971014420191451,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Jacutinga,2020-12-31T00:00:00,1.8000000000000005,1.436225414276123,-20.209699206882064,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Juruaia,2020-12-31T00:00:00,1.92,1.688136339187622,-12.076232333978014,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Lagoa_Dourada,2020-12-31T00:00:00,2.1,2.8179526329040527,34.188220614478695,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Lagoa_Formosa,2020-12-31T00:00:00,2.5494505494505484,2.056779384613037,-19.32460172422997,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Lagoa_Grande,2020-12-31T00:00:00,2.4,1.7996826171875,-25.013224283854164,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Matutina,2020-12-31T00:00:00,1.82995951417004,1.6796083450317385,-8.21609264981429,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Monte_Belo,2020-12-31T00:00:00,1.8000000000000005,1.753407001495361,-2.588499916924385,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Monte_Carmelo,2020-12-31T00:00:00,1.98,2.0059845447540283,1.3123507451529577,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Monte_Santo_de_Minas,2020-12-31T00:00:00,2.123987903619159,1.5677855014801023,-26.186702908774485,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Muzambinho,2020-12-31T00:00:00,1.764006791171477,1.6249897480010986,-7.880754420341948,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Nova_Resende,2020-12-31T00:00:00,2.353040067245727,2.058694362640381,-12.509166703220764,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Passos,2020-12-31T00:00:00,1.62,2.1277201175689697,31.340747998084545,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Patis,2020-12-31T00:00:00,3.6000000000000005,1.7418432235717771,-51.61546601189508,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Patos_de_Minas,2020-12-31T00:00:00,1.941759465478842,1.8289669752120967,-5.808777671591259,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Patrocinio,2020-12-31T00:00:00,1.7479986236953782,1.7262790203094482,-1.242541217796461,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Pedrinopolis,2020-12-31T00:00:00,2.158490566037736,1.4267818927764893,-33.899090631858456,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Perdizes,2020-12-31T00:00:00,2.050845253576073,1.7696874141693115,-13.70936392770271,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Piracema,2020-12-31T00:00:00,1.333333333333333,1.2702125310897827,-4.734060168266276,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Ponto_dos_Volantes,2020-12-31T00:00:00,0.631578947368421,0.5264063477516174,-16.65232827266057,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Pratapolis,2020-12-31T00:00:00,2.402298850574712,1.872759461402893,-22.04302720475992,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Pratinha,2020-12-31T00:00:00,2.1,1.7312614917755127,-17.558976582118447,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Presidente_Kubitschek,2020-12-31T00:00:00,1.789473684210526,1.1579495668411257,-35.2910536177018,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Presidente_Olegario,2020-12-31T00:00:00,1.9814229249011863,2.089024305343628,5.43051052302766,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Quartel_Geral,2020-12-31T00:00:00,3.6000000000000005,2.9056925773620605,-19.28631729549833,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Rio_Paranaiba,2020-12-31T00:00:00,1.818897637795276,1.6327476501464844,-10.234220100171656,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Rio_Vermelho,2020-12-31T00:00:00,0.9333333333333332,1.1181801557540894,19.80501668793817,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Romaria,2020-12-31T00:00:00,2.330769230769231,2.039274215698242,-12.506386785223938,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Sacramento,2020-12-31T00:00:00,1.8321428571428573,1.540656566619873,-15.909583108466975,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Santa_Rosa_da_Serra,2020-12-31T00:00:00,2.1,1.6531596183776855,-21.278113410586403,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Santo_Antonio_do_Amparo,2020-12-31T00:00:00,2.1,2.4241387844085693,15.435180209931868,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Sao_Goncalo_do_Abaete,2020-12-31T00:00:00,2.44,2.155958890914917,-11.641029060864057,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Sao_Gotardo,2020-12-31T00:00:00,1.38027397260274,1.659898042678833,20.25859181773996,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Sao_Joao_Batista_do_Gloria,2020-12-31T00:00:00,1.5,1.6773643493652344,11.824289957682293,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Sao_Pedro_da_Uniao,2020-12-31T00:00:00,2.1,1.735469102859497,-17.358614149547762,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Sao_Sebastiao_do_Paraiso,2020-12-31T00:00:00,1.929071661237785,1.5451114177703855,-19.90388699303332,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Serra_do_Salitre,2020-12-31T00:00:00,1.8000000000000005,1.842215061187744,2.3452811770968816,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Tapira,2020-12-31T00:00:00,2.1013698630137,1.884981751441956,-10.297478581966956,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Tiros,2020-12-31T00:00:00,1.740235294117647,1.9400570392608645,11.482455609230284,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Tupaciguara,2020-12-31T00:00:00,2.1,2.2380151748657227,6.572151184082027,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Uberaba,2020-12-31T00:00:00,2.5325581395348844,2.7503952980041504,8.601467230650538,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Uberlandia,2020-12-31T00:00:00,1.920754716981132,1.907886266708374,-0.6699684326681394,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Unai,2020-12-31T00:00:00,2.3398907103825146,2.7203733921051025,16.260703118924233,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Varginha,2020-12-31T00:00:00,1.899977968715576,1.9045668840408323,0.2415246598021955,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Varjao_de_Minas,2020-12-31T00:00:00,2.519786096256685,2.4616706371307373,-2.306364782799712,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Virginopolis,2020-12-31T00:00:00,1.626666666666667,1.465432047843933,-9.911964271889373,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Almenara,2021-12-31T00:00:00,0.78,0.8537800908088684,9.458986001136973,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Alpinopolis,2021-12-31T00:00:00,1.67993145468393,2.1543731689453125,28.24173051457305,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Alterosa,2021-12-31T00:00:00,1.08,1.5386652946472168,42.46900876363118,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Andradas,2021-12-31T00:00:00,1.26,2.0254337787628174,60.7487126002236,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Araguari,2021-12-31T00:00:00,1.8600340136054423,1.921703100204468,3.315481660439516,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Arapua,2021-12-31T00:00:00,1.32,1.399088740348816,5.991571238546656,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Araxa,2021-12-31T00:00:00,1.250440917107584,1.613055944442749,28.998973272078764,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Arceburgo,2021-12-31T00:00:00,1.354903268845897,1.409875512123108,4.057281766250076,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Areado,2021-12-31T00:00:00,1.5228426395939092,1.6760683059692385,10.061818758646606,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Bom_Jesus_da_Penha,2021-12-31T00:00:00,1.519900497512438,1.9174659252166748,26.15733255926403,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Botelhos,2021-12-31T00:00:00,1.26,1.930088758468628,53.18164749751016,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Botumirim,2021-12-31T00:00:00,0.7241379310344828,0.6755814552307129,-6.705418087187268,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Cabo_Verde,2021-12-31T00:00:00,1.32,1.853694558143616,40.43140591997088,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Campestre,2021-12-31T00:00:00,1.2899602385685882,1.7755866050720217,37.646615142363714,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Capetinga,2021-12-31T00:00:00,1.6399999999999997,1.7679927349090576,7.804435055430363,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Carmo_do_Paranaiba,2021-12-31T00:00:00,1.541078066914498,1.861457347869873,20.789296002170047,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Cascalho_Rico,2021-12-31T00:00:00,1.9411764705882348,1.863092064857483,-4.022529992190252,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Claraval,2021-12-31T00:00:00,1.050113378684807,1.6487131118774414,57.00334319541177,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Conceicao_da_Aparecida,2021-12-31T00:00:00,1.62,1.987415075302124,22.67994291988419,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Coromandel,2021-12-31T00:00:00,1.249939246658566,1.773897647857666,41.91870945726252,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Corrego_Fundo,2021-12-31T00:00:00,1.1984126984126982,1.5737519264221191,31.319697171647054,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Cruzeiro_da_Fortaleza,2021-12-31T00:00:00,1.800184162062615,1.8515076637268064,2.851013954338235,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Delfinopolis,2021-12-31T00:00:00,0.9,1.4482626914978027,60.91807683308919,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Divisa_Nova,2021-12-31T00:00:00,1.2601319509896318,1.5055911540985107,19.4788492519462,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Dom_Cavati,2021-12-31T00:00:00,0.6000000000000001,0.8486957550048828,41.44929250081378,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Dores_do_Turvo,2021-12-31T00:00:00,1.5,1.706324577331543,13.754971822102863,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Estrela_do_Indaia,2021-12-31T00:00:00,1.8000000000000005,1.5205442905426023,-15.525317192077653,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Estrela_do_Sul,2021-12-31T00:00:00,1.8000000000000005,1.6332406997680664,-9.264405568440768,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Guaranesia,2021-12-31T00:00:00,1.031014249790444,1.469756841659546,42.55446439836087,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Guarani,2021-12-31T00:00:00,0.953846153846154,1.882786989212036,97.38895854642308,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Guarda-Mor,2021-12-31T00:00:00,1.7994350282485878,1.7501978874206543,-2.736255549935394,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Guaxupe,2021-12-31T00:00:00,1.02,1.4775893688201904,44.861702825508864,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Guimarania,2021-12-31T00:00:00,1.541380188439012,1.680241584777832,9.00889977569051,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Ibia,2021-12-31T00:00:00,1.2,1.5171501636505127,26.4291803042094,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Ibiraci,2021-12-31T00:00:00,1.327034071867436,1.7472002506256104,31.662049051001816,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Indianopolis,2021-12-31T00:00:00,1.919956379498364,1.829810619354248,-4.695198344436794,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Itamogi,2021-12-31T00:00:00,1.260064724919094,1.797511100769043,42.65228326937391,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Itanhomi,2021-12-31T00:00:00,0.9606060606060604,1.0728806257247925,11.687888482391664,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Jacui,2021-12-31T00:00:00,0.96,1.6385717391967771,70.684556166331,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Jacutinga,2021-12-31T00:00:00,1.32014652014652,1.4542362689971924,10.157186858000443,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Juruaia,2021-12-31T00:00:00,1.5,1.663921356201172,10.928090413411455,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Lagoa_Dourada,2021-12-31T00:00:00,1.2,2.361643075942993,96.8035896619161,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Lagoa_Formosa,2021-12-31T00:00:00,1.4830188679245278,1.9391140937805176,30.75451268494589,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Lagoa_Grande,2021-12-31T00:00:00,0.6000000000000001,1.748956561088562,191.4927601814269,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Matutina,2021-12-31T00:00:00,1.502024291497976,1.6402926445007324,9.205467167568957,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Monte_Belo,2021-12-31T00:00:00,1.56,1.5205979347229004,-2.5257734151986964,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Monte_Carmelo,2021-12-31T00:00:00,1.8000000000000005,1.6736572980880735,-7.019038995107028,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Monte_Santo_de_Minas,2021-12-31T00:00:00,1.200018932222643,1.4793118238449097,23.27404044409264,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Muzambinho,2021-12-31T00:00:00,1.199937762564182,1.5484750270843506,29.0462785149264,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Nova_Resende,2021-12-31T00:00:00,1.3800031392246113,1.943296909332276,40.81829628475806,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Passos,2021-12-31T00:00:00,0.99,1.5189597606658936,53.43027885514077,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Patis,2021-12-31T00:00:00,2.111111111111112,2.150562286376953,1.8687398810135447,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Patos_de_Minas,2021-12-31T00:00:00,1.7224109589041097,1.620168685913086,-5.935997588872504,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Patrocinio,2021-12-31T00:00:00,1.4369951534733445,1.537778615951538,7.013486596290271,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Pedrinopolis,2021-12-31T00:00:00,1.514285714285714,1.1481057405471802,-24.181696378959785,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Perdizes,2021-12-31T00:00:00,1.261308677098151,1.629408359527588,29.18394910881856,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Piracema,2021-12-31T00:00:00,1.2,1.282270431518555,6.855869293212895,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Ponto_dos_Volantes,2021-12-31T00:00:00,0.5375,0.515101969242096,-4.167075489842611,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Pratapolis,2021-12-31T00:00:00,0.9602272727272728,1.8645248413085933,94.17536808894228,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Pratinha,2021-12-31T00:00:00,1.35,1.6652653217315674,23.352986794930903,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Presidente_Kubitschek,2021-12-31T00:00:00,0.7368421052631579,1.2645539045333862,71.61802990095957,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Presidente_Olegario,2021-12-31T00:00:00,1.637164750957854,1.855167269706726,13.31585710120656,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Quartel_Geral,2021-12-31T00:00:00,2.4571428571428573,2.6423702239990234,7.538323069727691,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Rio_Paranaiba,2021-12-31T00:00:00,1.080031384856807,1.448955535888672,34.15865096186789,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Rio_Vermelho,2021-12-31T00:00:00,0.7777777777777777,1.0104467868804932,29.91458688463485,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Romaria,2021-12-31T00:00:00,2.052631578947369,1.6913763284683228,-17.59961476692789,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Sacramento,2021-12-31T00:00:00,1.25645342312009,1.277961015701294,1.7117699856947572,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Santa_Rosa_da_Serra,2021-12-31T00:00:00,1.5,1.5773913860321045,5.159425735473633,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Santo_Antonio_do_Amparo,2021-12-31T00:00:00,1.3799999999999997,2.064091682434082,49.571861045948005,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Sao_Goncalo_do_Abaete,2021-12-31T00:00:00,1.67887323943662,2.00227427482605,19.262981134773085,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Sao_Gotardo,2021-12-31T00:00:00,1.26027397260274,1.4913859367370603,18.338231936744982,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Sao_Joao_Batista_do_Gloria,2021-12-31T00:00:00,1.110749185667752,1.360412836074829,22.47705005131164,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Sao_Pedro_da_Uniao,2021-12-31T00:00:00,1.479945054945055,1.686941146850586,13.986741684353676,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Sao_Sebastiao_do_Paraiso,2021-12-31T00:00:00,1.14450771643146,1.551635980606079,35.57234768534654,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Serra_do_Salitre,2021-12-31T00:00:00,1.4676384839650147,1.5511741638183594,5.691843104826621,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Tapira,2021-12-31T00:00:00,0.6000000000000001,1.5815787315368652,163.59645525614417,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Tiros,2021-12-31T00:00:00,1.2,1.7346296310424805,44.552469253540046,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Tupaciguara,2021-12-31T00:00:00,1.887700534759358,2.1067028045654297,11.601536672446288,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Uberaba,2021-12-31T00:00:00,2.0302325581395344,1.7877347469329834,-11.94433663445784,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Uberlandia,2021-12-31T00:00:00,1.77962962962963,1.639275074005127,-7.886728411782682,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Unai,2021-12-31T00:00:00,2.639892904953146,2.513801097869873,-4.7763985746047055,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Varginha,2021-12-31T00:00:00,1.2,1.7521262168884275,46.01051807403565,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Varjao_de_Minas,2021-12-31T00:00:00,2.390254805543138,2.393399238586426,0.1315522109189191,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_1_2021,Virginopolis,2021-12-31T00:00:00,1.202380952380952,1.2580760717391968,4.632069332764904,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:35:59
V42_cluster_2_2021,Abre_Campo,2020-12-31T00:00:00,1.5,1.191279411315918,-20.581372578938804,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Aimores,2020-12-31T00:00:00,2.063157894736842,1.4622583389282229,-29.125233572356542,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Alvarenga,2020-12-31T00:00:00,0.9,0.7313499450683594,-18.73889499240452,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Andrelandia,2020-12-31T00:00:00,1.8000000000000005,1.5330431461334229,-14.830936325920966,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Antonio_Dias,2020-12-31T00:00:00,1.333333333333333,0.939071774482727,-29.569616913795453,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Antonio_Prado_de_Minas,2020-12-31T00:00:00,1.5,0.8084810972213745,-46.1012601852417,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Bocaiuva,2020-12-31T00:00:00,1.622950819672131,1.8211551904678345,12.212592543977694,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Bom_Jesus_do_Amparo,2020-12-31T00:00:00,1.5,1.2073702812194824,-19.50864791870117,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Campo_Belo,2020-12-31T00:00:00,1.8000000000000005,1.31792151927948,-26.78213781780668,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Capela_Nova,2020-12-31T00:00:00,1.8000000000000005,1.9500623941421509,8.336799674563922,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Capitao_Eneas,2020-12-31T00:00:00,3.0,1.2357767820358276,-58.80744059880575,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Casa_Grande,2020-12-31T00:00:00,1.553846153846154,1.3762632608413696,-11.428602025060384,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Cataguases,2020-12-31T00:00:00,1.25,0.5216884613037109,-58.264923095703125,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Caxambu,2020-12-31T00:00:00,1.3265306122448983,1.0329082012176514,-22.13461252359245,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Coimbra,2020-12-31T00:00:00,1.8000000000000005,1.529738426208496,-15.014531877305783,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Conselheiro_Pena,2020-12-31T00:00:00,1.55981308411215,1.299663543701172,-16.6782509430645,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Corrego_Novo,2020-12-31T00:00:00,1.2545454545454553,0.8735906481742859,-30.3659628266874,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Cruzilia,2020-12-31T00:00:00,1.2,1.1936653852462769,-0.5278845628102584,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Divisopolis,2020-12-31T00:00:00,1.34029484029484,1.2915529012680054,-3.636657962222128,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Esmeraldas,2020-12-31T00:00:00,2.096153846153846,1.647260665893555,-21.41508749865609,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Espirito_Santo_do_Dourado,2020-12-31T00:00:00,1.619148936170213,1.5334330797195437,-5.293883381315986,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Eugenopolis,2020-12-31T00:00:00,1.8000000000000005,1.064549207687378,-40.85837735070123,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Felicio_dos_Santos,2020-12-31T00:00:00,3.548821548821549,1.6628117561340332,-53.14467821899356,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Formiga,2020-12-31T00:00:00,1.441791044776119,1.7368563413619995,20.46519137810972,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Guaraciaba,2020-12-31T00:00:00,1.2,1.222559928894043,1.8799940745035848,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Guiricema,2020-12-31T00:00:00,2.4,1.6253323554992676,-32.277818520863846,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Iapu,2020-12-31T00:00:00,1.0,1.8642131090164185,86.42131090164185,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Ijaci,2020-12-31T00:00:00,2.458064516129032,1.5684953927993774,-36.18981997797808,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Imbe_de_Minas,2020-12-31T00:00:00,1.439877300613497,1.0824328660964966,-24.82464543087817,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Inhapim,2020-12-31T00:00:00,1.8000000000000005,1.5455632209777832,-14.135376612345391,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Irai_de_Minas,2020-12-31T00:00:00,3.3595092024539883,1.521657943725586,-54.70594506441372,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Itamarati_de_Minas,2020-12-31T00:00:00,1.2,0.8237571716308594,-31.35356903076172,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Jequeri,2020-12-31T00:00:00,2.1,1.355870008468628,-35.43476150149391,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Lamim,2020-12-31T00:00:00,2.387096774193548,1.4368624687194824,-39.80711279688654,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Mar_de_Espanha,2020-12-31T00:00:00,1.8000000000000005,1.648182034492493,-8.434331417083754,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Mata_Verde,2020-12-31T00:00:00,1.319688109161793,1.3051557540893557,-1.101196182003015,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Monte_Alegre_de_Minas,2020-12-31T00:00:00,3.0,1.8219897747039795,-39.26700750986736,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Nova_Era,2020-12-31T00:00:00,1.565217391304348,1.4817978143692017,-5.329584081967676,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Nova_Ponte,2020-12-31T00:00:00,1.8591549295774648,1.3861188888549805,-25.44360522067908,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Paula_Candido,2020-12-31T00:00:00,1.62,1.343976616859436,-17.038480440775558,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Pecanha,2020-12-31T00:00:00,2.0,1.4670459032058716,-26.647704839706403,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Pedra_Bonita,2020-12-31T00:00:00,1.7400000000000002,1.070328950881958,-38.48684190333576,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Pedra_Dourada,2020-12-31T00:00:00,1.44,1.0529396533966064,-26.87919073634677,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Pedra_do_Anta,2020-12-31T00:00:00,1.5,1.2797017097473145,-14.68655268351237,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Piedade_de_Caratinga,2020-12-31T00:00:00,1.8000000000000005,1.363839864730835,-24.23111862606473,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Pocrane,2020-12-31T00:00:00,1.258064516129032,1.1169179677963257,-11.219341021317684,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Presidente_Bernardes,2020-12-31T00:00:00,1.5,1.3644802570343018,-9.03464953104655,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sabinopolis,2020-12-31T00:00:00,1.0,1.2487890720367432,24.87890720367432,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Santa_Barbara,2020-12-31T00:00:00,1.142857142857143,1.059715747833252,-7.274872064590466,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Santo_Antonio_do_Grama,2020-12-31T00:00:00,1.695652173913043,1.55137300491333,-8.508771505111277,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sao_Domingos_do_Prata,2020-12-31T00:00:00,0.8846153846153846,0.8021076321601868,-9.326963321022362,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sao_Geraldo,2020-12-31T00:00:00,1.8000000000000005,0.9601606726646424,-46.657740407519874,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sao_Jose_da_Barra,2020-12-31T00:00:00,2.537024221453287,1.795793533325195,-29.21653967117001,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sao_Jose_do_Alegre,2020-12-31T00:00:00,1.359090909090909,1.264275074005127,-6.976415959488974,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sao_Miguel_do_Anta,2020-12-31T00:00:00,1.5,1.2408506870269775,-17.276620864868164,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sao_Sebastiao_da_Vargem_Alegre,2020-12-31T00:00:00,1.8000000000000005,1.03456711769104,-42.52404901716445,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sao_Sebastiao_do_Anta,2020-12-31T00:00:00,1.8000000000000005,1.7725791931152344,-1.5233781602647716,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Senhora_dos_Remedios,2020-12-31T00:00:00,1.8000000000000005,1.494159460067749,-16.99114110734729,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Teixeiras,2020-12-31T00:00:00,1.5,1.1061620712280271,-26.25586191813151,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Visconde_do_Rio_Branco,2020-12-31T00:00:00,1.0,0.85477215051651,-14.522784948349,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Abre_Campo,2021-12-31T00:00:00,1.2,1.5871424674987793,32.26187229156495,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Aimores,2021-12-31T00:00:00,1.634328358208955,2.0785186290740967,27.17876543193105,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Alvarenga,2021-12-31T00:00:00,0.6304878048780488,0.8453632593154907,34.0808264291494,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Andrelandia,2021-12-31T00:00:00,1.4,1.8059568405151367,28.99691717965263,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Antonio_Dias,2021-12-31T00:00:00,1.5,1.0745525360107422,-28.36316426595052,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Antonio_Prado_de_Minas,2021-12-31T00:00:00,0.9,1.04012131690979,15.569035212198887,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Bocaiuva,2021-12-31T00:00:00,1.7407407407407405,1.8195642232894893,4.528157508119639,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Bom_Jesus_do_Amparo,2021-12-31T00:00:00,1.2,1.5965170860290527,33.043090502421066,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Campo_Belo,2021-12-31T00:00:00,1.5,1.6042640209197998,6.950934727986653,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Capela_Nova,2021-12-31T00:00:00,1.8000000000000005,2.043242692947388,13.51348294152152,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Capitao_Eneas,2021-12-31T00:00:00,2.0,4.398775100708008,119.93875503540043,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Casa_Grande,2021-12-31T00:00:00,1.3230769230769233,1.5245575904846191,15.22818997848864,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Cataguases,2021-12-31T00:00:00,1.25,1.0042531490325928,-19.65974807739257,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Caxambu,2021-12-31T00:00:00,1.4387755102040822,1.4581865072250366,1.3491331262790989,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Coimbra,2021-12-31T00:00:00,1.501182033096927,1.7304258346557615,15.270886308564894,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Conselheiro_Pena,2021-12-31T00:00:00,1.2898734177215188,1.851168751716613,43.5155361978533,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Corrego_Novo,2021-12-31T00:00:00,1.8000000000000005,1.0814247131347656,-39.92084927029081,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Cruzilia,2021-12-31T00:00:00,1.5027322404371577,1.5045125484466553,0.1184714057229297,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Divisopolis,2021-12-31T00:00:00,0.9595505617977528,1.8380515575408936,91.55338246035076,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Esmeraldas,2021-12-31T00:00:00,1.7115384615384608,1.9437482357025144,13.56731264778742,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Espirito_Santo_do_Dourado,2021-12-31T00:00:00,1.200854700854701,1.4259932041168213,18.748188527877627,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Eugenopolis,2021-12-31T00:00:00,1.5,1.2450840473175049,-16.994396845499672,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Felicio_dos_Santos,2021-12-31T00:00:00,3.542087542087541,3.754671812057495,6.001666176908387,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Formiga,2021-12-31T00:00:00,1.347576301615799,1.9213206768035889,42.57602144678908,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Guaraciaba,2021-12-31T00:00:00,2.4,1.2128119468688965,-49.46616888046265,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Guiricema,2021-12-31T00:00:00,2.1,1.9129582643508911,-8.906749316624236,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Iapu,2021-12-31T00:00:00,1.5578947368421048,1.5111117362976074,-3.002962872788683,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Ijaci,2021-12-31T00:00:00,1.2375,1.6178661584854126,30.73665927154849,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Imbe_de_Minas,2021-12-31T00:00:00,1.2,1.3159539699554443,9.662830829620365,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Inhapim,2021-12-31T00:00:00,1.2,1.9339611530303955,61.16342941919963,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Irai_de_Minas,2021-12-31T00:00:00,2.472300469483568,2.9217820167541504,18.180700639695026,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Itamarati_de_Minas,2021-12-31T00:00:00,1.2,1.144958734512329,-4.586772123972571,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Jequeri,2021-12-31T00:00:00,1.5,1.8799484968185425,25.329899787902832,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Lamim,2021-12-31T00:00:00,1.2800000000000002,1.938997745513916,51.48419886827465,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Mar_de_Espanha,2021-12-31T00:00:00,1.4199999999999997,1.9297621250152588,35.89874119825769,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Mata_Verde,2021-12-31T00:00:00,1.319277108433735,2.053197860717773,55.63052277586776,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Monte_Alegre_de_Minas,2021-12-31T00:00:00,2.765957446808511,3.305941104888916,19.52248609983003,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Nova_Era,2021-12-31T00:00:00,1.5,1.5323374271392822,2.1558284759521484,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Nova_Ponte,2021-12-31T00:00:00,2.5482352941176467,1.831925392150879,-28.110037704143707,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Paula_Candido,2021-12-31T00:00:00,1.5,1.48716402053833,-0.8557319641113281,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Pecanha,2021-12-31T00:00:00,1.807692307692308,2.059986114501953,13.9566786745761,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Pedra_Bonita,2021-12-31T00:00:00,1.08,1.684086561203003,55.93394085212989,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Pedra_Dourada,2021-12-31T00:00:00,0.96,1.217808485031128,26.85505052407585,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Pedra_do_Anta,2021-12-31T00:00:00,0.9,1.4209357500076294,57.88175000084771,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Piedade_de_Caratinga,2021-12-31T00:00:00,1.2,1.6647921800613403,38.732681671778366,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Pocrane,2021-12-31T00:00:00,1.3228346456692908,1.227889060974121,-7.177434081122948,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Presidente_Bernardes,2021-12-31T00:00:00,1.56,1.532914400100708,-1.7362564038007722,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sabinopolis,2021-12-31T00:00:00,1.2,1.552649736404419,29.387478033701587,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Santa_Barbara,2021-12-31T00:00:00,0.9333333333333332,1.1571791172027588,23.983476843152754,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Santo_Antonio_do_Grama,2021-12-31T00:00:00,1.9130434782608696,2.2048699855804443,15.254567428068665,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sao_Domingos_do_Prata,2021-12-31T00:00:00,0.903846153846154,0.8899366855621338,-1.538919895253284,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sao_Geraldo,2021-12-31T00:00:00,1.5,1.4217336177825928,-5.217758814493815,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sao_Jose_da_Barra,2021-12-31T00:00:00,1.682068965517241,2.022695541381836,20.25045243967457,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sao_Jose_do_Alegre,2021-12-31T00:00:00,1.7823529411764714,1.366068959236145,-23.35586697354965,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sao_Miguel_do_Anta,2021-12-31T00:00:00,1.2603053435114495,1.4296929836273191,13.440206453772822,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sao_Sebastiao_da_Vargem_Alegre,2021-12-31T00:00:00,1.020238095238095,1.4133496284484863,38.5313521466428,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Sao_Sebastiao_do_Anta,2021-12-31T00:00:00,1.5,1.8574333190917969,23.828887939453125,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Senhora_dos_Remedios,2021-12-31T00:00:00,1.5,1.7233951091766355,14.893007278442385,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Teixeiras,2021-12-31T00:00:00,1.32,1.2927190065383911,-2.0667419289097686,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_2_2021,Visconde_do_Rio_Branco,2021-12-31T00:00:00,1.25,0.9630391597747804,-22.95686721801757,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:36:34
V42_cluster_3_2021,Aiuruoca,2020-12-31T00:00:00,1.8000000000000005,1.4249930381774902,-20.83372010125056,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Albertina,2020-12-31T00:00:00,1.68,1.6756739616394043,-0.2575022833687881,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Baependi,2020-12-31T00:00:00,1.67948717948718,1.579747200012207,-5.938716335151058,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Bandeira_do_Sul,2020-12-31T00:00:00,1.5,1.598397731781006,6.559848785400391,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Borda_da_Mata,2020-12-31T00:00:00,1.62,1.7815049886703491,9.969443745083272,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Brazopolis,2020-12-31T00:00:00,1.680327868852459,1.427384853363037,-15.053194092541206,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Bueno_Brandao,2020-12-31T00:00:00,1.8000000000000005,1.6491718292236328,-8.379342820909303,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Cachoeira_de_Minas,2020-12-31T00:00:00,1.8596638655462183,1.8974988460540767,2.0345064077881494,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Caldas,2020-12-31T00:00:00,1.68,1.5382994413375854,-8.434557063238959,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Cambuquira,2020-12-31T00:00:00,1.800145348837209,1.681802749633789,-6.574057993698259,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Campanha,2020-12-31T00:00:00,1.6833930704898452,1.6222490072250366,-3.632191692877557,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Careacu,2020-12-31T00:00:00,1.320454545454546,1.425251841545105,7.936456158321159,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Carmo_da_Cachoeira,2020-12-31T00:00:00,1.8000000000000005,1.6771012544631958,-6.827708085378024,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Carmo_de_Minas,2020-12-31T00:00:00,1.7400000000000002,1.664244294166565,-4.353776197323866,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Carrancas,2020-12-31T00:00:00,1.785714285714286,1.5339787006378174,-14.097192764282244,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Carvalhopolis,2020-12-31T00:00:00,1.679765395894428,1.516087293624878,-9.74410490117259,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Conceicao_das_Pedras,2020-12-31T00:00:00,1.739921976592978,1.5855683088302612,-8.871298244359437,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Conceicao_do_Rio_Verde,2020-12-31T00:00:00,1.8000000000000005,1.879122495651245,4.395694202846936,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Conceicao_dos_Ouros,2020-12-31T00:00:00,1.5028571428571431,1.4877896308898926,-1.00259110048245,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Congonhal,2020-12-31T00:00:00,1.619354838709677,1.7966442108154297,10.948148476650069,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Cordislandia,2020-12-31T00:00:00,1.62,1.6249388456344604,0.3048670144728606,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Cristina,2020-12-31T00:00:00,2.100263852242744,1.6002986431121826,-23.804876163377227,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Datas,2020-12-31T00:00:00,2.0,2.16455626487732,8.22781324386599,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Dom_Vicoso,2020-12-31T00:00:00,1.4285714285714288,1.5972769260406494,11.80938482284544,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Fama,2020-12-31T00:00:00,1.8000000000000005,1.5240254402160645,-15.33191998799643,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Fortaleza_de_Minas,2020-12-31T00:00:00,1.68,1.441249132156372,-14.211361181168323,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Heliodora,2020-12-31T00:00:00,1.8000000000000005,1.604972243309021,-10.834875371721068,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Ibitiura_de_Minas,2020-12-31T00:00:00,1.68,1.8138515949249268,7.967356840769455,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Inconfidentes,2020-12-31T00:00:00,2.46,2.0871450901031494,-15.156703654343517,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Ingai,2020-12-31T00:00:00,1.6791666666666667,1.6806817054748535,0.0902256362195624,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Itajuba,2020-12-31T00:00:00,1.5882352941176472,2.1021831035614014,32.35967689090304,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Itutinga,2020-12-31T00:00:00,1.8000000000000005,1.5174479484558103,-15.697336196899425,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Jesuania,2020-12-31T00:00:00,1.5,1.540769338607788,2.717955907185872,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Lambari,2020-12-31T00:00:00,1.8000970402717127,1.4504859447479248,-19.42179158691448,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Luminarias,2020-12-31T00:00:00,1.860103626943005,1.7298364639282229,-7.003220741463213,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Maria_da_Fe,2020-12-31T00:00:00,1.7333333333333332,1.6462867259979248,-5.021919653965868,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Monsenhor_Paulo,2020-12-31T00:00:00,2.1,1.597775101661682,-23.91547134944372,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Monte_Siao,2020-12-31T00:00:00,1.8000000000000005,1.6345008611679075,-9.194396601782918,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Natercia,2020-12-31T00:00:00,1.4399193548387097,1.7030060291290283,18.27093117446066,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Olimpio_Noronha,2020-12-31T00:00:00,1.5595854922279788,1.4754469394683838,-5.3949304593361616,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Ouro_Fino,2020-12-31T00:00:00,2.099920063948841,1.552387237548828,-26.073984233970915,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Paraisopolis,2020-12-31T00:00:00,2.4,1.5855361223220823,-33.93599490324656,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Pedralva,2020-12-31T00:00:00,1.68,1.7985968589782717,7.059336843944736,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Pirangucu,2020-12-31T00:00:00,1.333333333333333,1.330624222755432,-0.2031832933425681,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Piranguinho,2020-12-31T00:00:00,1.458181818181818,1.5200914144515991,4.245670567129626,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Poco_Fundo,2020-12-31T00:00:00,1.740033329060377,1.276055932044983,-26.66485688903116,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Pocos_de_Caldas,2020-12-31T00:00:00,1.8000000000000005,1.592937707901001,-11.503460672166623,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Pouso_Alegre,2020-12-31T00:00:00,1.6375,1.4988051652908323,-8.469913570025492,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Pouso_Alto,2020-12-31T00:00:00,1.45,1.6841790676116943,16.150280524944442,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Santa_Rita_de_Caldas,2020-12-31T00:00:00,1.8000000000000005,1.9356905221939087,7.538362344106023,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Santa_Rita_do_Sapucai,2020-12-31T00:00:00,1.68,1.6261229515075684,-3.206967172168546,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Sao_Bento_Abade,2020-12-31T00:00:00,1.8000000000000005,1.860500693321228,3.3611496289570977,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Sao_Goncalo_do_Sapucai,2020-12-31T00:00:00,2.4,1.7375082969665527,-27.603820959726967,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Sao_Joao_da_Mata,2020-12-31T00:00:00,1.6399999999999997,1.3453402519226074,-17.967057809597094,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Sao_Joao_del_Rei,2020-12-31T00:00:00,1.9709821428571432,1.7944990396499634,-8.954069109492249,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Sao_Lourenco,2020-12-31T00:00:00,3.3928571428571423,1.7381389141082764,-48.77064253154553,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Sao_Sebastiao_da_Bela_Vista,2020-12-31T00:00:00,1.5,1.4471848011016846,-3.521013259887696,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Sao_Tome_das_Letras,2020-12-31T00:00:00,2.133763094278808,1.411102056503296,-33.86791343955476,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Senador_Jose_Bento,2020-12-31T00:00:00,1.620618556701031,1.3367465734481812,-17.51627377578017,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Serrania,2020-12-31T00:00:00,1.8000000000000005,1.4577984809875488,-19.011195500691745,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Silvianopolis,2020-12-31T00:00:00,1.50032154340836,1.191169261932373,-20.605735006222027,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Soledade_de_Minas,2020-12-31T00:00:00,3.2000000000000006,1.828963279724121,-42.84489750862123,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Tocos_do_Moji,2020-12-31T00:00:00,1.441340782122905,1.490037202835083,3.378550119178245,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Tres_Coracoes,2020-12-31T00:00:00,2.1,1.808618307113648,-13.875318708873936,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Turvolandia,2020-12-31T00:00:00,1.6202531645569618,1.6472675800323486,1.6672959551215327,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Virginia,2020-12-31T00:00:00,1.5333333333333332,1.6575212478637695,8.099211817202368,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Aiuruoca,2021-12-31T00:00:00,1.2,1.5573577880859375,29.77981567382813,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Albertina,2021-12-31T00:00:00,1.56,1.597731590270996,2.418691684038208,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Baependi,2021-12-31T00:00:00,1.079646017699115,1.5710128545761108,45.51184636647586,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Bandeira_do_Sul,2021-12-31T00:00:00,1.8011695906432754,1.642909288406372,-8.786529767048853,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Borda_da_Mata,2021-12-31T00:00:00,1.2,1.6843249797821045,40.36041498184205,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Brazopolis,2021-12-31T00:00:00,1.200934579439252,1.3183588981628418,9.777744827567394,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Bueno_Brandao,2021-12-31T00:00:00,1.2,1.643482685089111,36.95689042409261,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Cachoeira_de_Minas,2021-12-31T00:00:00,1.5602040816326532,1.827492952346801,17.131660778277666,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Caldas,2021-12-31T00:00:00,1.2,1.5297636985778809,27.480308214823413,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Cambuquira,2021-12-31T00:00:00,1.2,1.6921528577804563,41.01273814837138,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Campanha,2021-12-31T00:00:00,1.325757575757576,1.610036849975586,21.44277954101561,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Careacu,2021-12-31T00:00:00,1.2,1.3915386199951172,15.961551666259773,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Carmo_da_Cachoeira,2021-12-31T00:00:00,1.439980158730159,1.6885799169540403,17.264109975175508,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Carmo_de_Minas,2021-12-31T00:00:00,1.3799999999999997,1.5712883472442627,13.861474437990076,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Carrancas,2021-12-31T00:00:00,1.214285714285714,1.5930135250091553,31.18934911840105,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Carvalhopolis,2021-12-31T00:00:00,1.14,1.5820995569229126,38.780662887974806,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Conceicao_das_Pedras,2021-12-31T00:00:00,1.5,1.549795150756836,3.319676717122396,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Conceicao_do_Rio_Verde,2021-12-31T00:00:00,1.500115340253749,1.8449604511260984,22.98790643751553,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Conceicao_dos_Ouros,2021-12-31T00:00:00,1.26,1.4841516017913818,17.789809665982684,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Congonhal,2021-12-31T00:00:00,1.6215384615384625,1.7239426374435425,6.315248039687107,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Cordislandia,2021-12-31T00:00:00,1.319791666666667,1.6860257387161257,27.749385096091565,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Cristina,2021-12-31T00:00:00,1.68,1.5955218076705933,-5.02846382913135,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Datas,2021-12-31T00:00:00,1.0,2.039680957794189,103.96809577941896,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Dom_Vicoso,2021-12-31T00:00:00,1.2,1.4738452434539795,22.82043695449829,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Fama,2021-12-31T00:00:00,1.5,1.5486149787902832,3.2409985860188804,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Fortaleza_de_Minas,2021-12-31T00:00:00,1.26,1.3629132509231567,8.16771832723466,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Heliodora,2021-12-31T00:00:00,1.2,1.6033852100372314,33.615434169769294,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Ibitiura_de_Minas,2021-12-31T00:00:00,1.9198795180722887,1.6514577865600586,-13.981175849083863,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Inconfidentes,2021-12-31T00:00:00,1.5,2.1699349880218506,44.66233253479004,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Ingai,2021-12-31T00:00:00,1.320075757575758,1.684947490692139,27.64021163349339,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Itajuba,2021-12-31T00:00:00,1.181818181818182,1.8023335933685305,52.505150208106365,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Itutinga,2021-12-31T00:00:00,1.2,1.570420503616333,30.86837530136109,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Jesuania,2021-12-31T00:00:00,1.2,1.4914909601211548,24.290913343429573,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Lambari,2021-12-31T00:00:00,1.199954863461973,1.5274033546447754,27.28840068517956,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Luminarias,2021-12-31T00:00:00,1.380229885057471,1.7930419445037842,29.90893501984448,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Maria_da_Fe,2021-12-31T00:00:00,1.444444444444444,1.6171307563781738,11.955206210796684,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Monsenhor_Paulo,2021-12-31T00:00:00,1.41,1.7058744430541992,20.984003053489296,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Monte_Siao,2021-12-31T00:00:00,1.5,1.6958078145980835,13.0538543065389,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Natercia,2021-12-31T00:00:00,1.4399193548387097,1.5570669174194336,8.135703029969063,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Olimpio_Noronha,2021-12-31T00:00:00,1.20026525198939,1.4517886638641355,20.9556522158628,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Ouro_Fino,2021-12-31T00:00:00,1.2,1.6951476335525513,41.26230279604594,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Paraisopolis,2021-12-31T00:00:00,1.2,1.643693208694458,36.97443405787151,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Pedralva,2021-12-31T00:00:00,1.5,1.626944661140442,8.462977409362793,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Pirangucu,2021-12-31T00:00:00,1.333333333333333,1.2807620763778689,-3.9428442716598298,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Piranguinho,2021-12-31T00:00:00,1.380549682875264,1.4497339725494385,5.011358195388143,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Poco_Fundo,2021-12-31T00:00:00,0.9601760412194076,1.4200711250305176,47.89695473207714,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Pocos_de_Caldas,2021-12-31T00:00:00,1.4199999999999997,1.6420180797576904,15.635076039274,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Pouso_Alegre,2021-12-31T00:00:00,1.2,1.4570820331573486,21.42350276311239,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Pouso_Alto,2021-12-31T00:00:00,1.5047619047619047,1.569046139717102,4.272053588794758,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Santa_Rita_de_Caldas,2021-12-31T00:00:00,1.682051282051282,1.86995542049408,11.171130181812655,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Santa_Rita_do_Sapucai,2021-12-31T00:00:00,1.5,1.6544995307922363,10.299968719482422,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Sao_Bento_Abade,2021-12-31T00:00:00,1.32,1.8934565782547,43.44368017081058,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Sao_Goncalo_do_Sapucai,2021-12-31T00:00:00,1.62,1.8470866680145264,14.017695556452235,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Sao_Joao_da_Mata,2021-12-31T00:00:00,0.8995327102803738,1.400824785232544,55.72805404663086,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Sao_Joao_del_Rei,2021-12-31T00:00:00,1.296875,1.7977006435394287,38.6178809476186,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Sao_Lourenco,2021-12-31T00:00:00,1.5087719298245608,2.165187358856201,43.5066040172134,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Sao_Sebastiao_da_Bela_Vista,2021-12-31T00:00:00,1.2,1.462995529174805,21.9162940979004,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Sao_Tome_das_Letras,2021-12-31T00:00:00,1.2,1.6172168254852295,34.7680687904358,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Senador_Jose_Bento,2021-12-31T00:00:00,1.32,1.396816611289978,5.819440249240754,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Serrania,2021-12-31T00:00:00,1.08,1.607797622680664,48.870150248209626,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Silvianopolis,2021-12-31T00:00:00,0.9601265822784812,1.250762104988098,30.270542246617985,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Soledade_de_Minas,2021-12-31T00:00:00,1.7204819277108432,2.199048042297364,27.815817584510626,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Tocos_do_Moji,2021-12-31T00:00:00,1.259776536312849,1.4391506910324097,14.2385692659873,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Tres_Coracoes,2021-12-31T00:00:00,1.5,1.924785017967224,28.31900119781494,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Turvolandia,2021-12-31T00:00:00,1.3797385620915028,1.7298641204833984,25.37622474370443,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_3_2021,Virginia,2021-12-31T00:00:00,1.433333333333333,1.582623839378357,10.415616700815631,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:09
V42_cluster_4_2021,Abadia_dos_Dourados,2020-12-31T00:00:00,2.584210526315789,1.903377771377564,-26.34587035402502,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Acucena,2020-12-31T00:00:00,1.166666666666667,0.9654235243797302,-17.249412196023144,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Agua_Boa,2020-12-31T00:00:00,1.33,1.205520749092102,-9.359342173526167,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Aguas_Vermelhas,2020-12-31T00:00:00,3.6000000000000005,3.001776695251465,-16.617314020792655,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Angelandia,2020-12-31T00:00:00,1.8400349650349648,1.1598589420318604,-36.96538576320912,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Aricanduva,2020-12-31T00:00:00,1.3196428571428571,1.014054775238037,-23.15687765449245,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Ataleia,2020-12-31T00:00:00,1.204819277108434,1.0206836462020874,-15.283257365226769,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Bandeira,2020-12-31T00:00:00,1.083333333333333,0.9383379817008972,-13.38418630453254,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Berilo,2020-12-31T00:00:00,0.8428571428571429,0.8437111377716064,0.1013214305295778,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Berizal,2020-12-31T00:00:00,3.340236686390532,1.5923008918762207,-52.32969871973759,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Bonfinopolis_de_Minas,2020-12-31T00:00:00,3.3,2.5765748023986816,-21.92197568488843,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Buritis,2020-12-31T00:00:00,3.3,2.7076525688171387,-17.9499221570564,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Buritizeiro,2020-12-31T00:00:00,3.0,2.53438401222229,-15.520532925923666,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Capelinha,2020-12-31T00:00:00,1.434123222748815,1.1626856327056885,-18.92707584239911,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Carai,2020-12-31T00:00:00,1.5225,1.4123716354370115,-7.233390119079688,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Caranaiba,2020-12-31T00:00:00,1.645161290322581,1.343852162361145,-18.314868562361788,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Catuji,2020-12-31T00:00:00,1.197183098591549,1.588831901550293,32.71419412949509,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Congonhas_do_Norte,2020-12-31T00:00:00,1.2,1.0699623823165894,-10.836468140284216,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Coroaci,2020-12-31T00:00:00,1.8000000000000005,1.699732542037964,-5.570414331224244,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Cuparaque,2020-12-31T00:00:00,1.018518518518519,1.2237976789474487,20.15468120574945,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Diamantina,2020-12-31T00:00:00,2.3453815261044184,1.7524399757385254,-25.28124076046358,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Formoso,2020-12-31T00:00:00,3.0,2.9652345180511475,-1.1588493982950845,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Franciscopolis,2020-12-31T00:00:00,1.8000000000000005,3.0630204677581787,70.16780376434323,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Frei_Gaspar,2020-12-31T00:00:00,1.08,1.5735442638397217,45.698542948122366,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Grao_Mogol,2020-12-31T00:00:00,0.7749999999999999,1.555653095245361,100.72943164456278,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Indaiabira,2020-12-31T00:00:00,3.0,3.0769057273864746,2.5635242462158203,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Itabirinha,2020-12-31T00:00:00,0.7992424242424243,1.1041759252548218,38.152817188281006,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Itacambira,2020-12-31T00:00:00,0.75,2.121755123138428,182.9006830851237,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Itaipe,2020-12-31T00:00:00,0.9594594594594597,1.1214596033096311,16.884522035088327,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Itamarandiba,2020-12-31T00:00:00,1.7196261682242993,1.6888028383255005,-1.7924436408540585,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Itambacuri,2020-12-31T00:00:00,1.091304347826087,1.1314889192581177,3.682251565484873,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Jequitinhonha,2020-12-31T00:00:00,2.4,2.8736062049865723,19.73359187444052,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Joao_Pinheiro,2020-12-31T00:00:00,3.0,2.4699063301086426,-17.66978899637858,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Jose_Goncalves_de_Minas,2020-12-31T00:00:00,1.5,1.296494722366333,-13.567018508911133,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Juiz_de_Fora,2020-12-31T00:00:00,1.222222222222222,1.4350788593292236,17.4155430360274,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Ladainha,2020-12-31T00:00:00,1.242268041237113,1.104164719581604,-11.117030871854254,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Malacacheta,2020-12-31T00:00:00,1.8000000000000005,1.624849557876587,-9.730580117967406,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Mantena,2020-12-31T00:00:00,0.7,0.7073520421981812,1.050291742597314,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Minas_Novas,2020-12-31T00:00:00,1.0,1.126880407333374,12.688040733337402,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Monte_Formoso,2020-12-31T00:00:00,0.6000000000000001,1.7486354112625122,191.43923521041864,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Ninheira,2020-12-31T00:00:00,3.0,2.7869181632995605,-7.102727890014648,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Nova_Belem,2020-12-31T00:00:00,1.1,1.0903490781784058,-0.8773565292358478,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Novo_Cruzeiro,2020-12-31T00:00:00,1.19727047146402,1.251518487930298,4.530974225059053,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Novorizonte,2020-12-31T00:00:00,1.235294117647059,1.4096384048461914,14.11358515421547,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Ouro_Verde_de_Minas,2020-12-31T00:00:00,1.2,1.5947051048278809,32.892092068990074,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Padre_Paraiso,2020-12-31T00:00:00,0.7219512195121951,0.6958801746368408,-3.611192026653802,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Paracatu,2020-12-31T00:00:00,2.7000000000000006,2.6047935485839844,-3.52616486725986,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Pedra_Azul,2020-12-31T00:00:00,1.0,1.3895010948181152,38.95010948181152,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Pirapora,2020-12-31T00:00:00,3.6000000000000005,2.968068599700928,-17.553650008307574,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Pote,2020-12-31T00:00:00,0.6571428571428573,0.8380107879638672,27.5233807771102,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Rio_Pardo_de_Minas,2020-12-31T00:00:00,3.4487179487179493,2.827268123626709,-18.01973470524786,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Santa_Barbara_do_Monte_Verde,2020-12-31T00:00:00,1.6000000000000003,1.2773044109344482,-20.168474316597003,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Santa_Maria_do_Suacui,2020-12-31T00:00:00,0.8,0.8129594326019287,1.619929075241083,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Santo_Antonio_do_Retiro,2020-12-31T00:00:00,0.975,1.913832426071167,96.29050523806843,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Sao_Goncalo_do_Rio_Preto,2020-12-31T00:00:00,3.6000000000000005,2.627906560897827,-27.002595530615924,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Sao_Joao_do_Manteninha,2020-12-31T00:00:00,0.9166666666666664,0.8548918962478638,-6.739065863869392,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Sao_Joao_do_Paraiso,2020-12-31T00:00:00,3.146938775510204,2.2828290462493896,-27.45873977547335,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Sao_Sebastiao_do_Maranhao,2020-12-31T00:00:00,0.8148148148148149,0.9655807614326476,18.503093448552207,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Senador_Modestino_Goncalves,2020-12-31T00:00:00,3.6000000000000005,1.6481834650039673,-54.21712597211202,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Setubinha,2020-12-31T00:00:00,1.0,1.1311503648757937,13.115036487579346,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Taiobeiras,2020-12-31T00:00:00,3.240083507306889,2.212154150009156,-31.72539704546486,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Teofilo_Otoni,2020-12-31T00:00:00,1.09375,1.00155508518219,-8.429249354771205,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Turmalina,2020-12-31T00:00:00,1.320338983050847,1.87815523147583,42.24795719778436,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Urucuia,2020-12-31T00:00:00,2.4,2.59712028503418,8.213345209757492,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Vargem_Grande_do_Rio_Pardo,2020-12-31T00:00:00,1.8000000000000005,2.645268678665161,46.95937103695337,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Varzea_da_Palma,2020-12-31T00:00:00,3.0,3.356541395187378,11.884713172912598,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Abadia_dos_Dourados,2021-12-31T00:00:00,1.8000000000000005,1.903247594833374,5.735977490742986,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Acucena,2021-12-31T00:00:00,0.9,1.055826187133789,17.314020792643227,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Agua_Boa,2021-12-31T00:00:00,0.6171428571428572,1.2749003171920776,106.58106991538293,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Aguas_Vermelhas,2021-12-31T00:00:00,3.0,3.008767604827881,0.2922534942626953,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Angelandia,2021-12-31T00:00:00,1.5133433283358322,1.2530419826507568,-17.200415848221244,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Aricanduva,2021-12-31T00:00:00,0.8394736842105264,1.103348731994629,31.433391272087448,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Ataleia,2021-12-31T00:00:00,0.6266666666666667,0.9486334323883056,51.3776753811126,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Bandeira,2021-12-31T00:00:00,0.96,0.9785923957824708,1.9367078940073807,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Berilo,2021-12-31T00:00:00,1.8017241379310345,0.7824680209159851,-56.57115290609843,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Berizal,2021-12-31T00:00:00,3.541935483870968,1.863263726234436,-47.39419352161429,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Bonfinopolis_de_Minas,2021-12-31T00:00:00,3.0,2.659596920013428,-11.346769332885742,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Buritis,2021-12-31T00:00:00,3.1195039458850062,2.570999383926392,-17.583069984000403,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Buritizeiro,2021-12-31T00:00:00,3.0,2.672714233398437,-10.909525553385418,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Capelinha,2021-12-31T00:00:00,1.4350318471337582,1.2055153846740725,-15.993823615699371,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Carai,2021-12-31T00:00:00,1.218,1.4719034433364868,20.84593130841436,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Caranaiba,2021-12-31T00:00:00,2.28,1.3089216947555542,-42.59115373879148,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Catuji,2021-12-31T00:00:00,1.384615384615385,1.331157684326172,-3.860833909776504,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Congonhas_do_Norte,2021-12-31T00:00:00,0.7333333333333333,1.109642744064331,51.31491964513606,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Coroaci,2021-12-31T00:00:00,1.5,1.7649383544921875,17.662556966145836,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Cuparaque,2021-12-31T00:00:00,1.32,1.2286765575408936,-6.918442610538371,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Diamantina,2021-12-31T00:00:00,2.9196787148594376,1.948096513748169,-33.277024494732586,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Formoso,2021-12-31T00:00:00,3.0,2.8915908336639404,-3.613638877868652,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Franciscopolis,2021-12-31T00:00:00,1.2,2.5276567935943604,110.63806613286336,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Frei_Gaspar,2021-12-31T00:00:00,1.2,1.1774449348449707,-1.8795887629191044,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Grao_Mogol,2021-12-31T00:00:00,1.155555555555555,1.14601731300354,-0.8254248362320646,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Indaiabira,2021-12-31T00:00:00,1.984444444444444,2.9202942848205566,47.15928646912104,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Itabirinha,2021-12-31T00:00:00,0.9659090909090908,0.938226878643036,-2.865923152250394,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Itacambira,2021-12-31T00:00:00,0.5,1.481585144996643,196.3170289993286,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Itaipe,2021-12-31T00:00:00,0.7792792792792793,0.9830455183982848,26.148037621051586,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Itamarandiba,2021-12-31T00:00:00,1.632,1.6345467567443848,0.1560512711020141,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Itambacuri,2021-12-31T00:00:00,0.8987341772151899,1.0371366739273071,15.399714422897553,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Jequitinhonha,2021-12-31T00:00:00,1.8000000000000005,2.468013286590576,37.111849255031984,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Joao_Pinheiro,2021-12-31T00:00:00,2.4,2.6737966537475586,11.40819390614828,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Jose_Goncalves_de_Minas,2021-12-31T00:00:00,1.502673796791444,1.2442078590393066,-17.200402263220532,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Juiz_de_Fora,2021-12-31T00:00:00,1.142857142857143,1.2803664207458496,12.032061815261825,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Ladainha,2021-12-31T00:00:00,0.7199999999999999,1.01655912399292,41.188767221238905,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Malacacheta,2021-12-31T00:00:00,1.8000000000000005,1.7237354516983032,-4.236919350094279,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Mantena,2021-12-31T00:00:00,0.7289999999999999,0.6930098533630371,-4.936919977635497,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Minas_Novas,2021-12-31T00:00:00,1.7848101265822778,0.9857954978942872,-44.7674862881924,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Monte_Formoso,2021-12-31T00:00:00,0.55,1.158553123474121,110.6460224498402,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Ninheira,2021-12-31T00:00:00,2.100246002460024,2.7752902507781982,32.14119905608646,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Nova_Belem,2021-12-31T00:00:00,1.0,1.0686147212982178,6.861472129821777,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Novo_Cruzeiro,2021-12-31T00:00:00,1.226392251815981,1.211480736732483,-1.2158846455053804,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Novorizonte,2021-12-31T00:00:00,1.307692307692308,1.2549049854278564,-4.0366775849286425,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Ouro_Verde_de_Minas,2021-12-31T00:00:00,1.942857142857143,1.3040424585342407,-32.880167575443494,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Padre_Paraiso,2021-12-31T00:00:00,0.4787878787878789,0.6748703122138977,40.95392596872544,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Paracatu,2021-12-31T00:00:00,2.4,2.513821601867676,4.742566744486495,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Pedra_Azul,2021-12-31T00:00:00,1.2,1.1263668537139893,-6.136095523834225,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Pirapora,2021-12-31T00:00:00,3.0,2.981665849685669,-0.6111383438110352,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Pote,2021-12-31T00:00:00,0.6000000000000001,0.7513405680656433,25.22342801094053,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Rio_Pardo_de_Minas,2021-12-31T00:00:00,2.764006791171477,3.179129838943481,15.018886679220556,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Santa_Barbara_do_Monte_Verde,2021-12-31T00:00:00,1.4,1.133450984954834,-19.039215360369,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Santa_Maria_do_Suacui,2021-12-31T00:00:00,0.8,0.8214172720909119,2.677159011363978,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Santo_Antonio_do_Retiro,2021-12-31T00:00:00,1.2,1.429484248161316,19.12368734677633,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Sao_Goncalo_do_Rio_Preto,2021-12-31T00:00:00,3.36,3.0093564987182617,-10.435818490527922,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Sao_Joao_do_Manteninha,2021-12-31T00:00:00,0.7,0.8931227326393127,27.588961805616112,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Sao_Joao_do_Paraiso,2021-12-31T00:00:00,3.0614754098360666,1.9921129941940308,-34.92964249218964,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Sao_Sebastiao_do_Maranhao,2021-12-31T00:00:00,0.9655172413793104,0.8907462358474731,-7.744139858654571,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Senador_Modestino_Goncalves,2021-12-31T00:00:00,2.7000000000000006,2.6953182220458984,-0.173399183485266,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Setubinha,2021-12-31T00:00:00,0.9329787234042556,0.963592529296875,3.281297324864578,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Taiobeiras,2021-12-31T00:00:00,3.49869451697128,2.400848388671875,-31.37873635363224,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Teofilo_Otoni,2021-12-31T00:00:00,0.6800000000000002,0.9142008423805236,34.44130035007698,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Turmalina,2021-12-31T00:00:00,1.2797783933518008,1.970614552497864,53.98092066054734,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Urucuia,2021-12-31T00:00:00,1.799276672694394,2.226669788360596,23.753607333005984,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Vargem_Grande_do_Rio_Pardo,2021-12-31T00:00:00,1.644444444444444,1.998855471611023,21.55202192229197,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_4_2021,Varzea_da_Palma,2021-12-31T00:00:00,1.68,3.2533977031707764,93.6546251887367,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2021_(2021)
    Modelo LSTM treinado com dados de 2012 a 2019, validado com os dados de 2020 e testado com os dados de 2021.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:37:43
V42_cluster_0_2022,Aguanil,2021-12-31T00:00:00,1.7382352941176469,1.7644933462142944,1.5106155182504517,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Alfenas,2021-12-31T00:00:00,1.6209774981853378,2.0852882862091064,28.643876213183606,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Alto_Caparao,2021-12-31T00:00:00,1.08,1.5164077281951904,40.40812298103614,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Alto_Jequitiba,2021-12-31T00:00:00,0.9,1.5327351093292236,70.3039010365804,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Amparo_do_Serra,2021-12-31T00:00:00,1.2,1.310219407081604,9.18495059013367,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Araponga,2021-12-31T00:00:00,1.2,1.3066482543945312,8.887354532877609,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Astolfo_Dutra,2021-12-31T00:00:00,1.75,1.3008246421813965,-25.6671633039202,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Bambui,2021-12-31T00:00:00,1.439910025706941,1.547258734703064,7.455237277302711,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Boa_Esperanca,2021-12-31T00:00:00,1.570339108544351,1.9757938385009768,25.819565197765964,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Bom_Jesus_do_Galho,2021-12-31T00:00:00,1.079802955665025,1.1375218629837036,5.345318515370321,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Bom_Sucesso,2021-12-31T00:00:00,1.37989417989418,1.643163561820984,19.07895444178142,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Caete,2021-12-31T00:00:00,1.222222222222222,1.2532577514648438,2.539270574396317,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Caiana,2021-12-31T00:00:00,0.9,1.22074556350708,35.63839594523112,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Cajuri,2021-12-31T00:00:00,1.2602564102564102,1.407173991203308,11.65775311684439,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Camacho,2021-12-31T00:00:00,1.5,1.6044104099273682,6.960693995157878,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Campo_do_Meio,2021-12-31T00:00:00,1.255813953488372,1.8847169876098635,50.079315680044665,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Campos_Altos,2021-12-31T00:00:00,1.2,1.791557788848877,49.29648240407308,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Campos_Gerais,2021-12-31T00:00:00,1.589070422535211,2.141493797302246,34.763932858664376,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Cana_Verde,2021-12-31T00:00:00,1.32,1.385642170906067,4.972891735308093,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Canaa,2021-12-31T00:00:00,1.2,1.3508211374282837,12.568428119023643,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Candeias,2021-12-31T00:00:00,1.199963309484498,1.715485453605652,42.96149224284377,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Caparao,2021-12-31T00:00:00,0.9,1.4729773998260498,63.66415553622775,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Capitolio,2021-12-31T00:00:00,1.2,1.6102826595306396,34.190221627553306,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Caputira,2021-12-31T00:00:00,1.38008658008658,1.2155754566192627,-11.920348030411011,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Carangola,2021-12-31T00:00:00,1.02,1.240483641624451,21.61604329651477,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Caratinga,2021-12-31T00:00:00,1.26,1.4062522649765017,11.607322617182655,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Carmo_da_Mata,2021-12-31T00:00:00,1.32,1.3637704849243164,3.3159458275997227,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Carmo_do_Rio_Claro,2021-12-31T00:00:00,1.643702081051479,2.1845738887786865,32.905708033247166,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Carmopolis_de_Minas,2021-12-31T00:00:00,1.98,1.679444432258606,-15.179574128353224,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Cassia,2021-12-31T00:00:00,1.314977578475336,1.781700611114502,35.492851002091825,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Catas_Altas_da_Noruega,2021-12-31T00:00:00,1.0,0.8812370896339417,-11.876291036605837,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Chale,2021-12-31T00:00:00,1.26016713091922,1.3355507850646973,5.982036215346233,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Claudio,2021-12-31T00:00:00,1.800653594771242,1.6522667407989502,-8.24072183584779,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Conceicao_da_Barra_de_Minas,2021-12-31T00:00:00,1.462068965517241,1.8899139165878296,29.262980143978943,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Conceicao_de_Ipanema,2021-12-31T00:00:00,1.29,1.351705551147461,4.783376057942705,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Coqueiral,2021-12-31T00:00:00,1.5,1.757580041885376,17.1720027923584,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Corrego_Danta,2021-12-31T00:00:00,1.08029197080292,1.504446268081665,39.26293157242437,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Cristais,2021-12-31T00:00:00,1.318776371308017,1.8500932455062864,40.28862555910731,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Desterro_de_Entre_Rios,2021-12-31T00:00:00,1.5,2.024669647216797,34.9779764811198,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Divinesia,2021-12-31T00:00:00,2.1,2.06531834602356,-1.651507332211453,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Divino,2021-12-31T00:00:00,1.08,1.2737821340560913,17.94279019037882,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Doresopolis,2021-12-31T00:00:00,1.2,1.5258944034576416,27.15786695480348,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Durande,2021-12-31T00:00:00,1.32,1.7245092391967771,30.644639333089184,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Eloi_Mendes,2021-12-31T00:00:00,1.037885462555066,1.716096043586731,65.34541676323768,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Entre_Folhas,2021-12-31T00:00:00,0.8400000000000001,1.039520263671875,23.75241234188987,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Entre_Rios_de_Minas,2021-12-31T00:00:00,1.8000000000000005,2.021225690841675,12.290316157870809,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ervalia,2021-12-31T00:00:00,1.08,1.451690673828125,34.41580313223378,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Espera_Feliz,2021-12-31T00:00:00,0.78,1.4387662410736084,84.45721039405235,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Faria_Lemos,2021-12-31T00:00:00,0.7199999999999999,1.2228879928588867,69.8455545637343,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ferros,2021-12-31T00:00:00,1.205128205128205,0.8972799777984619,-25.54485290608507,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Fervedouro,2021-12-31T00:00:00,1.15006090133983,1.1931631565093994,3.7478237125838274,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Guape,2021-12-31T00:00:00,1.376842105263158,1.8816349506378167,36.663088922471424,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ibituruna,2021-12-31T00:00:00,1.6000000000000003,1.6291792392730713,1.823702454566936,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ilicinea,2021-12-31T00:00:00,1.560035211267606,2.074531316757202,32.979775185429475,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Itapecerica,2021-12-31T00:00:00,1.222784810126582,1.5465152263641355,26.47484770472748,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ituiutaba,2021-12-31T00:00:00,2.25,1.3198025226593018,-41.34211010403103,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Itumirim,2021-12-31T00:00:00,1.2595155709342565,1.5696496963500977,24.6232863310929,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Lajinha,2021-12-31T00:00:00,1.260014255167498,1.4340062141418457,13.808729406098536,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Lavras,2021-12-31T00:00:00,1.259900454447089,1.6100950241088867,27.79541577477097,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Luisburgo,2021-12-31T00:00:00,1.3799999999999997,1.66903817653656,20.944795401200032,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Machado,2021-12-31T00:00:00,1.444286871961102,1.7677216529846191,22.39408162620397,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Manhuacu,2021-12-31T00:00:00,0.9,1.3235023021697998,47.05581135219997,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Manhumirim,2021-12-31T00:00:00,1.2,1.4958186149597168,24.65155124664308,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Martins_Soares,2021-12-31T00:00:00,1.2,1.655271291732788,37.93927431106568,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Matipo,2021-12-31T00:00:00,1.07995337995338,1.2946343421936035,19.878724973247543,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Medeiros,2021-12-31T00:00:00,1.500161864681127,1.655517339706421,10.355914163856983,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Miradouro,2021-12-31T00:00:00,1.2,1.3686680793762207,14.05567328135173,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Mirai,2021-12-31T00:00:00,1.019512195121951,1.2289191484451294,20.53991647428305,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Moeda,2021-12-31T00:00:00,1.8000000000000005,1.173581838607788,-34.801008966234,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Muriae,2021-12-31T00:00:00,0.9,1.2922594547271729,43.58438385857476,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Mutum,2021-12-31T00:00:00,1.319934249850568,1.3909310102462769,5.378810376634031,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Nazareno,2021-12-31T00:00:00,1.5,1.8550171852111816,23.66781234741211,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Nepomuceno,2021-12-31T00:00:00,1.379962721342032,1.685110330581665,22.11274294010442,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Oliveira,2021-12-31T00:00:00,1.537805840568272,1.7053159475326538,10.892799503380813,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Orizania,2021-12-31T00:00:00,1.08,1.4775547981262207,36.81062945613154,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Paraguacu,2021-12-31T00:00:00,0.9712280701754386,1.847731590270996,90.24693035665965,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Passa_Tempo,2021-12-31T00:00:00,1.511627906976744,1.5221755504608154,0.6977671843308804,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Perdoes,2021-12-31T00:00:00,1.44017094017094,1.652994155883789,14.777635749794277,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Pimenta,2021-12-31T00:00:00,1.3502325581395351,1.847154140472412,36.80266627680624,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Piranga,2021-12-31T00:00:00,2.1,1.8810338973999023,-10.42695726667132,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Piumhi,2021-12-31T00:00:00,1.5,1.5713253021240234,4.7550201416015625,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ponte_Nova,2021-12-31T00:00:00,1.2,1.3106625080108645,9.221875667572023,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Porto_Firme,2021-12-31T00:00:00,1.319607843137255,1.2588434219360352,-4.604733255961689,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Raul_Soares,2021-12-31T00:00:00,1.320079522862823,1.1455310583114624,-13.222571938152772,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Reduto,2021-12-31T00:00:00,1.2,1.455418348312378,21.28486235936483,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ribeirao_Vermelho,2021-12-31T00:00:00,1.258899676375405,1.591919183731079,26.453220507173075,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ritapolis,2021-12-31T00:00:00,1.5,1.755725383758545,17.048358917236328,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Rosario_da_Limeira,2021-12-31T00:00:00,0.9,1.16484534740448,29.42726082272,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Santa_Barbara_do_Leste,2021-12-31T00:00:00,1.319917440660475,1.1773401498794556,-10.801985517342285,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Santa_Margarida,2021-12-31T00:00:00,0.9,1.4337643384933472,59.30714872148302,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Santa_Rita_de_Minas,2021-12-31T00:00:00,1.2,1.271759271621704,5.979939301808679,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Santa_Rita_do_Itueto,2021-12-31T00:00:00,1.3799999999999997,1.8253304958343504,32.270325785097896,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Santana_da_Vargem,2021-12-31T00:00:00,1.44,1.9122319221496584,32.79388348261516,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Santana_do_Jacare,2021-12-31T00:00:00,1.2,1.5462621450424194,28.85517875353496,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Santana_do_Manhuacu,2021-12-31T00:00:00,1.32,1.4884042739868164,12.757899544455784,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Domingos_das_Dores,2021-12-31T00:00:00,1.2,1.6597084999084473,38.30904165903728,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Francisco_de_Paula,2021-12-31T00:00:00,1.3200867052023115,1.538627028465271,16.554997668086244,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Francisco_do_Gloria,2021-12-31T00:00:00,0.78,1.199281930923462,53.75409370813614,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Joao_do_Manhuacu,2021-12-31T00:00:00,0.9,1.488815188407898,65.42390982309976,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Jose_do_Mantimento,2021-12-31T00:00:00,1.500917431192661,1.475699543952942,-1.680164858881038,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Roque_de_Minas,2021-12-31T00:00:00,1.2,1.4515269994735718,20.96058328946432,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Tiago,2021-12-31T00:00:00,1.5,2.022552967071533,34.83686447143555,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Tomas_de_Aquino,2021-12-31T00:00:00,1.2,1.7597055435180664,46.64212862650554,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Senador_Firmino,2021-12-31T00:00:00,1.563636363636364,1.5451160669326782,-1.1844375798868767,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Senhora_de_Oliveira,2021-12-31T00:00:00,1.62037037037037,1.558037042617798,-3.8468567984444544,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sericita,2021-12-31T00:00:00,0.9,1.4235007762908936,58.1667529212104,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Simonesia,2021-12-31T00:00:00,1.08,1.3520605564117432,25.19079226034658,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Tapirai,2021-12-31T00:00:00,1.396563119629874,1.658864974975586,18.78195490478288,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Tombos,2021-12-31T00:00:00,0.8497959183673469,1.1827985048294067,39.186183325266406,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Tres_Pontas,2021-12-31T00:00:00,1.3500296384113808,1.896700739860535,40.4932666583852,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ubaporanga,2021-12-31T00:00:00,1.319867549668874,1.4554158449172974,10.269840733824342,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Vargem_Bonita,2021-12-31T00:00:00,1.519565217391304,1.731189250946045,13.92661737270113,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Vermelho_Novo,2021-12-31T00:00:00,0.96,1.1904289722442627,24.00301794211072,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Vicosa,2021-12-31T00:00:00,0.9006711409395973,1.3562828302383425,50.58579858831072,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Vieiras,2021-12-31T00:00:00,0.78,1.2396727800369265,58.93240769704183,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Aguanil,2022-12-31T00:00:00,1.56,1.9182370901107788,22.963916032742222,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Alfenas,2022-12-31T00:00:00,0.9243951612903224,1.9798619747161863,114.17917981662563,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Alto_Caparao,2022-12-31T00:00:00,1.5,1.4833234548568726,-1.111769676208496,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Alto_Jequitiba,2022-12-31T00:00:00,0.9,1.4030423164367676,55.8935907151964,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Amparo_do_Serra,2022-12-31T00:00:00,1.8000000000000005,1.3089230060577393,-27.28205521901449,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Araponga,2022-12-31T00:00:00,1.260041407867495,1.3166356086730957,4.49145563409548,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Astolfo_Dutra,2022-12-31T00:00:00,0.625,1.5526392459869385,148.42227935791016,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Bambui,2022-12-31T00:00:00,1.5,1.5592155456542969,3.947703043619792,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Boa_Esperanca,2022-12-31T00:00:00,0.9726256983240223,1.9682799577713013,102.3676694089965,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Bom_Jesus_do_Galho,2022-12-31T00:00:00,0.7801587301587303,1.2380433082580566,58.691207365732566,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Bom_Sucesso,2022-12-31T00:00:00,1.440136054421769,1.691826581954956,17.476857603863248,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Caete,2022-12-31T00:00:00,1.5333333333333332,1.3034918308258057,-14.989663207012669,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Caiana,2022-12-31T00:00:00,0.9,1.2072014808654783,34.133497873942055,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Cajuri,2022-12-31T00:00:00,1.050602409638554,1.3841744661331177,31.750551248909165,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Camacho,2022-12-31T00:00:00,1.380176211453745,1.6990599632263184,23.104568034591164,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Campo_do_Meio,2022-12-31T00:00:00,1.580968858131488,1.822340965270996,15.267353679868204,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Campos_Altos,2022-12-31T00:00:00,1.680042238648363,1.7266924381256104,2.776727775295596,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Campos_Gerais,2022-12-31T00:00:00,1.535771358328211,2.0617661476135254,34.24955065302784,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Cana_Verde,2022-12-31T00:00:00,1.02,1.4076672792434692,38.00659600426169,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Canaa,2022-12-31T00:00:00,1.320338983050847,1.401221752166748,6.125909342539366,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Candeias,2022-12-31T00:00:00,1.020017406440383,1.682440996170044,64.94238093851367,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Caparao,2022-12-31T00:00:00,1.32,1.497487187385559,13.445999044360532,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Capitolio,2022-12-31T00:00:00,1.08,1.5338151454925537,42.01992087894015,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Caputira,2022-12-31T00:00:00,1.440044247787611,1.2901850938796997,-10.406565918939297,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Carangola,2022-12-31T00:00:00,1.02,1.2646812200546265,23.98835490731632,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Caratinga,2022-12-31T00:00:00,1.620022123893805,1.5316483974456787,-5.455093800553516,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Carmo_da_Mata,2022-12-31T00:00:00,1.56,1.3586111068725586,-12.909544431246248,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Carmo_do_Rio_Claro,2022-12-31T00:00:00,1.619555143651529,2.108447313308716,30.186818372538177,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Carmopolis_de_Minas,2022-12-31T00:00:00,1.8000000000000005,1.8048893213272093,0.2716289626227336,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Cassia,2022-12-31T00:00:00,1.413626373626374,1.6869935989379885,19.338011118903047,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Catas_Altas_da_Noruega,2022-12-31T00:00:00,1.0,0.8925483226776123,-10.74516773223877,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Chale,2022-12-31T00:00:00,1.44,1.3119547367095947,-8.892032172944807,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Claudio,2022-12-31T00:00:00,1.140522875816993,1.712950348854065,50.1899159740241,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Conceicao_da_Barra_de_Minas,2022-12-31T00:00:00,1.370833333333333,1.9366495609283447,41.27534790966653,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Conceicao_de_Ipanema,2022-12-31T00:00:00,1.5,1.394405484199524,-7.039634386698405,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Coqueiral,2022-12-31T00:00:00,1.2,1.7396361827850342,44.96968189875285,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Corrego_Danta,2022-12-31T00:00:00,1.5,1.4987154006958008,-0.0856399536132812,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Cristais,2022-12-31T00:00:00,1.059063136456212,1.8697067499160769,76.54346427092182,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Desterro_de_Entre_Rios,2022-12-31T00:00:00,1.8000000000000005,2.011317729949951,11.739873886108382,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Divinesia,2022-12-31T00:00:00,2.2230769230769227,2.270321846008301,2.125204145702128,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Divino,2022-12-31T00:00:00,1.08,1.288957953453064,19.34795865306147,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Doresopolis,2022-12-31T00:00:00,1.32,1.5009959936141968,13.711817698045206,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Durande,2022-12-31T00:00:00,1.5,1.6648536920547483,10.990246136983234,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Eloi_Mendes,2022-12-31T00:00:00,1.090201870999508,1.6227359771728516,48.84729336275225,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Entre_Folhas,2022-12-31T00:00:00,1.2,1.019067883491516,-15.07767637570699,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Entre_Rios_de_Minas,2022-12-31T00:00:00,1.8000000000000005,2.110359668731689,17.242203818427175,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ervalia,2022-12-31T00:00:00,1.2,1.4277949333190918,18.982911109924324,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Espera_Feliz,2022-12-31T00:00:00,1.2,1.334134817123413,11.177901426951095,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Faria_Lemos,2022-12-31T00:00:00,1.200305810397553,1.0767991542816162,-10.289599120996524,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ferros,2022-12-31T00:00:00,1.230769230769231,0.9290886521339417,-24.51154701411726,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Fervedouro,2022-12-31T00:00:00,1.4399108138238568,1.228928565979004,-14.652452486592836,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Guape,2022-12-31T00:00:00,1.262758620689655,1.7899675369262695,41.75056955451071,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ibituruna,2022-12-31T00:00:00,1.2,1.6877095699310305,40.6424641609192,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ilicinea,2022-12-31T00:00:00,1.080069324090121,2.057436943054199,90.49119321923514,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Itapecerica,2022-12-31T00:00:00,1.040506329113924,1.4912934303283691,43.32382116294545,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ituiutaba,2022-12-31T00:00:00,1.75,1.838556766510009,5.060386657714844,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Itumirim,2022-12-31T00:00:00,1.249307479224377,1.611329197883606,28.97779167094936,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Lajinha,2022-12-31T00:00:00,1.419975786924939,1.402073860168457,-1.2607205644858122,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Lavras,2022-12-31T00:00:00,1.260078277886497,1.7320125102996826,37.45277104568068,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Luisburgo,2022-12-31T00:00:00,1.5,1.632490634918213,8.832708994547527,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Machado,2022-12-31T00:00:00,0.964047619047619,1.739811658859253,80.46947313432607,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Manhuacu,2022-12-31T00:00:00,1.44,1.2429683208465576,-13.68275549676683,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Manhumirim,2022-12-31T00:00:00,1.3799999999999997,1.3948451280593872,1.0757339173469231,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Martins_Soares,2022-12-31T00:00:00,1.3799999999999997,1.537339448928833,11.401409342669083,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Matipo,2022-12-31T00:00:00,1.26,1.2433141469955444,-1.324274047972665,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Medeiros,2022-12-31T00:00:00,1.5,1.6825008392333984,12.166722615559896,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Miradouro,2022-12-31T00:00:00,0.96,1.3273661136627195,38.26730350653333,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Mirai,2022-12-31T00:00:00,1.2,1.1968728303909302,-0.2605974674224817,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Moeda,2022-12-31T00:00:00,1.25,1.2875534296035769,3.004274368286133,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Muriae,2022-12-31T00:00:00,0.9897142857142858,1.252873182296753,26.58938042836706,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Mutum,2022-12-31T00:00:00,1.379940564635958,1.4661340713500977,6.246175300809309,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Nazareno,2022-12-31T00:00:00,1.319811320754717,1.7760329246520996,34.56718371202469,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Nepomuceno,2022-12-31T00:00:00,0.8699884125144843,1.6695401668548584,91.90372455990182,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Oliveira,2022-12-31T00:00:00,1.535560504825538,1.7324196100234983,12.82001618166954,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Orizania,2022-12-31T00:00:00,1.5,1.4641845226287842,-2.387698491414388,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Paraguacu,2022-12-31T00:00:00,1.043954802259887,1.6942949295043943,62.295812599998825,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Passa_Tempo,2022-12-31T00:00:00,1.507692307692308,1.5655083656311035,3.834738536756837,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Perdoes,2022-12-31T00:00:00,1.259859154929577,1.741043210029602,38.193480058246834,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Pimenta,2022-12-31T00:00:00,1.3502325581395351,1.7203516960144043,27.41151038342985,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Piranga,2022-12-31T00:00:00,1.8000000000000005,2.069758415222168,14.986578623453758,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Piumhi,2022-12-31T00:00:00,1.2,1.558627367019653,29.88561391830445,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ponte_Nova,2022-12-31T00:00:00,1.25,1.2556726932525637,0.4538154602050782,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Porto_Firme,2022-12-31T00:00:00,1.32,1.2687585353851318,-3.881929137490017,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Raul_Soares,2022-12-31T00:00:00,0.8399548532731377,1.1809406280517578,40.59572647861562,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Reduto,2022-12-31T00:00:00,1.260059171597633,1.4303135871887207,13.51162067851318,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ribeirao_Vermelho,2022-12-31T00:00:00,1.32,1.6153364181518557,22.37397107211026,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ritapolis,2022-12-31T00:00:00,1.8000000000000005,1.7823728322982788,-0.9792870945400808,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Rosario_da_Limeira,2022-12-31T00:00:00,1.02,1.1244077682495115,10.23605571073644,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Santa_Barbara_do_Leste,2022-12-31T00:00:00,1.05015873015873,1.3119845390319824,24.93202230806364,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Santa_Margarida,2022-12-31T00:00:00,1.3799999999999997,1.2968871593475342,-6.0226696124975,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Santa_Rita_de_Minas,2022-12-31T00:00:00,1.019909502262443,1.3324867486953735,30.64754723233261,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Santa_Rita_do_Itueto,2022-12-31T00:00:00,1.6399999999999997,1.7203662395477295,4.900380460227429,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Santana_da_Vargem,2022-12-31T00:00:00,0.9900921658986176,1.8541266918182373,87.26809035352919,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Santana_do_Jacare,2022-12-31T00:00:00,1.5,1.4720278978347778,-1.8648068110148115,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Santana_do_Manhuacu,2022-12-31T00:00:00,1.26,1.481837272644043,17.60613274952722,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Domingos_das_Dores,2022-12-31T00:00:00,1.2,1.6867892742156982,40.56577285130819,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Francisco_de_Paula,2022-12-31T00:00:00,1.62,1.5885918140411377,-1.9387769110408888,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Francisco_do_Gloria,2022-12-31T00:00:00,0.9601476014760146,1.1110209226608276,15.713554973514348,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Joao_do_Manhuacu,2022-12-31T00:00:00,1.2,1.3816591501235962,15.138262510299688,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Jose_do_Mantimento,2022-12-31T00:00:00,1.499236641221374,1.480091214179993,-1.277011681477065,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Roque_de_Minas,2022-12-31T00:00:00,1.32,1.4498387575149536,9.836269508708602,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Tiago,2022-12-31T00:00:00,1.3210526315789468,2.017577171325684,52.72496516011157,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sao_Tomas_de_Aquino,2022-12-31T00:00:00,1.2,1.6636183261871338,38.63486051559449,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Senador_Firmino,2022-12-31T00:00:00,1.565217391304348,1.6152689456939695,3.197738197114728,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Senhora_de_Oliveira,2022-12-31T00:00:00,1.5,1.6928467750549316,12.856451670328775,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Sericita,2022-12-31T00:00:00,1.3799999999999997,1.3322219848632812,-3.4621750099071327,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Simonesia,2022-12-31T00:00:00,1.8000000000000005,1.2999517917633057,-27.780456013149696,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Tapirai,2022-12-31T00:00:00,1.324520819563781,1.6630889177322388,25.56155351940501,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Tombos,2022-12-31T00:00:00,1.2,1.0806233882904053,-9.948050975799555,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Tres_Pontas,2022-12-31T00:00:00,0.975023651844844,1.868042230606079,91.58942729969196,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Ubaporanga,2022-12-31T00:00:00,1.2,1.7491161823272705,45.75968186060588,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Vargem_Bonita,2022-12-31T00:00:00,1.235217391304348,1.7265936136245728,39.78054598157397,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Vermelho_Novo,2022-12-31T00:00:00,0.9598214285714286,1.157084345817566,20.552043471225467,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Vicosa,2022-12-31T00:00:00,1.079768786127168,1.2295438051223757,13.871026919791683,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_0_2022,Vieiras,2022-12-31T00:00:00,1.32,1.2060387134552002,-8.633430798848474,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:17
V42_cluster_1_2022,Almenara,2021-12-31T00:00:00,0.78,0.8918040990829468,14.333858856788046,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Alpinopolis,2021-12-31T00:00:00,1.67993145468393,2.967581272125244,76.64894980394175,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Alterosa,2021-12-31T00:00:00,1.08,2.2398130893707275,107.39010086765994,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Andradas,2021-12-31T00:00:00,1.26,2.912323474884033,131.136783720955,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Araguari,2021-12-31T00:00:00,1.8600340136054423,2.07445764541626,11.52794143775813,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Arapua,2021-12-31T00:00:00,1.32,1.437633991241455,8.911666003140532,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Araxa,2021-12-31T00:00:00,1.250440917107584,1.9611577987670896,56.83730210168402,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Arceburgo,2021-12-31T00:00:00,1.354903268845897,2.280986785888672,68.35052644249727,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Areado,2021-12-31T00:00:00,1.5228426395939092,2.2317018508911133,46.54842154184971,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Bom_Jesus_da_Penha,2021-12-31T00:00:00,1.519900497512438,2.373544931411743,56.16449466898865,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Botelhos,2021-12-31T00:00:00,1.26,2.0180413722991943,60.16201367453922,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Botumirim,2021-12-31T00:00:00,0.7241379310344828,0.6978681087493896,-3.627737363179525,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Cabo_Verde,2021-12-31T00:00:00,1.32,2.481003761291504,87.9548304008715,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Campestre,2021-12-31T00:00:00,1.2899602385685882,2.496802806854248,93.55657114089342,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Capetinga,2021-12-31T00:00:00,1.6399999999999997,3.090881824493408,88.46840393252494,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Carmo_do_Paranaiba,2021-12-31T00:00:00,1.541078066914498,1.9177606105804443,24.442794414700185,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Cascalho_Rico,2021-12-31T00:00:00,1.9411764705882348,2.0674493312835693,6.50496555097178,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Claraval,2021-12-31T00:00:00,1.050113378684807,3.9477035999298096,275.9311784860822,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Conceicao_da_Aparecida,2021-12-31T00:00:00,1.62,2.5744900703430176,58.9191401446307,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Coromandel,2021-12-31T00:00:00,1.249939246658566,1.8010647296905518,44.092181640451486,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Corrego_Fundo,2021-12-31T00:00:00,1.1984126984126982,2.219711780548096,85.2209830126226,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Cruzeiro_da_Fortaleza,2021-12-31T00:00:00,1.800184162062615,1.8963419198989868,5.341551151422004,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Delfinopolis,2021-12-31T00:00:00,0.9,2.048854351043701,127.65048344930014,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Divisa_Nova,2021-12-31T00:00:00,1.2601319509896318,2.344926357269287,86.08577898748801,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Dom_Cavati,2021-12-31T00:00:00,0.6000000000000001,0.8666881918907166,44.44803198178607,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Dores_do_Turvo,2021-12-31T00:00:00,1.5,1.9778623580932613,31.85749053955078,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Estrela_do_Indaia,2021-12-31T00:00:00,1.8000000000000005,1.8989378213882449,5.496545632680242,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Estrela_do_Sul,2021-12-31T00:00:00,1.8000000000000005,1.8118836879730225,0.6602048873901218,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Guaranesia,2021-12-31T00:00:00,1.031014249790444,2.58583664894104,150.80513188509445,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Guarani,2021-12-31T00:00:00,0.953846153846154,2.503070831298828,162.4187161845545,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Guarda-Mor,2021-12-31T00:00:00,1.7994350282485878,1.7684924602508545,-1.719571282762573,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Guaxupe,2021-12-31T00:00:00,1.02,1.941757678985596,90.3683999005486,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Guimarania,2021-12-31T00:00:00,1.541380188439012,2.29873275756836,49.13470244458859,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Ibia,2021-12-31T00:00:00,1.2,1.8472706079483032,53.93921732902528,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Ibiraci,2021-12-31T00:00:00,1.327034071867436,3.966864109039306,198.9270730220992,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Indianopolis,2021-12-31T00:00:00,1.919956379498364,2.0578861236572266,7.1840040550765165,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Itamogi,2021-12-31T00:00:00,1.260064724919094,2.7807164192199707,120.68044317314848,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Itanhomi,2021-12-31T00:00:00,0.9606060606060604,1.1195528507232666,16.546511274030927,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Jacui,2021-12-31T00:00:00,0.96,1.8554729223251345,93.27842940886818,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Jacutinga,2021-12-31T00:00:00,1.32014652014652,1.9802545309066768,50.00263233560571,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Juruaia,2021-12-31T00:00:00,1.5,1.948038578033448,29.869238535563152,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Lagoa_Dourada,2021-12-31T00:00:00,1.2,2.997030973434448,149.75258111953735,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Lagoa_Formosa,2021-12-31T00:00:00,1.4830188679245278,1.9951406717300413,34.53238626169496,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Lagoa_Grande,2021-12-31T00:00:00,0.6000000000000001,2.056640148162842,242.77335802714023,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Matutina,2021-12-31T00:00:00,1.502024291497976,1.6951541900634766,12.857974378889118,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Monte_Belo,2021-12-31T00:00:00,1.56,2.3592543601989746,51.234253858908616,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Monte_Carmelo,2021-12-31T00:00:00,1.8000000000000005,1.7446833848953247,-3.073145283593086,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Monte_Santo_de_Minas,2021-12-31T00:00:00,1.200018932222643,2.5229134559631348,110.23947107986552,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Muzambinho,2021-12-31T00:00:00,1.199937762564182,2.361576557159424,96.80825379750546,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Nova_Resende,2021-12-31T00:00:00,1.3800031392246113,2.718592882156372,96.99903608073532,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Passos,2021-12-31T00:00:00,0.99,2.2169222831726074,123.93154375480884,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Patis,2021-12-31T00:00:00,2.111111111111112,2.3775906562805176,12.622715297498177,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Patos_de_Minas,2021-12-31T00:00:00,1.7224109589041097,1.641643762588501,-4.689194288858747,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Patrocinio,2021-12-31T00:00:00,1.4369951534733445,1.877537965774536,30.65722325063944,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Pedrinopolis,2021-12-31T00:00:00,1.514285714285714,2.57122540473938,69.79790408656285,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Perdizes,2021-12-31T00:00:00,1.261308677098151,2.056852340698242,63.0728764532383,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Piracema,2021-12-31T00:00:00,1.2,1.525378704071045,27.114892005920417,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Ponto_dos_Volantes,2021-12-31T00:00:00,0.5375,0.5548893809318542,3.2352336617403297,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Pratapolis,2021-12-31T00:00:00,0.9602272727272728,2.865436553955078,198.41232751248148,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Pratinha,2021-12-31T00:00:00,1.35,2.081352472305298,54.17425720779983,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Presidente_Kubitschek,2021-12-31T00:00:00,0.7368421052631579,1.3545331954956057,83.82950510297503,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Presidente_Olegario,2021-12-31T00:00:00,1.637164750957854,1.8975924253463743,15.907236839551576,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Quartel_Geral,2021-12-31T00:00:00,2.4571428571428573,3.3921890258789062,38.05420454158339,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Rio_Paranaiba,2021-12-31T00:00:00,1.080031384856807,1.68959641456604,56.439566317792746,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Rio_Vermelho,2021-12-31T00:00:00,0.7777777777777777,1.0109472274780271,29.978929247174964,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Romaria,2021-12-31T00:00:00,2.052631578947369,1.9436376094818115,-5.309962614988698,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Sacramento,2021-12-31T00:00:00,1.25645342312009,1.913874149322509,52.32352541727165,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Santa_Rosa_da_Serra,2021-12-31T00:00:00,1.5,1.9978944063186648,33.19296042124431,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Santo_Antonio_do_Amparo,2021-12-31T00:00:00,1.3799999999999997,2.4195942878723145,75.33291941103732,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Sao_Goncalo_do_Abaete,2021-12-31T00:00:00,1.67887323943662,2.176496982574463,29.64034040502252,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Sao_Gotardo,2021-12-31T00:00:00,1.26027397260274,1.7261980772018433,36.97006482145056,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Sao_Joao_Batista_do_Gloria,2021-12-31T00:00:00,1.110749185667752,1.8389509916305544,65.55951742832268,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Sao_Pedro_da_Uniao,2021-12-31T00:00:00,1.479945054945055,2.1798207759857178,47.29065573766498,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Sao_Sebastiao_do_Paraiso,2021-12-31T00:00:00,1.14450771643146,2.167521953582764,89.3846517995554,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Serra_do_Salitre,2021-12-31T00:00:00,1.4676384839650147,2.018491744995117,37.533307217585445,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Tapira,2021-12-31T00:00:00,0.6000000000000001,2.271446704864502,278.57445081075025,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Tiros,2021-12-31T00:00:00,1.2,1.869691014289856,55.80758452415468,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Tupaciguara,2021-12-31T00:00:00,1.887700534759358,2.1288511753082275,12.774835632475522,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Uberaba,2021-12-31T00:00:00,2.0302325581395344,2.830507040023804,39.41787253267307,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Uberlandia,2021-12-31T00:00:00,1.77962962962963,1.7159069776535034,-3.5806693097927464,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Unai,2021-12-31T00:00:00,2.639892904953146,2.5272974967956543,-4.265150603126076,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Varginha,2021-12-31T00:00:00,1.2,2.366162776947021,97.1802314122518,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Varjao_de_Minas,2021-12-31T00:00:00,2.390254805543138,2.4011106491088867,0.4541709754363143,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Virginopolis,2021-12-31T00:00:00,1.202380952380952,1.301791429519653,8.267802059060301,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Almenara,2022-12-31T00:00:00,0.78,0.9858941435813904,26.396685074537224,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Alpinopolis,2022-12-31T00:00:00,1.4420494699646638,2.89928936958313,101.05339171576232,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Alterosa,2022-12-31T00:00:00,1.080104712041885,2.136581897735596,97.81247817135178,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Andradas,2022-12-31T00:00:00,1.26,2.5875120162963867,105.35809653145928,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Araguari,2022-12-31T00:00:00,1.919968366943456,2.8188493251800537,46.81748791869914,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Arapua,2022-12-31T00:00:00,1.14,1.6011139154434204,40.44858907398426,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Araxa,2022-12-31T00:00:00,1.077872465471643,1.9419825077056885,80.16811542318582,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Arceburgo,2022-12-31T00:00:00,0.9244897959183672,2.131433725357056,130.55243386864404,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Areado,2022-12-31T00:00:00,0.9076923076923076,2.27984881401062,151.1697845943904,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Bom_Jesus_da_Penha,2022-12-31T00:00:00,1.6298200514138823,2.118025779724121,29.954578598214955,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Botelhos,2022-12-31T00:00:00,0.9,1.882296323776245,109.14403597513834,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Botumirim,2022-12-31T00:00:00,0.7241379310344828,0.8271046280860901,14.219210545221964,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Cabo_Verde,2022-12-31T00:00:00,1.35,2.307915210723877,70.95668227584272,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Campestre,2022-12-31T00:00:00,1.020018115942029,2.2805233001708984,123.57674481739384,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Capetinga,2022-12-31T00:00:00,1.407056229327453,2.313449144363404,64.41767543783162,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Carmo_do_Paranaiba,2022-12-31T00:00:00,1.392969472710453,2.063708782196045,48.15175943378438,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Cascalho_Rico,2022-12-31T00:00:00,2.580434782608696,2.595509052276612,0.5841755726403595,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Claraval,2022-12-31T00:00:00,1.289915966386555,3.0361506938934326,135.37585183929536,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Conceicao_da_Aparecida,2022-12-31T00:00:00,1.53,2.102303504943848,37.40545783946717,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Coromandel,2022-12-31T00:00:00,1.260025542784164,2.0380406379699707,61.74597805903977,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Corrego_Fundo,2022-12-31T00:00:00,1.079051383399209,2.0493979454040527,89.92589017847091,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Cruzeiro_da_Fortaleza,2022-12-31T00:00:00,1.5598526703499078,2.221241235733032,42.40070732031128,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Delfinopolis,2022-12-31T00:00:00,1.3799999999999997,1.777904033660889,28.833625627600657,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Divisa_Nova,2022-12-31T00:00:00,1.260204081632653,2.3742668628692627,88.40336239772289,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Dom_Cavati,2022-12-31T00:00:00,0.6000000000000001,1.2550851106643677,109.18085177739458,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Dores_do_Turvo,2022-12-31T00:00:00,1.2,2.224284172058105,85.35701433817546,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Estrela_do_Indaia,2022-12-31T00:00:00,1.86,2.2597367763519287,21.491224535049938,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Estrela_do_Sul,2022-12-31T00:00:00,1.460045146726862,2.517096519470215,72.39854021727047,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Guaranesia,2022-12-31T00:00:00,1.086968085106383,1.8201323747634888,67.45039709103787,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Guarani,2022-12-31T00:00:00,1.2,2.9050514698028564,142.08762248357138,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Guarda-Mor,2022-12-31T00:00:00,1.858757062146893,2.0461578369140625,10.082047761030092,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Guaxupe,2022-12-31T00:00:00,1.14,1.7707741260528564,55.331063688847074,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Guimarania,2022-12-31T00:00:00,1.2220588235294123,1.9636298418045044,60.68210498520607,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Ibia,2022-12-31T00:00:00,1.14,1.8349549770355225,60.96096289785287,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Ibiraci,2022-12-31T00:00:00,1.377007874015748,2.2558140754699707,63.81998375153606,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Indianopolis,2022-12-31T00:00:00,1.8000000000000005,2.534837484359741,40.82430468665227,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Itamogi,2022-12-31T00:00:00,1.32,2.4412682056427,84.94456103353788,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Itanhomi,2022-12-31T00:00:00,1.081818181818182,1.2733182907104492,17.701690737940663,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Jacui,2022-12-31T00:00:00,1.08,1.8492392301559448,71.22585464406896,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Jacutinga,2022-12-31T00:00:00,1.289920424403183,1.9120142459869385,48.22730222847539,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Juruaia,2022-12-31T00:00:00,1.5,1.8148527145385744,20.99018096923828,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Lagoa_Dourada,2022-12-31T00:00:00,1.5,2.633692502975464,75.57950019836426,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Lagoa_Formosa,2022-12-31T00:00:00,2.088607594936709,2.033764123916626,-2.625838915507,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Lagoa_Grande,2022-12-31T00:00:00,2.4,1.7124733924865725,-28.646941979726154,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Matutina,2022-12-31T00:00:00,1.8097165991902835,1.998279690742493,10.419481792705978,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Monte_Belo,2022-12-31T00:00:00,0.96,2.19815731048584,128.974719842275,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Monte_Carmelo,2022-12-31T00:00:00,1.7458874458874465,2.5158796310424805,44.103197314855656,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Monte_Santo_de_Minas,2022-12-31T00:00:00,1.4699677072120565,1.9994168281555176,36.01773823641443,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Muzambinho,2022-12-31T00:00:00,1.43993993993994,2.3347060680389404,62.13912839561358,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Nova_Resende,2022-12-31T00:00:00,1.5,2.435802936553955,62.386862436930336,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Passos,2022-12-31T00:00:00,1.290038314176245,2.026702880859375,57.10408431966051,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Patis,2022-12-31T00:00:00,2.044444444444444,2.853232860565185,39.5603029624276,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Patos_de_Minas,2022-12-31T00:00:00,1.91614730878187,1.7142667770385742,-10.53575217406614,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Patrocinio,2022-12-31T00:00:00,0.9438953724513282,2.961705207824707,213.7747354490211,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Pedrinopolis,2022-12-31T00:00:00,2.02051282051282,2.4040720462799072,18.983261173751785,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Perdizes,2022-12-31T00:00:00,1.501272727272727,2.302700996398926,53.38325639086949,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Piracema,2022-12-31T00:00:00,1.0,1.389122724533081,38.912272453308105,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Ponto_dos_Volantes,2022-12-31T00:00:00,0.6000000000000001,0.6117652654647827,1.9608775774637703,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Pratapolis,2022-12-31T00:00:00,1.2,2.208218097686768,84.01817480723064,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Pratinha,2022-12-31T00:00:00,1.720111731843575,2.06101655960083,19.81876069131168,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Presidente_Kubitschek,2022-12-31T00:00:00,0.896551724137931,2.092881441116333,133.43677612451407,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Presidente_Olegario,2022-12-31T00:00:00,1.8899598393574304,2.258877277374268,19.519855942667338,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Quartel_Geral,2022-12-31T00:00:00,3.0,4.379298210144043,45.97660700480144,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Rio_Paranaiba,2022-12-31T00:00:00,1.230031446540881,2.2803096771240234,85.38629102018129,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Rio_Vermelho,2022-12-31T00:00:00,0.9090909090909092,1.1202486753463743,23.2273542881012,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Romaria,2022-12-31T00:00:00,1.910526315789474,2.727268695831299,42.74960115921396,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Sacramento,2022-12-31T00:00:00,1.5575892857142857,1.8358922004699707,17.867541675343492,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Santa_Rosa_da_Serra,2022-12-31T00:00:00,2.22,2.0737032890319824,-6.5899419354962765,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Santo_Antonio_do_Amparo,2022-12-31T00:00:00,1.26,2.188077211380005,73.65692153809562,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Sao_Goncalo_do_Abaete,2022-12-31T00:00:00,1.5,2.369446992874145,57.96313285827637,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Sao_Gotardo,2022-12-31T00:00:00,1.50025974025974,2.054100513458252,36.91632577587035,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Sao_Joao_Batista_do_Gloria,2022-12-31T00:00:00,1.232209737827715,1.7386105060577393,41.09696204176793,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Sao_Pedro_da_Uniao,2022-12-31T00:00:00,1.4700000000000002,1.9000567197799685,29.255559168705307,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Sao_Sebastiao_do_Paraiso,2022-12-31T00:00:00,1.3172147001934242,2.024604797363281,53.703477274128616,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Serra_do_Salitre,2022-12-31T00:00:00,1.5168750000000002,2.45573091506958,61.89408587191296,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Tapira,2022-12-31T00:00:00,1.8000000000000005,1.7949235439300537,-0.282025337219253,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Tiros,2022-12-31T00:00:00,1.6801801801801797,2.765894651412964,64.61893099562417,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Tupaciguara,2022-12-31T00:00:00,2.4,2.3171329498291016,-3.4527937571207645,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Uberaba,2022-12-31T00:00:00,2.102941176470588,3.394837617874145,61.43283777303632,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Uberlandia,2022-12-31T00:00:00,1.681818181818182,2.5918760299682617,54.11154772784258,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Unai,2022-12-31T00:00:00,2.52010582010582,2.7274770736694336,8.228672459274183,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Varginha,2022-12-31T00:00:00,1.020039292730845,2.2425169944763184,119.84613832597184,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Varjao_de_Minas,2022-12-31T00:00:00,2.383928571428572,2.446541309356689,2.626451928070841,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_1_2022,Virginopolis,2022-12-31T00:00:00,1.314606741573034,1.6224286556243896,23.41551312014585,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:38:52
V42_cluster_2_2022,Abre_Campo,2021-12-31T00:00:00,1.2,1.9845329523086548,65.37774602572124,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Aimores,2021-12-31T00:00:00,1.634328358208955,2.522833585739136,54.36516004065948,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Alvarenga,2021-12-31T00:00:00,0.6304878048780488,1.012980580329895,60.66616554555395,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Andrelandia,2021-12-31T00:00:00,1.4,2.016239643096924,44.017117364066,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Antonio_Dias,2021-12-31T00:00:00,1.5,1.3936445713043213,-7.090361913045247,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Antonio_Prado_de_Minas,2021-12-31T00:00:00,0.9,2.015437602996826,123.9375114440918,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Bocaiuva,2021-12-31T00:00:00,1.7407407407407405,1.964441418647766,12.850890007424878,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Bom_Jesus_do_Amparo,2021-12-31T00:00:00,1.2,1.3824968338012695,15.208069483439132,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Campo_Belo,2021-12-31T00:00:00,1.5,2.4062716960906982,60.41811307271322,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Capela_Nova,2021-12-31T00:00:00,1.8000000000000005,2.4360084533691406,35.33380296495224,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Capitao_Eneas,2021-12-31T00:00:00,2.0,6.622181415557861,231.10907077789312,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Casa_Grande,2021-12-31T00:00:00,1.3230769230769233,2.401721954345703,81.52549654938451,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Cataguases,2021-12-31T00:00:00,1.25,2.8183393478393555,125.46714782714842,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Caxambu,2021-12-31T00:00:00,1.4387755102040822,2.0134098529815674,39.93912453347058,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Coimbra,2021-12-31T00:00:00,1.501182033096927,2.26402235031128,50.815977036483616,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Conselheiro_Pena,2021-12-31T00:00:00,1.2898734177215188,2.054803371429444,59.3027147624397,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Corrego_Novo,2021-12-31T00:00:00,1.8000000000000005,1.9717230796813965,9.5401710934109,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Cruzilia,2021-12-31T00:00:00,1.5027322404371577,1.7493596076965332,16.411930257623904,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Divisopolis,2021-12-31T00:00:00,0.9595505617977528,1.7916452884674072,86.71713193629888,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Esmeraldas,2021-12-31T00:00:00,1.7115384615384608,1.9000110626220703,11.011882310503031,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Espirito_Santo_do_Dourado,2021-12-31T00:00:00,1.200854700854701,2.2018041610717773,83.35308672270314,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Eugenopolis,2021-12-31T00:00:00,1.5,2.437124967575073,62.47499783833822,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Felicio_dos_Santos,2021-12-31T00:00:00,3.542087542087541,4.801625728607178,35.559205455925095,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Formiga,2021-12-31T00:00:00,1.347576301615799,2.762443780899048,104.99349666410464,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Guaraciaba,2021-12-31T00:00:00,2.4,1.2495657205581665,-47.93476164340973,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Guiricema,2021-12-31T00:00:00,2.1,2.838160514831543,35.15050070626394,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Iapu,2021-12-31T00:00:00,1.5578947368421048,1.9132899045944207,22.81252765977709,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Ijaci,2021-12-31T00:00:00,1.2375,3.5537655353546143,187.17297255390824,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Imbe_de_Minas,2021-12-31T00:00:00,1.2,1.7711719274520874,47.597660621007286,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Inhapim,2021-12-31T00:00:00,1.2,2.1205978393554688,76.71648661295573,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Irai_de_Minas,2021-12-31T00:00:00,2.472300469483568,4.810460090637207,94.57424977321024,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Itamarati_de_Minas,2021-12-31T00:00:00,1.2,1.3251866102218628,10.43221751848857,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Jequeri,2021-12-31T00:00:00,1.5,2.5310490131378174,68.73660087585449,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Lamim,2021-12-31T00:00:00,1.2800000000000002,3.102344036102295,142.37062782049173,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Mar_de_Espanha,2021-12-31T00:00:00,1.4199999999999997,2.6847565174102783,89.06736038100556,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Mata_Verde,2021-12-31T00:00:00,1.319277108433735,2.1296956539154053,61.4289856392499,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Monte_Alegre_de_Minas,2021-12-31T00:00:00,2.765957446808511,4.842387676239014,75.07093906402586,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Nova_Era,2021-12-31T00:00:00,1.5,1.447379231452942,-3.5080512364705405,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Nova_Ponte,2021-12-31T00:00:00,2.5482352941176467,2.625895023345948,3.047588635459625,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Paula_Candido,2021-12-31T00:00:00,1.5,1.7842425107955933,18.94950071970621,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Pecanha,2021-12-31T00:00:00,1.807692307692308,2.5822913646698,42.85016059875485,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Pedra_Bonita,2021-12-31T00:00:00,1.08,2.6025280952453613,140.97482363382974,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Pedra_Dourada,2021-12-31T00:00:00,0.96,1.795890212059021,87.07189708948138,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Pedra_do_Anta,2021-12-31T00:00:00,0.9,1.6329662799835205,81.44069777594672,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Piedade_de_Caratinga,2021-12-31T00:00:00,1.2,1.5680707693099976,30.672564109166466,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Pocrane,2021-12-31T00:00:00,1.3228346456692908,1.4034357070922852,6.093056429000174,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Presidente_Bernardes,2021-12-31T00:00:00,1.56,1.780097723007202,14.108828397897568,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sabinopolis,2021-12-31T00:00:00,1.2,1.6420927047729492,36.84105873107911,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Santa_Barbara,2021-12-31T00:00:00,0.9333333333333332,1.3944472074508667,49.40505794116432,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Santo_Antonio_do_Grama,2021-12-31T00:00:00,1.9130434782608696,5.254419326782227,174.6628284454345,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sao_Domingos_do_Prata,2021-12-31T00:00:00,0.903846153846154,0.8950801491737366,-0.9698558360972312,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sao_Geraldo,2021-12-31T00:00:00,1.5,3.4083049297332764,127.22032864888509,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sao_Jose_da_Barra,2021-12-31T00:00:00,1.682068965517241,3.5894742012023926,113.39637522523454,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sao_Jose_do_Alegre,2021-12-31T00:00:00,1.7823529411764714,1.7864519357681274,0.2299765942513311,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sao_Miguel_do_Anta,2021-12-31T00:00:00,1.2603053435114495,1.834009885787964,45.52107512914802,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sao_Sebastiao_da_Vargem_Alegre,2021-12-31T00:00:00,1.020238095238095,2.113220691680908,107.13014947630846,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sao_Sebastiao_do_Anta,2021-12-31T00:00:00,1.5,2.3537635803222656,56.917572021484375,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Senhora_dos_Remedios,2021-12-31T00:00:00,1.5,2.425410032272339,61.69400215148926,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Teixeiras,2021-12-31T00:00:00,1.32,1.939736008644104,46.94969762455333,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Visconde_do_Rio_Branco,2021-12-31T00:00:00,1.25,1.5851303339004517,26.810426712036133,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Abre_Campo,2022-12-31T00:00:00,1.2,1.805925726890564,50.49381057421367,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Aimores,2022-12-31T00:00:00,2.332558139534884,2.59087872505188,11.074561492752574,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Alvarenga,2022-12-31T00:00:00,0.9,0.9857949018478394,9.53276687198215,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Andrelandia,2022-12-31T00:00:00,1.8000000000000005,1.8527860641479488,2.932559119330497,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Antonio_Dias,2022-12-31T00:00:00,1.5,1.95321261882782,30.21417458852132,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Antonio_Prado_de_Minas,2022-12-31T00:00:00,1.2,1.8671144247055047,55.59286872545879,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Bocaiuva,2022-12-31T00:00:00,3.6000000000000005,1.9430513381958008,-46.02635171678332,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Bom_Jesus_do_Amparo,2022-12-31T00:00:00,1.8000000000000005,1.6208558082580566,-9.952455096774644,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Campo_Belo,2022-12-31T00:00:00,1.2,1.9965569972991943,66.37974977493288,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Capela_Nova,2022-12-31T00:00:00,2.1016393442622947,2.241973161697388,6.677350127566828,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Capitao_Eneas,2022-12-31T00:00:00,2.6000000000000005,6.680395126342773,156.9382740901066,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Casa_Grande,2022-12-31T00:00:00,2.107692307692308,2.373573064804077,12.614780446908744,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Cataguases,2022-12-31T00:00:00,1.5,2.952012300491333,96.80082003275552,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Caxambu,2022-12-31T00:00:00,1.4387755102040822,2.0237982273101807,40.661153387516045,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Coimbra,2022-12-31T00:00:00,1.799054373522459,2.2878217697143555,27.16801689739451,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Conselheiro_Pena,2022-12-31T00:00:00,1.8000000000000005,1.8792378902435305,4.402105013529444,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Corrego_Novo,2022-12-31T00:00:00,1.377777777777778,2.4493248462677,77.7735775516879,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Cruzilia,2022-12-31T00:00:00,1.5027322404371577,1.8738055229187007,24.693240252408177,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Divisopolis,2022-12-31T00:00:00,1.259523809523809,1.7789181470870972,41.237357613720434,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Esmeraldas,2022-12-31T00:00:00,2.111111111111112,2.157899618148804,2.216297701785415,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Espirito_Santo_do_Dourado,2022-12-31T00:00:00,1.319148936170213,2.1393725872039795,62.17824451385007,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Eugenopolis,2022-12-31T00:00:00,1.8000000000000005,2.292893886566162,27.3829936981201,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Felicio_dos_Santos,2022-12-31T00:00:00,3.306397306397306,5.753465175628662,74.01009747064286,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Formiga,2022-12-31T00:00:00,1.8000000000000005,2.551762580871582,41.76458782619898,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Guaraciaba,2022-12-31T00:00:00,1.8000000000000005,2.593027114868164,44.05706193712021,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Guiricema,2022-12-31T00:00:00,2.4,2.8698549270629883,19.57728862762452,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Iapu,2022-12-31T00:00:00,1.5614035087719298,2.007356643676758,28.561043471432807,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Ijaci,2022-12-31T00:00:00,1.173913043478261,3.157270908355713,168.95270700807922,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Imbe_de_Minas,2022-12-31T00:00:00,1.3799999999999997,1.919859766960144,39.120272968126415,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Inhapim,2022-12-31T00:00:00,1.8000000000000005,2.0532894134521484,14.071634080674896,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Irai_de_Minas,2022-12-31T00:00:00,2.386666666666667,4.73677921295166,98.46840277730416,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Itamarati_de_Minas,2022-12-31T00:00:00,1.261538461538461,1.457937479019165,15.568214800299712,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Jequeri,2022-12-31T00:00:00,1.8000000000000005,2.3214974403381348,28.972080018785245,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Lamim,2022-12-31T00:00:00,2.4,2.842085123062134,18.42021346092225,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Mar_de_Espanha,2022-12-31T00:00:00,2.1,2.562906265258789,22.04315548851376,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Mata_Verde,2022-12-31T00:00:00,1.6792452830188678,2.378488540649414,41.64032882518984,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Monte_Alegre_de_Minas,2022-12-31T00:00:00,2.3296703296703294,4.869174957275391,109.00703826040596,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Nova_Era,2022-12-31T00:00:00,1.5,1.6100900173187256,7.339334487915039,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Nova_Ponte,2022-12-31T00:00:00,2.710714285714286,3.3040366172790527,21.888043852191664,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Paula_Candido,2022-12-31T00:00:00,1.6198529411764713,1.798938274383545,11.055653797622336,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Pecanha,2022-12-31T00:00:00,2.287671232876712,2.6046030521392822,13.853905871956655,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Pedra_Bonita,2022-12-31T00:00:00,1.8000000000000005,2.053009510040283,14.056083891126828,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Pedra_Dourada,2022-12-31T00:00:00,1.2,1.7347071170806885,44.55892642339071,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Pedra_do_Anta,2022-12-31T00:00:00,1.438297872340426,1.7630064487457275,22.575892146522435,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Piedade_de_Caratinga,2022-12-31T00:00:00,1.56,2.18251371383667,39.90472524594038,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Pocrane,2022-12-31T00:00:00,1.440944881889764,1.560086011886597,8.268264212894954,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Presidente_Bernardes,2022-12-31T00:00:00,1.501449275362319,1.8403003215789795,22.56826466114824,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sabinopolis,2022-12-31T00:00:00,1.32,1.18550705909729,-10.188859159296214,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Santa_Barbara,2022-12-31T00:00:00,1.083333333333333,1.3985401391983032,29.09601284907417,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Santo_Antonio_do_Grama,2022-12-31T00:00:00,1.565217391304348,4.199753761291504,168.3176014158461,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sao_Domingos_do_Prata,2022-12-31T00:00:00,1.078125,1.3600326776504517,26.14795850670856,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sao_Geraldo,2022-12-31T00:00:00,1.561038961038961,3.320772647857666,112.72836429703852,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sao_Jose_da_Barra,2022-12-31T00:00:00,2.3076923076923066,3.0948240756988525,34.10904328028367,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sao_Jose_do_Alegre,2022-12-31T00:00:00,1.682352941176471,2.1524112224578857,27.94052720903513,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sao_Miguel_do_Anta,2022-12-31T00:00:00,1.5,1.7887554168701172,19.25036112467448,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sao_Sebastiao_da_Vargem_Alegre,2022-12-31T00:00:00,1.020238095238095,2.022966861724853,98.28379974899389,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Sao_Sebastiao_do_Anta,2022-12-31T00:00:00,1.8000000000000005,2.2021892070770264,22.34384483761256,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Senhora_dos_Remedios,2022-12-31T00:00:00,1.466666666666667,2.4211130142211914,65.07588733326301,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Teixeiras,2022-12-31T00:00:00,1.5,1.891207456588745,26.08049710591634,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_2_2022,Visconde_do_Rio_Branco,2022-12-31T00:00:00,1.6666666666666672,1.778397798538208,6.703867912292447,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:39:27
V42_cluster_3_2022,Aiuruoca,2021-12-31T00:00:00,1.2,1.6159751415252686,34.66459512710572,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Albertina,2021-12-31T00:00:00,1.56,1.771198034286499,13.53833553118583,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Baependi,2021-12-31T00:00:00,1.079646017699115,1.6840369701385498,55.98047346365258,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Bandeira_do_Sul,2021-12-31T00:00:00,1.8011695906432754,1.6632698774337769,-7.656120441176703,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Borda_da_Mata,2021-12-31T00:00:00,1.2,1.7954046726226809,49.61705605189006,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Brazopolis,2021-12-31T00:00:00,1.200934579439252,1.3886401653289795,15.629959291985088,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Bueno_Brandao,2021-12-31T00:00:00,1.2,1.7654110193252563,47.11758494377137,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Cachoeira_de_Minas,2021-12-31T00:00:00,1.5602040816326532,1.91960084438324,23.035240516388143,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Caldas,2021-12-31T00:00:00,1.2,1.642287015914917,36.85725132624309,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Cambuquira,2021-12-31T00:00:00,1.2,1.7642498016357422,47.02081680297852,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Campanha,2021-12-31T00:00:00,1.325757575757576,1.6921385526657104,27.635593686785004,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Careacu,2021-12-31T00:00:00,1.2,1.4236994981765747,18.641624848047897,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Carmo_da_Cachoeira,2021-12-31T00:00:00,1.439980158730159,1.7493367195129397,21.48339051112936,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Carmo_de_Minas,2021-12-31T00:00:00,1.3799999999999997,1.6069300174713137,16.444204164587962,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Carrancas,2021-12-31T00:00:00,1.214285714285714,1.7267892360687256,42.20617238213038,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Carvalhopolis,2021-12-31T00:00:00,1.14,1.6171038150787354,41.85121184901189,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Conceicao_das_Pedras,2021-12-31T00:00:00,1.5,1.6148046255111694,7.653641700744629,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Conceicao_do_Rio_Verde,2021-12-31T00:00:00,1.500115340253749,1.8794043064117432,25.2839869028895,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Conceicao_dos_Ouros,2021-12-31T00:00:00,1.26,1.4991648197174072,18.98133489820692,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Congonhal,2021-12-31T00:00:00,1.6215384615384625,1.77544367313385,9.49130811546509,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Cordislandia,2021-12-31T00:00:00,1.319791666666667,1.7595672607421875,33.32159197415152,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Cristina,2021-12-31T00:00:00,1.68,1.6989331245422363,1.1269716989426424,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Datas,2021-12-31T00:00:00,1.0,2.1042654514312744,110.42654514312744,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Dom_Vicoso,2021-12-31T00:00:00,1.2,1.516392469406128,26.36603911717733,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Fama,2021-12-31T00:00:00,1.5,1.627573847770691,8.504923184712727,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Fortaleza_de_Minas,2021-12-31T00:00:00,1.26,1.5000663995742798,19.052888855101568,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Heliodora,2021-12-31T00:00:00,1.2,1.65679931640625,38.06660970052084,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Ibitiura_de_Minas,2021-12-31T00:00:00,1.9198795180722887,1.7898521423339844,-6.772684145766739,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Inconfidentes,2021-12-31T00:00:00,1.5,2.628473997116089,75.23159980773926,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Ingai,2021-12-31T00:00:00,1.320075757575758,1.7383415699005127,31.68498549605027,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Itajuba,2021-12-31T00:00:00,1.181818181818182,1.8207600116729736,54.064308680020815,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Itutinga,2021-12-31T00:00:00,1.2,1.607125997543335,33.92716646194459,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Jesuania,2021-12-31T00:00:00,1.2,1.5688278675079346,30.735655625661217,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Lambari,2021-12-31T00:00:00,1.199954863461973,1.5960237979888916,33.00698606147782,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Luminarias,2021-12-31T00:00:00,1.380229885057471,1.910714864730835,38.4345380009849,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Maria_da_Fe,2021-12-31T00:00:00,1.444444444444444,1.6381734609603882,13.412008835719218,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Monsenhor_Paulo,2021-12-31T00:00:00,1.41,1.8000078201293943,27.660129087191088,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Monte_Siao,2021-12-31T00:00:00,1.5,1.824820637702942,21.65470918019613,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Natercia,2021-12-31T00:00:00,1.4399193548387097,1.586794376373291,10.200225522424017,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Olimpio_Noronha,2021-12-31T00:00:00,1.20026525198939,1.4995416402816772,24.934187488661287,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Ouro_Fino,2021-12-31T00:00:00,1.2,1.880194902420044,56.68290853500368,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Paraisopolis,2021-12-31T00:00:00,1.2,1.7672491073608398,47.270758946736656,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Pedralva,2021-12-31T00:00:00,1.5,1.7328593730926514,15.523958206176758,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Pirangucu,2021-12-31T00:00:00,1.333333333333333,1.2954564094543457,-2.840769290924051,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Piranguinho,2021-12-31T00:00:00,1.380549682875264,1.4616966247558594,5.8778719003861655,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Poco_Fundo,2021-12-31T00:00:00,0.9601760412194076,1.496376633644104,55.84398791535462,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Pocos_de_Caldas,2021-12-31T00:00:00,1.4199999999999997,1.7261005640029907,21.55637774668952,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Pouso_Alegre,2021-12-31T00:00:00,1.2,1.5594656467437744,29.95547056198121,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Pouso_Alto,2021-12-31T00:00:00,1.5047619047619047,1.5967485904693604,6.113039240052429,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Santa_Rita_de_Caldas,2021-12-31T00:00:00,1.682051282051282,1.9876716136932373,18.169501423835754,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Santa_Rita_do_Sapucai,2021-12-31T00:00:00,1.5,1.6946399211883545,12.9759947458903,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Sao_Bento_Abade,2021-12-31T00:00:00,1.32,1.9692082405090328,49.18244246280554,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Sao_Goncalo_do_Sapucai,2021-12-31T00:00:00,1.62,1.9797724485397337,22.20817583578604,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Sao_Joao_da_Mata,2021-12-31T00:00:00,0.8995327102803738,1.4608123302459717,62.39679931046126,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Sao_Joao_del_Rei,2021-12-31T00:00:00,1.296875,1.847078800201416,42.42535326854292,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Sao_Lourenco,2021-12-31T00:00:00,1.5087719298245608,2.4309566020965576,61.12154223198121,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Sao_Sebastiao_da_Bela_Vista,2021-12-31T00:00:00,1.2,1.5053184032440186,25.443200270334888,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Sao_Tome_das_Letras,2021-12-31T00:00:00,1.2,1.7085646390914917,42.38038659095765,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Senador_Jose_Bento,2021-12-31T00:00:00,1.32,1.4809646606445312,12.194292473070544,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Serrania,2021-12-31T00:00:00,1.08,1.7338684797286987,60.54337775265728,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Silvianopolis,2021-12-31T00:00:00,0.9601265822784812,1.323599100112915,37.85672895045521,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Soledade_de_Minas,2021-12-31T00:00:00,1.7204819277108432,2.425651788711548,40.98676362959278,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Tocos_do_Moji,2021-12-31T00:00:00,1.259776536312849,1.5310144424438477,21.530636451196788,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Tres_Coracoes,2021-12-31T00:00:00,1.5,2.083336591720581,38.8891061147054,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Turvolandia,2021-12-31T00:00:00,1.3797385620915028,1.8634743690490725,35.059961375892065,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Virginia,2021-12-31T00:00:00,1.433333333333333,1.606461763381958,12.078727677811058,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Aiuruoca,2022-12-31T00:00:00,1.5,1.5737627744674685,4.917518297831217,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Albertina,2022-12-31T00:00:00,1.44,1.7656341791152954,22.613484660784408,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Baependi,2022-12-31T00:00:00,1.079558011049724,1.579953670501709,46.35190090113063,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Bandeira_do_Sul,2022-12-31T00:00:00,1.110344827586207,1.7715227603912354,59.54708090480069,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Borda_da_Mata,2022-12-31T00:00:00,1.2,1.7191225290298462,43.26021075248719,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Brazopolis,2022-12-31T00:00:00,1.200934579439252,1.424128770828247,18.585041617605032,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Bueno_Brandao,2022-12-31T00:00:00,1.440196078431373,1.7881271839141846,24.158592756464785,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Cachoeira_de_Minas,2022-12-31T00:00:00,1.5,1.916699171066284,27.77994473775228,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Caldas,2022-12-31T00:00:00,1.2,1.649522304534912,37.46019204457601,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Cambuquira,2022-12-31T00:00:00,1.32,1.7210159301757812,30.37999471028645,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Campanha,2022-12-31T00:00:00,1.266469038208169,1.6777584552764893,32.4752840109082,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Careacu,2022-12-31T00:00:00,1.2,1.4169447422027588,18.078728516896568,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Carmo_da_Cachoeira,2022-12-31T00:00:00,1.2,1.7505496740341189,45.87913950284322,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Carmo_de_Minas,2022-12-31T00:00:00,1.14,1.5643024444580078,37.21951267175508,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Carrancas,2022-12-31T00:00:00,1.6785714285714293,1.680863618850708,0.1365560166378822,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Carvalhopolis,2022-12-31T00:00:00,1.44,1.5506892204284668,7.686751418643531,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Conceicao_das_Pedras,2022-12-31T00:00:00,1.375,1.6396169662475586,19.244870272549715,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Conceicao_do_Rio_Verde,2022-12-31T00:00:00,1.5,1.811190843582153,20.74605623881022,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Conceicao_dos_Ouros,2022-12-31T00:00:00,1.261168384879725,1.488947868347168,18.06098901608335,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Congonhal,2022-12-31T00:00:00,1.080597014925373,1.7522523403167725,62.15594862047482,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Cordislandia,2022-12-31T00:00:00,1.260122699386503,1.7152671813964844,36.11906064636173,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Cristina,2022-12-31T00:00:00,1.320183486238532,1.8189001083374023,37.77631119442451,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Datas,2022-12-31T00:00:00,0.75,1.9777384996414185,163.69846661885578,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Dom_Vicoso,2022-12-31T00:00:00,1.320588235294118,1.4727661609649658,11.5234954850976,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Fama,2022-12-31T00:00:00,1.379787234042553,1.6622841358184814,20.47394662061471,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Fortaleza_de_Minas,2022-12-31T00:00:00,1.5,1.5800433158874512,5.336221059163412,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Heliodora,2022-12-31T00:00:00,1.080140597539543,1.6331889629364014,51.20151641894116,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Ibitiura_de_Minas,2022-12-31T00:00:00,1.3799999999999997,1.9221558570861816,39.2866563105929,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Inconfidentes,2022-12-31T00:00:00,1.739917695473251,2.4897050857543945,43.09326770064283,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Ingai,2022-12-31T00:00:00,1.380228136882129,1.726681351661682,25.1011557815489,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Itajuba,2022-12-31T00:00:00,1.068965517241379,1.803670883178711,68.73050197478268,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Itutinga,2022-12-31T00:00:00,1.8000000000000005,1.5903570652008057,-11.646829711066363,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Jesuania,2022-12-31T00:00:00,1.2,1.5544631481170654,29.538595676422126,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Lambari,2022-12-31T00:00:00,1.2,1.5917052030563354,32.642100254694626,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Luminarias,2022-12-31T00:00:00,1.380229885057471,1.898874998092652,37.57671954868479,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Maria_da_Fe,2022-12-31T00:00:00,1.2,1.6862142086029053,40.517850716908775,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Monsenhor_Paulo,2022-12-31T00:00:00,1.23,1.7653751373291016,43.52643392919525,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Monte_Siao,2022-12-31T00:00:00,1.2,1.7923052310943604,49.358769257863365,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Natercia,2022-12-31T00:00:00,1.4399193548387097,1.5907957553863523,10.478114627783652,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Olimpio_Noronha,2022-12-31T00:00:00,1.2,1.488880276679993,24.0733563899994,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Ouro_Fino,2022-12-31T00:00:00,1.2,1.80817973613739,50.68164467811586,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Paraisopolis,2022-12-31T00:00:00,1.322222222222222,1.8368991613388064,38.92514665587613,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Pedralva,2022-12-31T00:00:00,1.320091324200913,1.9100598096847528,44.69149025284023,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Pirangucu,2022-12-31T00:00:00,1.333333333333333,1.304652452468872,-2.151066064834573,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Piranguinho,2022-12-31T00:00:00,1.380549682875264,1.4652302265167236,6.133828046310943,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Poco_Fundo,2022-12-31T00:00:00,1.079978925184405,1.4107341766357422,30.62608387426275,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Pocos_de_Caldas,2022-12-31T00:00:00,1.169867549668874,1.8442832231521609,57.64889142144144,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Pouso_Alegre,2022-12-31T00:00:00,1.5,1.4894230365753174,-0.7051308949788411,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Pouso_Alto,2022-12-31T00:00:00,1.5,1.5700080394744873,4.667202631632486,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Santa_Rita_de_Caldas,2022-12-31T00:00:00,1.8000000000000005,1.9862362146377563,10.346456368764226,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Santa_Rita_do_Sapucai,2022-12-31T00:00:00,1.14,1.684475660324097,47.76102283544709,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Sao_Bento_Abade,2022-12-31T00:00:00,1.2,1.898624062538147,58.21867187817892,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Sao_Goncalo_do_Sapucai,2022-12-31T00:00:00,1.08,1.987911581993103,84.06588722158361,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Sao_Joao_da_Mata,2022-12-31T00:00:00,0.959748427672956,1.4343819618225098,49.45395277180803,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Sao_Joao_del_Rei,2022-12-31T00:00:00,1.836244541484716,1.8668465614318848,1.666554712937375,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Sao_Lourenco,2022-12-31T00:00:00,1.8000000000000005,2.0986244678497314,16.590248213873952,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Sao_Sebastiao_da_Bela_Vista,2022-12-31T00:00:00,1.5,1.46375572681427,-2.416284879048665,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Sao_Tome_das_Letras,2022-12-31T00:00:00,1.434375,1.5464004278182983,7.81005161260468,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Senador_Jose_Bento,2022-12-31T00:00:00,1.32,1.4679205417633057,11.206101648735272,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Serrania,2022-12-31T00:00:00,1.2,1.6366345882415771,36.3862156867981,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Silvianopolis,2022-12-31T00:00:00,0.9,1.2929635047912598,43.662611643473305,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Soledade_de_Minas,2022-12-31T00:00:00,1.5,2.231384515762329,48.7589677174886,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Tocos_do_Moji,2022-12-31T00:00:00,1.5,1.514596462249756,0.9730974833170573,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Tres_Coracoes,2022-12-31T00:00:00,1.08,2.063184261322021,91.035579752039,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Turvolandia,2022-12-31T00:00:00,1.4398692810457523,1.8262494802474976,26.83439422508716,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_3_2022,Virginia,2022-12-31T00:00:00,1.62,1.5561922788619995,-3.938748218395098,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:02
V42_cluster_4_2022,Abadia_dos_Dourados,2021-12-31T00:00:00,1.8000000000000005,2.4540328979492188,36.3351609971788,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Acucena,2021-12-31T00:00:00,0.9,1.1093242168426514,23.258246315850148,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Agua_Boa,2021-12-31T00:00:00,0.6171428571428572,1.269086837768555,105.6390709347195,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Aguas_Vermelhas,2021-12-31T00:00:00,3.0,3.8917624950408936,29.72541650136312,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Angelandia,2021-12-31T00:00:00,1.5133433283358322,1.2877891063690186,-14.904365568839378,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Aricanduva,2021-12-31T00:00:00,0.8394736842105264,1.196152687072754,42.48840786446597,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Ataleia,2021-12-31T00:00:00,0.6266666666666667,0.93543803691864,49.272027167868096,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Bandeira,2021-12-31T00:00:00,0.96,0.986481547355652,2.75849451621375,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Berilo,2021-12-31T00:00:00,1.8017241379310345,0.8209282159805298,-54.43652006998016,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Berizal,2021-12-31T00:00:00,3.541935483870968,3.1620802879333496,-10.724509174923648,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Bonfinopolis_de_Minas,2021-12-31T00:00:00,3.0,2.9329137802124023,-2.236207326253255,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Buritis,2021-12-31T00:00:00,3.1195039458850062,2.6855318546295166,-13.911573724019489,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Buritizeiro,2021-12-31T00:00:00,3.0,2.680116891860962,-10.66277027130127,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Capelinha,2021-12-31T00:00:00,1.4350318471337582,1.233936786651611,-14.013282066443434,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Carai,2021-12-31T00:00:00,1.218,1.3205689191818235,8.421093528885367,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Caranaiba,2021-12-31T00:00:00,2.28,1.3708994388580322,-39.87283162903367,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Catuji,2021-12-31T00:00:00,1.384615384615385,1.1549110412597656,-16.589758131239172,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Congonhas_do_Norte,2021-12-31T00:00:00,0.7333333333333333,1.245777249336243,69.87871581857857,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Coroaci,2021-12-31T00:00:00,1.5,1.7196338176727295,14.6422545115153,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Cuparaque,2021-12-31T00:00:00,1.32,1.1250810623168943,-14.766586188114056,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Diamantina,2021-12-31T00:00:00,2.9196787148594376,2.2830474376678467,-21.804840167910065,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Formoso,2021-12-31T00:00:00,3.0,3.0515129566192627,1.717098553975423,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Franciscopolis,2021-12-31T00:00:00,1.2,1.9428752660751345,61.90627217292787,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Frei_Gaspar,2021-12-31T00:00:00,1.2,1.1217526197433472,-6.520615021387732,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Grao_Mogol,2021-12-31T00:00:00,1.155555555555555,1.156752586364746,0.1035892046415367,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Indaiabira,2021-12-31T00:00:00,1.984444444444444,2.8898911476135254,45.62721348556404,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Itabirinha,2021-12-31T00:00:00,0.9659090909090908,0.8966224789619446,-7.173202178057495,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Itacambira,2021-12-31T00:00:00,0.5,1.53282368183136,206.564736366272,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Itaipe,2021-12-31T00:00:00,0.7792792792792793,0.9201646447181702,18.07893128753397,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Itamarandiba,2021-12-31T00:00:00,1.632,1.7233253717422483,5.595917386167196,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Itambacuri,2021-12-31T00:00:00,0.8987341772151899,1.040987253189087,15.828159157658964,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Jequitinhonha,2021-12-31T00:00:00,1.8000000000000005,1.8620250225067136,3.445834583706311,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Joao_Pinheiro,2021-12-31T00:00:00,2.4,2.841710090637207,18.40458710988363,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Jose_Goncalves_de_Minas,2021-12-31T00:00:00,1.502673796791444,1.3771418333053589,-8.35390646686759,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Juiz_de_Fora,2021-12-31T00:00:00,1.142857142857143,1.3599913120269775,18.99923980236052,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Ladainha,2021-12-31T00:00:00,0.7199999999999999,1.0009139776229858,39.01583022541472,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Malacacheta,2021-12-31T00:00:00,1.8000000000000005,1.736175537109375,-3.545803493923625,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Mantena,2021-12-31T00:00:00,0.7289999999999999,0.6871622800827026,-5.739056230081926,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Minas_Novas,2021-12-31T00:00:00,1.7848101265822778,1.0151898860931396,-43.1205666657035,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Monte_Formoso,2021-12-31T00:00:00,0.55,0.9563098549842834,73.87451908805153,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Ninheira,2021-12-31T00:00:00,2.100246002460024,2.837333917617798,35.095313324935326,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Nova_Belem,2021-12-31T00:00:00,1.0,1.0068957805633545,0.6895780563354492,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Novo_Cruzeiro,2021-12-31T00:00:00,1.226392251815981,1.1190122365951538,-8.755764321066467,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Novorizonte,2021-12-31T00:00:00,1.307692307692308,1.2973964214324951,-0.787332478691568,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Ouro_Verde_de_Minas,2021-12-31T00:00:00,1.942857142857143,1.174762487411499,-39.534283736172846,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Padre_Paraiso,2021-12-31T00:00:00,0.4787878787878789,0.6988900303840637,45.970702548570245,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Paracatu,2021-12-31T00:00:00,2.4,2.485123872756958,3.546828031539921,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Pedra_Azul,2021-12-31T00:00:00,1.2,1.0759578943252563,-10.336842139561968,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Pirapora,2021-12-31T00:00:00,3.0,3.112126111984253,3.737537066141764,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Pote,2021-12-31T00:00:00,0.6000000000000001,0.7659190893173218,27.653181552886945,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Rio_Pardo_de_Minas,2021-12-31T00:00:00,2.764006791171477,3.225904941558838,16.71118001094322,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Santa_Barbara_do_Monte_Verde,2021-12-31T00:00:00,1.4,1.2137283086776731,-13.305120808737614,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Santa_Maria_do_Suacui,2021-12-31T00:00:00,0.8,0.8520358800888062,6.504485011100764,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Santo_Antonio_do_Retiro,2021-12-31T00:00:00,1.2,1.4613466262817385,21.778885523478195,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Sao_Goncalo_do_Rio_Preto,2021-12-31T00:00:00,3.36,3.68094539642334,9.551946322123214,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Sao_Joao_do_Manteninha,2021-12-31T00:00:00,0.7,0.7979665398597717,13.995219979967397,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Sao_Joao_do_Paraiso,2021-12-31T00:00:00,3.0614754098360666,2.278035879135132,-25.590260440566,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Sao_Sebastiao_do_Maranhao,2021-12-31T00:00:00,0.9655172413793104,0.8939496874809265,-7.412353796618329,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Senador_Modestino_Goncalves,2021-12-31T00:00:00,2.7000000000000006,3.0209059715271,11.885406352855515,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Setubinha,2021-12-31T00:00:00,0.9329787234042556,0.9881657361984252,5.9151416221801165,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Taiobeiras,2021-12-31T00:00:00,3.49869451697128,2.63655161857605,-24.641845528759188,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Teofilo_Otoni,2021-12-31T00:00:00,0.6800000000000002,0.9275843501091005,36.40946325133825,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Turmalina,2021-12-31T00:00:00,1.2797783933518008,1.661956548690796,29.862838544886845,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Urucuia,2021-12-31T00:00:00,1.799276672694394,2.271763801574707,26.25983741415207,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Vargem_Grande_do_Rio_Pardo,2021-12-31T00:00:00,1.644444444444444,1.9745525121688845,20.07413925351327,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Varzea_da_Palma,2021-12-31T00:00:00,1.68,3.2051925659179688,90.78527178083148,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Abadia_dos_Dourados,2022-12-31T00:00:00,2.0384615384615383,2.0431785583496094,0.2314009756412213,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Acucena,2022-12-31T00:00:00,1.0,1.0701807737350464,7.018077373504639,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Agua_Boa,2022-12-31T00:00:00,0.9136363636363636,1.1323845386505127,23.94258631995662,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Aguas_Vermelhas,2022-12-31T00:00:00,3.0,3.511438131332397,17.047937711079918,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Angelandia,2022-12-31T00:00:00,1.9799126637554585,1.408581256866455,-28.85639439293819,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Aricanduva,2022-12-31T00:00:00,1.2,1.203251600265503,0.2709666887919145,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Ataleia,2022-12-31T00:00:00,0.7142857142857143,0.8544705510139465,19.62587714195251,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Bandeira,2022-12-31T00:00:00,1.5,1.067967176437378,-28.802188237508137,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Berilo,2022-12-31T00:00:00,1.5625,1.2761476039886477,-18.32655334472656,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Berizal,2022-12-31T00:00:00,3.570967741935484,4.328365325927734,21.209869109087418,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Bonfinopolis_de_Minas,2022-12-31T00:00:00,3.3,3.080850124359131,-6.640905322450576,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Buritis,2022-12-31T00:00:00,3.0,3.0508172512054443,1.6939083735148113,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Buritizeiro,2022-12-31T00:00:00,3.0,2.883734703063965,-3.8755098978678384,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Capelinha,2022-12-31T00:00:00,1.576751592356688,1.320788860321045,-16.23354834562551,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Carai,2022-12-31T00:00:00,1.2,1.6334872245788574,36.12393538157146,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Caranaiba,2022-12-31T00:00:00,3.405405405405405,1.329455852508545,-60.96042337871732,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Catuji,2022-12-31T00:00:00,1.3230769230769233,1.569188117980957,18.6014275218165,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Congonhas_do_Norte,2022-12-31T00:00:00,0.9,1.2329362630844116,36.992918120490174,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Coroaci,2022-12-31T00:00:00,2.1,1.7163305282592771,-18.269974844796323,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Cuparaque,2022-12-31T00:00:00,1.02,1.2783453464508057,25.327975142235847,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Diamantina,2022-12-31T00:00:00,2.965183752417796,2.212503671646118,-25.3839270553788,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Formoso,2022-12-31T00:00:00,3.3607142857142858,3.0554986000061035,-9.081869500349738,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Franciscopolis,2022-12-31T00:00:00,1.255555555555556,1.8564258813858032,47.85692860594888,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Frei_Gaspar,2022-12-31T00:00:00,1.133333333333333,1.5724968910217283,38.74972567838784,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Grao_Mogol,2022-12-31T00:00:00,1.177777777777778,1.2119776010513306,2.9037585798299315,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Indaiabira,2022-12-31T00:00:00,2.1723404255319148,2.7429463863372803,26.266875766750424,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Itabirinha,2022-12-31T00:00:00,0.8674242424242423,0.9607735872268676,10.761671191219694,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Itacambira,2022-12-31T00:00:00,0.75,1.2991316318511963,73.21755091349283,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Itaipe,2022-12-31T00:00:00,0.9604651162790696,0.9223594665527344,-3.967416315332713,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Itamarandiba,2022-12-31T00:00:00,2.136,1.883145809173584,-11.837743016217985,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Itambacuri,2022-12-31T00:00:00,0.9620253164556964,0.9870390892028807,2.6001158513520752,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Jequitinhonha,2022-12-31T00:00:00,2.4,2.3457603454589844,-2.259985605875648,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Joao_Pinheiro,2022-12-31T00:00:00,2.4,2.627075433731079,9.461476405461632,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Jose_Goncalves_de_Minas,2022-12-31T00:00:00,1.5,1.5702316761016846,4.682111740112305,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Juiz_de_Fora,2022-12-31T00:00:00,1.5714285714285707,0.8312005996704102,-47.10541638461024,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Ladainha,2022-12-31T00:00:00,0.6000000000000001,0.9672991633415222,61.216527223587015,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Malacacheta,2022-12-31T00:00:00,1.5595238095238098,1.9303703308105469,23.77947159395871,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Mantena,2022-12-31T00:00:00,0.75,0.6735609769821167,-10.191869735717772,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Minas_Novas,2022-12-31T00:00:00,1.7848101265822778,1.5542597770690918,-12.917360008185602,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Monte_Formoso,2022-12-31T00:00:00,0.6000000000000001,1.1528222560882568,92.13704268137612,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Ninheira,2022-12-31T00:00:00,2.9401197604790417,2.6879565715789795,-8.57662984649906,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Nova_Belem,2022-12-31T00:00:00,0.96,1.0675952434539795,11.207837859789548,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Novo_Cruzeiro,2022-12-31T00:00:00,1.180722891566265,1.1963956356048584,1.3273854644931071,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Novorizonte,2022-12-31T00:00:00,1.461538461538461,1.2726495265960691,-12.923979759216278,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Ouro_Verde_de_Minas,2022-12-31T00:00:00,2.0,1.3453083038330078,-32.734584808349595,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Padre_Paraiso,2022-12-31T00:00:00,0.6000000000000001,0.6755878925323486,12.597982088724754,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Paracatu,2022-12-31T00:00:00,2.4,2.660806179046631,10.866924126942957,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Pedra_Azul,2022-12-31T00:00:00,1.5333333333333332,1.170845627784729,-23.640502535778538,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Pirapora,2022-12-31T00:00:00,3.3595505617977524,3.1617746353149414,-5.886975738117116,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Pote,2022-12-31T00:00:00,1.2,0.7485550045967102,-37.62041628360748,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Rio_Pardo_de_Minas,2022-12-31T00:00:00,2.515254237288135,3.366272687911988,33.834291500544,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Santa_Barbara_do_Monte_Verde,2022-12-31T00:00:00,1.3,1.039676308631897,-20.024899336007927,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Santa_Maria_do_Suacui,2022-12-31T00:00:00,1.0,0.8241094946861267,-17.58905053138733,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Santo_Antonio_do_Retiro,2022-12-31T00:00:00,1.2,1.6145415306091309,34.54512755076091,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Sao_Goncalo_do_Rio_Preto,2022-12-31T00:00:00,3.0,2.958613395690918,-1.379553476969401,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Sao_Joao_do_Manteninha,2022-12-31T00:00:00,1.2,0.816348671913147,-31.97094400723775,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Sao_Joao_do_Paraiso,2022-12-31T00:00:00,3.09016393442623,3.284644603729248,6.293538900522062,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Sao_Sebastiao_do_Maranhao,2022-12-31T00:00:00,1.141666666666667,0.9090445041656494,-20.37566386870227,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Senador_Modestino_Goncalves,2022-12-31T00:00:00,2.7000000000000006,3.1356356143951416,16.134652385005218,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Setubinha,2022-12-31T00:00:00,1.079761904761905,1.015851974487305,-5.918891006688444,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Taiobeiras,2022-12-31T00:00:00,3.14031971580817,3.58679461479187,14.217498197275072,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Teofilo_Otoni,2022-12-31T00:00:00,0.8181818181818183,0.9437440633773804,15.346496635013136,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Turmalina,2022-12-31T00:00:00,1.205163043478261,1.773332595825195,47.14462125449195,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Urucuia,2022-12-31T00:00:00,1.9193245778611627,2.2300333976745605,16.18844584169512,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Vargem_Grande_do_Rio_Pardo,2022-12-31T00:00:00,1.8000000000000005,2.059903144836426,14.43906360202364,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_4_2022,Varzea_da_Palma,2022-12-31T00:00:00,2.5200000000000005,2.830229043960572,12.310676347641698,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2022_(2022)
    Modelo LSTM treinado com dados de 2012 a 2020, validado com os dados de 2021 e testado com os dados de 2022.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:40:36
V42_cluster_0_2023,Aguanil,2022-12-31T00:00:00,1.56,1.832957029342652,17.497245470682778,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Alfenas,2022-12-31T00:00:00,0.9243951612903224,1.7750972509384155,92.02796869475554,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Alto_Caparao,2022-12-31T00:00:00,1.5,1.2451266050338743,-16.991559664408364,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Alto_Jequitiba,2022-12-31T00:00:00,0.9,1.1842799186706543,31.586657630072697,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Amparo_do_Serra,2022-12-31T00:00:00,1.8000000000000005,1.2385737895965576,-31.190345022413474,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Araponga,2022-12-31T00:00:00,1.260041407867495,1.189919114112854,-5.565078521769904,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Astolfo_Dutra,2022-12-31T00:00:00,0.625,1.2752978801727295,104.04766082763672,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Bambui,2022-12-31T00:00:00,1.5,1.4387526512145996,-4.083156585693359,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Boa_Esperanca,2022-12-31T00:00:00,0.9726256983240223,1.8484561443328853,90.04804700493196,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Bom_Jesus_do_Galho,2022-12-31T00:00:00,0.7801587301587303,1.112982153892517,42.66098818968172,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Bom_Sucesso,2022-12-31T00:00:00,1.440136054421769,1.5787930488586426,9.62804826746359,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Caete,2022-12-31T00:00:00,1.5333333333333332,1.2362310886383057,-19.37623334967571,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Caiana,2022-12-31T00:00:00,0.9,1.0572786331176758,17.47540367974175,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Cajuri,2022-12-31T00:00:00,1.050602409638554,1.2387439012527466,17.907963077956406,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Camacho,2022-12-31T00:00:00,1.380176211453745,1.5623013973236084,13.195792273367053,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Campo_do_Meio,2022-12-31T00:00:00,1.580968858131488,1.660444736480713,5.027036297423066,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Campos_Altos,2022-12-31T00:00:00,1.680042238648363,1.6136397123336792,-3.952431956002883,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Campos_Gerais,2022-12-31T00:00:00,1.535771358328211,1.818903446197509,18.43582290644532,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Cana_Verde,2022-12-31T00:00:00,1.02,1.3523588180541992,32.5841978484509,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Canaa,2022-12-31T00:00:00,1.320338983050847,1.2776049375534058,-3.236596513926869,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Candeias,2022-12-31T00:00:00,1.020017406440383,1.496951937675476,46.75748945299674,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Caparao,2022-12-31T00:00:00,1.32,1.2156729698181152,-7.903562892567032,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Capitolio,2022-12-31T00:00:00,1.08,1.3900361061096191,28.70704686200176,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Caputira,2022-12-31T00:00:00,1.440044247787611,1.0478743314743042,-27.233185154956924,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Carangola,2022-12-31T00:00:00,1.02,1.140994429588318,11.862198979246848,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Caratinga,2022-12-31T00:00:00,1.620022123893805,1.3954297304153442,-13.863538661968496,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Carmo_da_Mata,2022-12-31T00:00:00,1.56,1.2743009328842163,-18.31404276383229,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Carmo_do_Rio_Claro,2022-12-31T00:00:00,1.619555143651529,1.9040617942810056,17.566962862901608,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Carmopolis_de_Minas,2022-12-31T00:00:00,1.8000000000000005,1.7496769428253174,-2.7957253985934933,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Cassia,2022-12-31T00:00:00,1.413626373626374,1.5515178442001345,9.754449488659958,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Catas_Altas_da_Noruega,2022-12-31T00:00:00,1.0,0.7970965504646301,-20.290344953536987,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Chale,2022-12-31T00:00:00,1.44,1.156864881515503,-19.662161005867848,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Claudio,2022-12-31T00:00:00,1.140522875816993,1.6410425901412964,43.88510962270399,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Conceicao_da_Barra_de_Minas,2022-12-31T00:00:00,1.370833333333333,1.765493392944336,28.789791582565584,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Conceicao_de_Ipanema,2022-12-31T00:00:00,1.5,1.326919436454773,-11.538704236348469,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Coqueiral,2022-12-31T00:00:00,1.2,1.6163456439971924,34.69547033309937,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Corrego_Danta,2022-12-31T00:00:00,1.5,1.405924916267395,-6.271672248840332,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Cristais,2022-12-31T00:00:00,1.059063136456212,1.7624666690826416,66.4175258691494,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Desterro_de_Entre_Rios,2022-12-31T00:00:00,1.8000000000000005,1.88938307762146,4.965726534525538,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Divinesia,2022-12-31T00:00:00,2.2230769230769227,2.008199453353882,-9.665768534254436,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Divino,2022-12-31T00:00:00,1.08,1.1193594932556152,3.644397523668071,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Doresopolis,2022-12-31T00:00:00,1.32,1.3567144870758057,2.781400536045879,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Durande,2022-12-31T00:00:00,1.5,1.4082098007202148,-6.119346618652344,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Eloi_Mendes,2022-12-31T00:00:00,1.090201870999508,1.408731460571289,29.21748696686329,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Entre_Folhas,2022-12-31T00:00:00,1.2,0.9175695180892944,-23.5358734925588,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Entre_Rios_de_Minas,2022-12-31T00:00:00,1.8000000000000005,1.9200773239135744,6.670962439642997,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ervalia,2022-12-31T00:00:00,1.2,1.273622989654541,6.135249137878422,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Espera_Feliz,2022-12-31T00:00:00,1.2,1.0089277029037476,-15.922691424687702,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Faria_Lemos,2022-12-31T00:00:00,1.200305810397553,0.9666146039962769,-19.46930560336748,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ferros,2022-12-31T00:00:00,1.230769230769231,0.7868635058403015,-36.06734015047552,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Fervedouro,2022-12-31T00:00:00,1.4399108138238568,1.1471669673919678,-20.330692958300133,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Guape,2022-12-31T00:00:00,1.262758620689655,1.6769870519638062,32.80345305011028,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ibituruna,2022-12-31T00:00:00,1.2,1.6522237062454224,37.6853088537852,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ilicinea,2022-12-31T00:00:00,1.080069324090121,1.92636513710022,78.35569385539588,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Itapecerica,2022-12-31T00:00:00,1.040506329113924,1.3529754877090454,30.030490911209963,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ituiutaba,2022-12-31T00:00:00,1.75,2.748090982437134,57.03377042497907,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Itumirim,2022-12-31T00:00:00,1.249307479224377,1.4684834480285645,17.543797059492604,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Lajinha,2022-12-31T00:00:00,1.419975786924939,1.2043007612228394,-15.188641080222895,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Lavras,2022-12-31T00:00:00,1.260078277886497,1.5697953701019287,24.579194614394414,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Luisburgo,2022-12-31T00:00:00,1.5,1.4221538305282593,-5.189744631449381,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Machado,2022-12-31T00:00:00,0.964047619047619,1.5514856576919556,60.93454587073879,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Manhuacu,2022-12-31T00:00:00,1.44,1.0430946350097656,-27.56287256876628,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Manhumirim,2022-12-31T00:00:00,1.3799999999999997,1.2472373247146606,-9.620483716328918,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Martins_Soares,2022-12-31T00:00:00,1.3799999999999997,1.3884713649749756,0.6138670271721679,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Matipo,2022-12-31T00:00:00,1.26,1.077678680419922,-14.4699459984189,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Medeiros,2022-12-31T00:00:00,1.5,1.5694196224212646,4.627974828084311,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Miradouro,2022-12-31T00:00:00,0.96,1.2445824146270752,29.644001523653685,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Mirai,2022-12-31T00:00:00,1.2,1.1093677282333374,-7.552689313888547,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Moeda,2022-12-31T00:00:00,1.25,1.0518289804458618,-15.853681564331056,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Muriae,2022-12-31T00:00:00,0.9897142857142858,1.124732494354248,13.642140018471938,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Mutum,2022-12-31T00:00:00,1.379940564635958,1.4044392108917236,1.7753406837655132,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Nazareno,2022-12-31T00:00:00,1.319811320754717,1.6704387664794922,26.566482663921487,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Nepomuceno,2022-12-31T00:00:00,0.8699884125144843,1.5706390142440796,80.53562457280778,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Oliveira,2022-12-31T00:00:00,1.535560504825538,1.6238586902618408,5.7502250910220365,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Orizania,2022-12-31T00:00:00,1.5,1.3252990245819092,-11.646731694539389,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Paraguacu,2022-12-31T00:00:00,1.043954802259887,1.4068633317947388,34.76285838709209,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Passa_Tempo,2022-12-31T00:00:00,1.507692307692308,1.4963406324386597,-0.752917236211376,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Perdoes,2022-12-31T00:00:00,1.259859154929577,1.6686218976974487,32.44511429459911,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Pimenta,2022-12-31T00:00:00,1.3502325581395351,1.5372037887573242,13.84733537127959,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Piranga,2022-12-31T00:00:00,1.8000000000000005,1.8522818088531487,2.904544936286064,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Piumhi,2022-12-31T00:00:00,1.2,1.4638874530792236,21.99062108993531,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ponte_Nova,2022-12-31T00:00:00,1.25,1.1736061573028564,-6.111507415771484,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Porto_Firme,2022-12-31T00:00:00,1.32,1.1814807653427124,-10.493881413430884,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Raul_Soares,2022-12-31T00:00:00,0.8399548532731377,1.0680142641067505,27.15138914251289,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Reduto,2022-12-31T00:00:00,1.260059171597633,1.2374212741851809,-1.796574154827166,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ribeirao_Vermelho,2022-12-31T00:00:00,1.32,1.4908825159072876,12.94564514449148,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ritapolis,2022-12-31T00:00:00,1.8000000000000005,1.6936814785003662,-5.906584527757447,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Rosario_da_Limeira,2022-12-31T00:00:00,1.02,0.9985639452934264,-2.1015739908405395,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Santa_Barbara_do_Leste,2022-12-31T00:00:00,1.05015873015873,1.1848160028457642,12.82256375345095,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Santa_Margarida,2022-12-31T00:00:00,1.3799999999999997,1.084555983543396,-21.408986699753896,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Santa_Rita_de_Minas,2022-12-31T00:00:00,1.019909502262443,1.2644264698028564,23.974378804982884,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Santa_Rita_do_Itueto,2022-12-31T00:00:00,1.6399999999999997,1.5303171873092651,-6.687976383581376,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Santana_da_Vargem,2022-12-31T00:00:00,0.9900921658986176,1.718285322189331,73.5480171817942,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Santana_do_Jacare,2022-12-31T00:00:00,1.5,1.3359359502792358,-10.93760331471761,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Santana_do_Manhuacu,2022-12-31T00:00:00,1.26,1.3832629919052124,9.782777135334316,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Domingos_das_Dores,2022-12-31T00:00:00,1.2,1.5047961473464966,25.39967894554139,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Francisco_de_Paula,2022-12-31T00:00:00,1.62,1.510297179222107,-6.771779060363775,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Francisco_do_Gloria,2022-12-31T00:00:00,0.9601476014760146,0.9302570819854736,-3.113117133718913,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Joao_do_Manhuacu,2022-12-31T00:00:00,1.2,1.164067029953003,-2.994414170583086,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Jose_do_Mantimento,2022-12-31T00:00:00,1.499236641221374,1.335831880569458,-10.899197375458757,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Roque_de_Minas,2022-12-31T00:00:00,1.32,1.3524696826934814,2.4598244464758623,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Tiago,2022-12-31T00:00:00,1.3210526315789468,1.8015419244766235,36.371699462373954,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Tomas_de_Aquino,2022-12-31T00:00:00,1.2,1.4901543855667114,24.179532130559288,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Senador_Firmino,2022-12-31T00:00:00,1.565217391304348,1.4625611305236816,-6.558594438764788,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Senhora_de_Oliveira,2022-12-31T00:00:00,1.5,1.5105878114700315,0.705854098002116,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sericita,2022-12-31T00:00:00,1.3799999999999997,1.0342295169830322,-25.055832102678803,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Simonesia,2022-12-31T00:00:00,1.8000000000000005,1.13096284866333,-37.168730629815,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Tapirai,2022-12-31T00:00:00,1.324520819563781,1.6050132513046265,21.17689866386722,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Tombos,2022-12-31T00:00:00,1.2,0.9784045219421388,-18.46628983815511,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Tres_Pontas,2022-12-31T00:00:00,0.975023651844844,1.6919169425964355,73.52573339068815,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ubaporanga,2022-12-31T00:00:00,1.2,1.615333080291748,34.61109002431234,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Vargem_Bonita,2022-12-31T00:00:00,1.235217391304348,1.5535123348236084,25.768334040630016,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Vermelho_Novo,2022-12-31T00:00:00,0.9598214285714286,1.0707464218139648,11.556836505268892,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Vicosa,2022-12-31T00:00:00,1.079768786127168,1.0555124282836914,-2.246439992998643,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Vieiras,2022-12-31T00:00:00,1.32,1.0403125286102295,-21.188444802255347,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Aguanil,2023-12-31T00:00:00,1.717774762550882,1.7105388641357422,-0.4212367402810406,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Alfenas,2023-12-31T00:00:00,1.698430922311519,1.3991020917892456,-17.623844843503843,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Alto_Caparao,2023-12-31T00:00:00,1.32,1.2744512557983398,-3.450662439519713,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Alto_Jequitiba,2023-12-31T00:00:00,1.08,1.086101770401001,0.5649787408334171,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Amparo_do_Serra,2023-12-31T00:00:00,1.319444444444444,1.2923834323883057,-2.050939861096799,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Araponga,2023-12-31T00:00:00,1.5,1.164495825767517,-22.36694494883219,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Astolfo_Dutra,2023-12-31T00:00:00,0.625,1.0171916484832764,62.75066375732422,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Bambui,2023-12-31T00:00:00,1.5,1.4962306022644043,-0.2512931823730469,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Boa_Esperanca,2023-12-31T00:00:00,1.08,1.393815040588379,29.05694820262767,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Bom_Jesus_do_Galho,2023-12-31T00:00:00,1.2003192338387871,0.8619036674499512,-28.19379685406991,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Bom_Sucesso,2023-12-31T00:00:00,1.5,1.414259672164917,-5.716021855672201,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Caete,2023-12-31T00:00:00,1.166666666666667,1.2968552112579346,11.159018107822934,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Caiana,2023-12-31T00:00:00,1.08,0.9844078421592712,-8.851125725993409,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Cajuri,2023-12-31T00:00:00,0.8200000000000001,1.1739624738693235,43.16615534991752,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Camacho,2023-12-31T00:00:00,1.680155642023346,1.5116350650787354,-10.030057497629684,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Campo_do_Meio,2023-12-31T00:00:00,0.8771626297577856,1.5013412237167358,71.15882195429451,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Campos_Altos,2023-12-31T00:00:00,1.380040526849037,1.4279069900512695,3.468482430115522,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Campos_Gerais,2023-12-31T00:00:00,1.560242401107029,1.5674132108688354,0.4595958779686115,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Cana_Verde,2023-12-31T00:00:00,1.3196428571428571,1.1753631830215454,-10.93323646927396,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Canaa,2023-12-31T00:00:00,1.560135135135135,1.299051284790039,-16.734694608520652,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Candeias,2023-12-31T00:00:00,1.080032206119163,1.2338919639587402,14.245849055968012,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Caparao,2023-12-31T00:00:00,1.32,1.1666715145111084,-11.615794355219066,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Capitolio,2023-12-31T00:00:00,1.32,1.2952624559402466,-1.8740563681631424,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Caputira,2023-12-31T00:00:00,1.380065005417118,1.0811326503753662,-21.66074452061053,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Carangola,2023-12-31T00:00:00,1.140118343195266,1.0765639543533323,-5.574367715531841,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Caratinga,2023-12-31T00:00:00,1.3799999999999997,1.178911566734314,-14.571625598962736,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Carmo_da_Mata,2023-12-31T00:00:00,1.2,1.249077081680298,4.089756806691492,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Carmo_do_Rio_Claro,2023-12-31T00:00:00,1.816856256463288,1.7046221494674685,-6.1773795585951214,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Carmopolis_de_Minas,2023-12-31T00:00:00,1.5,1.7116914987564087,14.11276658376058,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Cassia,2023-12-31T00:00:00,1.6377649325626198,1.5071840286254885,-7.973116369808386,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Catas_Altas_da_Noruega,2023-12-31T00:00:00,1.1,0.7821670770645142,-28.89390208504417,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Chale,2023-12-31T00:00:00,1.3799999999999997,1.122646450996399,-18.64880789881165,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Claudio,2023-12-31T00:00:00,2.398692810457516,1.2989526987075806,-45.84747604843055,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Conceicao_da_Barra_de_Minas,2023-12-31T00:00:00,1.680769230769231,1.5367975234985352,-8.565822400544834,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Conceicao_de_Ipanema,2023-12-31T00:00:00,1.5,1.3109685182571411,-12.60209878285726,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Coqueiral,2023-12-31T00:00:00,1.32,1.4049171209335327,6.433115222237322,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Corrego_Danta,2023-12-31T00:00:00,1.44,1.3481065034866333,-6.38149281342824,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Cristais,2023-12-31T00:00:00,1.5631111111111111,1.3862746953964231,-11.31310592431184,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Desterro_de_Entre_Rios,2023-12-31T00:00:00,1.375,1.767473816871643,28.54355031793768,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Divinesia,2023-12-31T00:00:00,2.1,1.9057714939117432,-9.248976480393187,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Divino,2023-12-31T00:00:00,1.02,1.0789134502410889,5.7758284550087104,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Doresopolis,2023-12-31T00:00:00,1.501960784313725,1.3961529731750488,-7.044645389128575,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Durande,2023-12-31T00:00:00,1.3799999999999997,1.336075782775879,-3.182914291602955,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Eloi_Mendes,2023-12-31T00:00:00,1.577412806790921,1.3449347019195557,-14.73793694779982,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Entre_Folhas,2023-12-31T00:00:00,1.3210526315789468,0.9096102714538574,-31.145039212656183,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Entre_Rios_de_Minas,2023-12-31T00:00:00,1.56,1.8797178268432613,20.494732489952668,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ervalia,2023-12-31T00:00:00,1.380044843049327,1.1866352558135986,-14.0147320726458,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Espera_Feliz,2023-12-31T00:00:00,1.08,0.952093780040741,-11.843168514746212,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Faria_Lemos,2023-12-31T00:00:00,1.380152671755725,0.9299858212471008,-32.61717777468461,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ferros,2023-12-31T00:00:00,1.17948717948718,0.8115218281745911,-31.1970623938934,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Fervedouro,2023-12-31T00:00:00,1.2,1.131692886352539,-5.692259470621741,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Guape,2023-12-31T00:00:00,1.560674157303371,1.4685592651367188,-5.902250110030266,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ibituruna,2023-12-31T00:00:00,1.8000000000000005,1.433421611785889,-20.365466011895087,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ilicinea,2023-12-31T00:00:00,1.560032362459547,1.50627338886261,-3.446016562898776,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Itapecerica,2023-12-31T00:00:00,0.9037974683544304,1.239035964012146,37.09221450554554,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ituiutaba,2023-12-31T00:00:00,1.0,2.2518022060394287,125.18022060394289,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Itumirim,2023-12-31T00:00:00,1.532033426183844,1.3012423515319824,-15.0643628727306,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Lajinha,2023-12-31T00:00:00,1.4399536768963517,1.157912015914917,-19.58685654314532,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Lavras,2023-12-31T00:00:00,1.5,1.4011268615722656,-6.591542561848958,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Luisburgo,2023-12-31T00:00:00,1.44,1.4933576583862305,3.705392943488231,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Machado,2023-12-31T00:00:00,1.500560931145702,1.3764630556106567,-8.270099064907319,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Manhuacu,2023-12-31T00:00:00,1.2,1.1430526971817017,-4.745608568191525,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Manhumirim,2023-12-31T00:00:00,1.44,1.3242162466049194,-8.040538430213925,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Martins_Soares,2023-12-31T00:00:00,1.2,1.3796415328979492,14.970127741495771,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Matipo,2023-12-31T00:00:00,1.25990675990676,1.1096432209014893,-11.926560265173194,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Medeiros,2023-12-31T00:00:00,1.86,1.4697741270065308,-20.97988564481017,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Miradouro,2023-12-31T00:00:00,1.140084388185654,1.1102997064590454,-2.612497990083724,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Mirai,2023-12-31T00:00:00,1.08,1.0655968189239502,-1.3336278774120254,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Moeda,2023-12-31T00:00:00,1.0,1.2147858142852783,21.478581428527832,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Muriae,2023-12-31T00:00:00,1.057297297297297,1.030756592750549,-2.5102404607097704,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Mutum,2023-12-31T00:00:00,1.019985196150999,1.3883217573165894,36.1119516788616,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Nazareno,2023-12-31T00:00:00,1.5599056603773582,1.4823238849639893,-4.973491499133422,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Nepomuceno,2023-12-31T00:00:00,1.650045578851413,1.2447209358215332,-24.564451323340045,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Oliveira,2023-12-31T00:00:00,1.8240227434257288,1.4998290538787842,-17.773555221031444,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Orizania,2023-12-31T00:00:00,1.2,1.3279447555541992,10.66206296284994,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Paraguacu,2023-12-31T00:00:00,1.298291457286432,1.2453670501708984,-4.076465790366623,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Passa_Tempo,2023-12-31T00:00:00,1.625,1.5188350677490234,-6.533226600060097,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Perdoes,2023-12-31T00:00:00,1.6799401197604789,1.480138659477234,-11.893367979790382,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Pimenta,2023-12-31T00:00:00,1.504186046511628,1.4480221271514893,-3.733841268531172,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Piranga,2023-12-31T00:00:00,2.1,1.934919595718384,-7.860971632457919,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Piumhi,2023-12-31T00:00:00,1.7399633363886338,1.346534013748169,-22.611357056352908,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ponte_Nova,2023-12-31T00:00:00,1.333333333333333,1.179306983947754,-11.551976203918436,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Porto_Firme,2023-12-31T00:00:00,1.200764818355641,1.1873265504837036,-1.119142372137458,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Raul_Soares,2023-12-31T00:00:00,1.5,0.9469914436340332,-36.86723709106445,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Reduto,2023-12-31T00:00:00,0.9,1.2704474925994873,41.16083251105414,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ribeirao_Vermelho,2023-12-31T00:00:00,1.5,1.3562867641448977,-9.580882390340168,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ritapolis,2023-12-31T00:00:00,2.102222222222222,1.6746565103530884,-20.338749507516937,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Rosario_da_Limeira,2023-12-31T00:00:00,1.2,0.9238265156745912,-23.01445702711741,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Santa_Barbara_do_Leste,2023-12-31T00:00:00,1.08,1.0647826194763184,-1.4090167151557138,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Santa_Margarida,2023-12-31T00:00:00,1.32,1.1868218183517456,-10.089256185473822,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Santa_Rita_de_Minas,2023-12-31T00:00:00,1.019930675909879,1.134033203125,11.18728261735341,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Santa_Rita_do_Itueto,2023-12-31T00:00:00,1.2,1.3833990097045898,15.283250808715826,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Santana_da_Vargem,2023-12-31T00:00:00,1.5593593593593589,1.3650240898132324,-12.462507014801668,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Santana_do_Jacare,2023-12-31T00:00:00,0.9011235955056182,1.2504651546478271,38.767330129247625,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Santana_do_Manhuacu,2023-12-31T00:00:00,1.32,1.3171393871307373,-0.2167130961562695,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Domingos_das_Dores,2023-12-31T00:00:00,1.296078431372549,1.289732813835144,-0.4896013531129451,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Francisco_de_Paula,2023-12-31T00:00:00,1.56,1.4890789985656738,-4.5462180406619375,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Francisco_do_Gloria,2023-12-31T00:00:00,1.08,0.8295098543167114,-23.19353200771191,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Joao_do_Manhuacu,2023-12-31T00:00:00,1.2,1.2419767379760742,3.4980614980061886,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Jose_do_Mantimento,2023-12-31T00:00:00,1.5,1.3611457347869873,-9.256951014200848,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Roque_de_Minas,2023-12-31T00:00:00,1.8000000000000005,1.3817201852798462,-23.237767484453,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Tiago,2023-12-31T00:00:00,1.560526315789474,1.6487116813659668,5.65100150405856,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sao_Tomas_de_Aquino,2023-12-31T00:00:00,1.319968346082828,1.3566124439239502,2.7761345906413726,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Senador_Firmino,2023-12-31T00:00:00,1.436170212765957,1.4175474643707275,-1.2966950734456089,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Senhora_de_Oliveira,2023-12-31T00:00:00,1.2,1.6352765560150146,36.27304633458456,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Sericita,2023-12-31T00:00:00,1.26,1.0570803880691528,-16.10473110562279,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Simonesia,2023-12-31T00:00:00,1.5,1.208932638168335,-19.40449078877767,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Tapirai,2023-12-31T00:00:00,1.5579665220086798,1.4824626445770264,-4.84630936320163,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Tombos,2023-12-31T00:00:00,1.32,0.9688655734062196,-26.60109292377125,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Tres_Pontas,2023-12-31T00:00:00,1.5900000000000003,1.335451602935791,-16.009333148692406,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Ubaporanga,2023-12-31T00:00:00,1.5,1.1723580360412598,-21.84279759724935,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Vargem_Bonita,2023-12-31T00:00:00,1.487234042553192,1.6027076244354248,7.764318095085749,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Vermelho_Novo,2023-12-31T00:00:00,1.5,1.0233596563339231,-31.776022911071777,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Vicosa,2023-12-31T00:00:00,1.5004329004329,1.0291756391525269,-31.40808636923434,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_0_2023,Vieiras,2023-12-31T00:00:00,1.5,0.9752538204193116,-34.98307863871257,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:11
V42_cluster_1_2023,Almenara,2022-12-31T00:00:00,0.78,0.6934486627578735,-11.096325287452114,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Alpinopolis,2022-12-31T00:00:00,1.4420494699646638,1.8502050638198853,28.303855197507406,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Alterosa,2022-12-31T00:00:00,1.080104712041885,1.335073947906494,23.605973848831965,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Andradas,2022-12-31T00:00:00,1.26,1.1609759330749512,-7.859052930559432,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Araguari,2022-12-31T00:00:00,1.919968366943456,2.036903142929077,6.090453259486695,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Arapua,2022-12-31T00:00:00,1.14,1.2303887605667114,7.928838646202766,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Araxa,2022-12-31T00:00:00,1.077872465471643,1.528897762298584,41.844031763960736,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Arceburgo,2022-12-31T00:00:00,0.9244897959183672,1.4101507663726809,52.53286435377784,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Areado,2022-12-31T00:00:00,0.9076923076923076,1.4988183975219729,65.12406074394616,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Bom_Jesus_da_Penha,2022-12-31T00:00:00,1.6298200514138823,1.65742826461792,1.6939424189859034,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Botelhos,2022-12-31T00:00:00,0.9,1.0673792362213137,18.59769291347927,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Botumirim,2022-12-31T00:00:00,0.7241379310344828,0.7617359757423401,5.192110935846964,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Cabo_Verde,2022-12-31T00:00:00,1.35,1.2686361074447632,-6.026955004091622,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Campestre,2022-12-31T00:00:00,1.020018115942029,1.1457853317260742,12.329900206516824,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Capetinga,2022-12-31T00:00:00,1.407056229327453,1.6650453805923462,18.335383184239014,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Carmo_do_Paranaiba,2022-12-31T00:00:00,1.392969472710453,1.524071216583252,9.411673869471098,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Cascalho_Rico,2022-12-31T00:00:00,2.580434782608696,2.2990734577178955,-10.903640223232369,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Claraval,2022-12-31T00:00:00,1.289915966386555,1.150076150894165,-10.84100198279765,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Conceicao_da_Aparecida,2022-12-31T00:00:00,1.53,1.738051414489746,13.598131665996474,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Coromandel,2022-12-31T00:00:00,1.260025542784164,1.4038255214691162,11.412465366948869,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Corrego_Fundo,2022-12-31T00:00:00,1.079051383399209,0.8932594656944275,-17.218078820259983,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Cruzeiro_da_Fortaleza,2022-12-31T00:00:00,1.5598526703499078,1.64634370803833,5.544820952162135,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Delfinopolis,2022-12-31T00:00:00,1.3799999999999997,1.0853255987167358,-21.353217484294483,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Divisa_Nova,2022-12-31T00:00:00,1.260204081632653,1.237810492515564,-1.776981160708277,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Dom_Cavati,2022-12-31T00:00:00,0.6000000000000001,0.5922452807426453,-1.2924532095591377,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Dores_do_Turvo,2022-12-31T00:00:00,1.2,0.832280158996582,-30.643320083618164,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Estrela_do_Indaia,2022-12-31T00:00:00,1.86,1.8151262998580933,-2.4125645237584203,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Estrela_do_Sul,2022-12-31T00:00:00,1.460045146726862,1.657411813735962,13.517846859157586,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Guaranesia,2022-12-31T00:00:00,1.086968085106383,1.0478090047836304,-3.6025970641925578,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Guarani,2022-12-31T00:00:00,1.2,0.8792369365692139,-26.73025528589884,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Guarda-Mor,2022-12-31T00:00:00,1.858757062146893,1.847460985183716,-0.6077220737028428,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Guaxupe,2022-12-31T00:00:00,1.14,1.0851099491119385,-4.814916744566792,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Guimarania,2022-12-31T00:00:00,1.2220588235294123,1.4169232845306396,15.945587663156983,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Ibia,2022-12-31T00:00:00,1.14,1.3953592777252195,22.399936642563144,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Ibiraci,2022-12-31T00:00:00,1.377007874015748,1.081477403640747,-21.46178507412233,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Indianopolis,2022-12-31T00:00:00,1.8000000000000005,2.2028515338897705,22.380640771653898,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Itamogi,2022-12-31T00:00:00,1.32,1.3450863361358645,1.900480010292742,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Itanhomi,2022-12-31T00:00:00,1.081818181818182,0.9009307622909546,-16.72068583865127,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Jacui,2022-12-31T00:00:00,1.08,1.236345887184143,14.476471035568794,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Jacutinga,2022-12-31T00:00:00,1.289920424403183,1.3436095714569092,4.162206135976716,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Juruaia,2022-12-31T00:00:00,1.5,1.551169753074646,3.4113168716430664,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Lagoa_Dourada,2022-12-31T00:00:00,1.5,1.3146228790283203,-12.358474731445312,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Lagoa_Formosa,2022-12-31T00:00:00,2.088607594936709,1.618443250656128,-22.51089890797933,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Lagoa_Grande,2022-12-31T00:00:00,2.4,0.6945491433143616,-71.06045236190161,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Matutina,2022-12-31T00:00:00,1.8097165991902835,1.6660431623458862,-7.939001991178092,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Monte_Belo,2022-12-31T00:00:00,0.96,1.3752896785736084,43.25934151808424,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Monte_Carmelo,2022-12-31T00:00:00,1.7458874458874465,1.9802683591842647,13.42474360812426,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Monte_Santo_de_Minas,2022-12-31T00:00:00,1.4699677072120565,1.22283136844635,-16.812365166471956,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Muzambinho,2022-12-31T00:00:00,1.43993993993994,1.1601389646530151,-19.43143373734014,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Nova_Resende,2022-12-31T00:00:00,1.5,1.548854947090149,3.256996472676595,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Passos,2022-12-31T00:00:00,1.290038314176245,1.2468242645263672,-3.349826836536418,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Patis,2022-12-31T00:00:00,2.044444444444444,1.7647079229354858,-13.682764639025123,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Patos_de_Minas,2022-12-31T00:00:00,1.91614730878187,1.8782111406326287,-1.9798148073154784,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Patrocinio,2022-12-31T00:00:00,0.9438953724513282,1.7023940086364746,80.35833825684513,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Pedrinopolis,2022-12-31T00:00:00,2.02051282051282,2.089552402496338,3.416933626087816,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Perdizes,2022-12-31T00:00:00,1.501272727272727,1.911198973655701,27.30524833603433,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Piracema,2022-12-31T00:00:00,1.0,1.1155694723129272,11.556947231292725,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Ponto_dos_Volantes,2022-12-31T00:00:00,0.6000000000000001,0.5138702392578125,-14.35496012369793,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Pratapolis,2022-12-31T00:00:00,1.2,1.1760271787643433,-1.9977351029713912,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Pratinha,2022-12-31T00:00:00,1.720111731843575,1.561208963394165,-9.23793294980331,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Presidente_Kubitschek,2022-12-31T00:00:00,0.896551724137931,0.6427883505821228,-28.30437628122476,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Presidente_Olegario,2022-12-31T00:00:00,1.8899598393574304,1.669792413711548,-11.64931767654583,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Quartel_Geral,2022-12-31T00:00:00,3.0,2.8767752647399902,-4.107491175333658,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Rio_Paranaiba,2022-12-31T00:00:00,1.230031446540881,1.20730459690094,-1.847664114917874,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Rio_Vermelho,2022-12-31T00:00:00,0.9090909090909092,0.7831950783729553,-13.848541378974913,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Romaria,2022-12-31T00:00:00,1.910526315789474,2.3521549701690674,23.115549402788627,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Sacramento,2022-12-31T00:00:00,1.5575892857142857,1.3512787818908691,-13.245500962007824,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Santa_Rosa_da_Serra,2022-12-31T00:00:00,2.22,1.6651086807250977,-24.99510447184244,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Santo_Antonio_do_Amparo,2022-12-31T00:00:00,1.26,1.3315060138702393,5.675080465892004,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Sao_Goncalo_do_Abaete,2022-12-31T00:00:00,1.5,1.6437629461288452,9.58419640858968,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Sao_Gotardo,2022-12-31T00:00:00,1.50025974025974,1.3335273265838623,-11.113569817384509,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Sao_Joao_Batista_do_Gloria,2022-12-31T00:00:00,1.232209737827715,1.243938684463501,0.9518628424786768,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Sao_Pedro_da_Uniao,2022-12-31T00:00:00,1.4700000000000002,1.511439323425293,2.819001593557332,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Sao_Sebastiao_do_Paraiso,2022-12-31T00:00:00,1.3172147001934242,1.2908947467803955,-1.9981521166719305,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Serra_do_Salitre,2022-12-31T00:00:00,1.5168750000000002,1.3559725284576416,-10.60749709385141,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Tapira,2022-12-31T00:00:00,1.8000000000000005,0.9425156712532043,-47.63801826371088,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Tiros,2022-12-31T00:00:00,1.6801801801801797,1.472769021987915,-12.34457831600074,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Tupaciguara,2022-12-31T00:00:00,2.4,2.121734857559204,-11.59438093503316,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Uberaba,2022-12-31T00:00:00,2.102941176470588,2.203319549560547,4.773237321760289,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Uberlandia,2022-12-31T00:00:00,1.681818181818182,1.801045060157776,7.089165739110995,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Unai,2022-12-31T00:00:00,2.52010582010582,2.710777759552002,7.5660290899282705,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Varginha,2022-12-31T00:00:00,1.020039292730845,1.198825478553772,17.527382238803888,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Varjao_de_Minas,2022-12-31T00:00:00,2.383928571428572,2.344896078109741,-1.637318071801144,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Virginopolis,2022-12-31T00:00:00,1.314606741573034,1.1351908445358276,-13.647875928471256,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Almenara,2023-12-31T00:00:00,0.78,0.6431385278701782,-17.546342580746384,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Alpinopolis,2023-12-31T00:00:00,1.8000000000000005,1.2338100671768188,-31.45499626795452,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Alterosa,2023-12-31T00:00:00,1.2603960396039595,0.9950039982795716,-21.05624208465296,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Andradas,2023-12-31T00:00:00,1.22,0.9583221077919006,-21.44900755804093,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Araguari,2023-12-31T00:00:00,3.0,2.024152994155884,-32.52823352813721,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Arapua,2023-12-31T00:00:00,0.9,1.298709511756897,44.30105686187744,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Araxa,2023-12-31T00:00:00,1.638401296246287,1.1209594011306765,-31.58212193197802,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Arceburgo,2023-12-31T00:00:00,1.8482558139534884,0.8619847893714905,-53.3622573853739,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Areado,2023-12-31T00:00:00,1.590034364261168,1.01395583152771,-36.23057121794604,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Bom_Jesus_da_Penha,2023-12-31T00:00:00,1.2,1.3624441623687744,13.537013530731208,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Botelhos,2023-12-31T00:00:00,1.080052666227781,0.9571873545646667,-11.375863002332718,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Botumirim,2023-12-31T00:00:00,0.59375,0.6107839941978455,2.8688832333213408,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Cabo_Verde,2023-12-31T00:00:00,1.319976635514019,1.1769616603851318,-10.834659590258198,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Campestre,2023-12-31T00:00:00,1.319964428634949,1.0453463792800903,-20.804958330495086,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Capetinga,2023-12-31T00:00:00,1.4484046164290565,1.420677900314331,-1.9142935475505152,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Carmo_do_Paranaiba,2023-12-31T00:00:00,2.369801007771799,1.5856044292449951,-33.091241667761096,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Cascalho_Rico,2023-12-31T00:00:00,2.4,2.3388116359710693,-2.549515167872108,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Claraval,2023-12-31T00:00:00,1.8600214362272245,1.3218741416931152,-28.932316803061177,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Conceicao_da_Aparecida,2023-12-31T00:00:00,1.679948420373952,1.4486258029937744,-13.769626172730105,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Coromandel,2023-12-31T00:00:00,2.0926575541308825,1.2143183946609497,-41.97242677073949,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Corrego_Fundo,2023-12-31T00:00:00,1.320158102766798,1.0233839750289917,-22.480195903492515,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Cruzeiro_da_Fortaleza,2023-12-31T00:00:00,1.5,1.6432420015335083,9.549466768900556,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Delfinopolis,2023-12-31T00:00:00,1.501818181818182,1.1115710735321045,-25.984976943988208,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Divisa_Nova,2023-12-31T00:00:00,1.680092592592593,1.1852209568023682,-29.455021584648257,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Dom_Cavati,2023-12-31T00:00:00,0.6000000000000001,0.3853999376296997,-35.76667706171672,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Dores_do_Turvo,2023-12-31T00:00:00,1.5,0.7312007546424866,-51.25328302383423,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Estrela_do_Indaia,2023-12-31T00:00:00,1.8000000000000005,1.565171480178833,-13.046028878953736,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Estrela_do_Sul,2023-12-31T00:00:00,2.43015873015873,1.3189040422439575,-45.727658614389725,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Guaranesia,2023-12-31T00:00:00,1.1961439588688951,0.8791666030883789,-26.499933676901087,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Guarani,2023-12-31T00:00:00,0.9655172413793104,0.5398436784744263,-44.08761901514871,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Guarda-Mor,2023-12-31T00:00:00,1.7994350282485878,1.676090955734253,-6.854599948206365,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Guaxupe,2023-12-31T00:00:00,1.2,0.8490492701530457,-29.24589415391286,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Guimarania,2023-12-31T00:00:00,1.7704600484261497,1.1813807487487793,-33.272668321492624,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Ibia,2023-12-31T00:00:00,1.668,1.096784234046936,-34.24554951756978,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Ibiraci,2023-12-31T00:00:00,1.804169884169884,1.1321711540222168,-37.24697635427277,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Indianopolis,2023-12-31T00:00:00,2.7000000000000006,1.9277836084365845,-28.60060709494133,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Itamogi,2023-12-31T00:00:00,1.740011254924029,1.1935317516326904,-31.406664856038446,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Itanhomi,2023-12-31T00:00:00,1.078787878787879,0.7926008701324463,-26.52857102704853,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Jacui,2023-12-31T00:00:00,1.32,0.8274706602096558,-37.312828771995775,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Jacutinga,2023-12-31T00:00:00,1.160053262316911,1.167571187019348,0.6480672005888697,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Juruaia,2023-12-31T00:00:00,1.5,1.3805710077285769,-7.96193281809489,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Lagoa_Dourada,2023-12-31T00:00:00,1.8000000000000005,1.1252542734146118,-37.48587369918825,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Lagoa_Formosa,2023-12-31T00:00:00,1.978723404255319,2.107513189315796,6.508731072948833,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Lagoa_Grande,2023-12-31T00:00:00,2.4,0.3459202945232391,-85.58665439486504,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Matutina,2023-12-31T00:00:00,1.502024291497976,1.8116999864578247,20.61722281808157,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Monte_Belo,2023-12-31T00:00:00,1.3799999999999997,0.9296414256095886,-32.63467930365298,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Monte_Carmelo,2023-12-31T00:00:00,2.4900284900284904,1.8341987133026123,-26.33824389368228,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Monte_Santo_de_Minas,2023-12-31T00:00:00,1.620050377833753,1.04386568069458,-35.56585060704205,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Muzambinho,2023-12-31T00:00:00,1.68,1.0389634370803833,-38.156938269024806,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Nova_Resende,2023-12-31T00:00:00,1.56,1.2069718837738037,-22.6300074503972,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Passos,2023-12-31T00:00:00,1.829801324503311,0.7937119007110596,-56.62305573385088,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Patis,2023-12-31T00:00:00,1.2,1.5096933841705322,25.80778201421102,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Patos_de_Minas,2023-12-31T00:00:00,1.782452316076294,1.7988054752349854,0.9174528267151376,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Patrocinio,2023-12-31T00:00:00,2.4370275910039414,0.9933760166168212,-59.23821214483679,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Pedrinopolis,2023-12-31T00:00:00,2.187804878048781,1.9876177310943604,-9.150137151762836,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Perdizes,2023-12-31T00:00:00,2.106451612903226,1.4565672874450684,-30.852088957431672,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Piracema,2023-12-31T00:00:00,1.2,1.010938048362732,-15.755162636439,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Ponto_dos_Volantes,2023-12-31T00:00:00,0.6000000000000001,0.4638767838478088,-22.68720269203187,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Pratapolis,2023-12-31T00:00:00,1.8011363636363642,0.7908717393875122,-56.09040185104035,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Pratinha,2023-12-31T00:00:00,2.13,1.3794134855270386,-35.23880349638316,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Presidente_Kubitschek,2023-12-31T00:00:00,0.8947368421052632,0.5347820520401001,-40.23024124257704,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Presidente_Olegario,2023-12-31T00:00:00,2.160074626865672,1.597794771194458,-26.03057545688121,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Quartel_Geral,2023-12-31T00:00:00,3.0,3.1566309928894043,5.221033096313477,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Rio_Paranaiba,2023-12-31T00:00:00,1.8000000000000005,1.2975212335586548,-27.915487024519187,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Rio_Vermelho,2023-12-31T00:00:00,0.9,0.7314187288284302,-18.73125235239665,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Romaria,2023-12-31T00:00:00,2.131578947368421,2.270817279815674,6.532168682710637,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Sacramento,2023-12-31T00:00:00,1.75,1.2648011445999146,-27.725648880004883,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Santa_Rosa_da_Serra,2023-12-31T00:00:00,1.68,1.6090537309646606,-4.222992204484481,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Santo_Antonio_do_Amparo,2023-12-31T00:00:00,1.8000000000000005,1.1718538999557495,-34.89700555801393,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Sao_Goncalo_do_Abaete,2023-12-31T00:00:00,1.619047619047619,1.367635726928711,-15.528381572050206,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Sao_Gotardo,2023-12-31T00:00:00,1.5,1.258776068687439,-16.081595420837402,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Sao_Joao_Batista_do_Gloria,2023-12-31T00:00:00,1.5232974910394272,0.9899945855140686,-35.00976720978235,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Sao_Pedro_da_Uniao,2023-12-31T00:00:00,1.5,1.2775888442993164,-14.827410380045572,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Sao_Sebastiao_do_Paraiso,2023-12-31T00:00:00,1.440017746228926,1.187168836593628,-17.558735668185403,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Serra_do_Salitre,2023-12-31T00:00:00,1.489636363636364,1.3893415927886963,-6.732835831345938,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Tapira,2023-12-31T00:00:00,1.92,0.8112233281135559,-57.74878499408563,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Tiros,2023-12-31T00:00:00,2.382038834951457,1.501097321510315,-36.98266797816899,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Tupaciguara,2023-12-31T00:00:00,2.622222222222222,1.9563151597976685,-25.394760855173647,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Uberaba,2023-12-31T00:00:00,2.1200980392156863,2.044898509979248,-3.5469835755452954,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Uberlandia,2023-12-31T00:00:00,1.8000000000000005,1.699217438697815,-5.59903118345474,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Unai,2023-12-31T00:00:00,2.520095187731359,2.569763898849488,1.970906153066433,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Varginha,2023-12-31T00:00:00,1.679990280646337,0.9202948808670044,-45.220225886488905,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Varjao_de_Minas,2023-12-31T00:00:00,2.3892857142857142,2.319478988647461,-2.921656678432126,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_1_2023,Virginopolis,2023-12-31T00:00:00,1.3258426966292132,1.3015072345733645,-1.8354712906530115,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:41:45
V42_cluster_2_2023,Abre_Campo,2022-12-31T00:00:00,1.2,1.5587308406829834,29.89423672358196,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Aimores,2022-12-31T00:00:00,2.332558139534884,1.7339069843292236,-25.6650046598638,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Alvarenga,2022-12-31T00:00:00,0.9,0.8885111212730408,-1.27654208077325,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Andrelandia,2022-12-31T00:00:00,1.8000000000000005,1.5124120712280271,-15.977107153998492,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Antonio_Dias,2022-12-31T00:00:00,1.5,1.69327974319458,12.885316212972006,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Antonio_Prado_de_Minas,2022-12-31T00:00:00,1.2,1.0446196794509888,-12.948360045750936,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Bocaiuva,2022-12-31T00:00:00,3.6000000000000005,1.7572635412216189,-51.18712385495504,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Bom_Jesus_do_Amparo,2022-12-31T00:00:00,1.8000000000000005,1.278247594833374,-28.986244731479232,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Campo_Belo,2022-12-31T00:00:00,1.2,1.7185994386672974,43.21661988894145,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Capela_Nova,2022-12-31T00:00:00,2.1016393442622947,1.989066243171692,-5.356442407587187,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Capitao_Eneas,2022-12-31T00:00:00,2.6000000000000005,3.638500690460205,39.94233424846939,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Casa_Grande,2022-12-31T00:00:00,2.107692307692308,1.4463224411010742,-31.37886228352569,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Cataguases,2022-12-31T00:00:00,1.5,2.4097836017608643,60.65224011739095,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Caxambu,2022-12-31T00:00:00,1.4387755102040822,1.7329277992248535,20.444627180167075,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Coimbra,2022-12-31T00:00:00,1.799054373522459,1.6608827114105225,-7.680238248797516,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Conselheiro_Pena,2022-12-31T00:00:00,1.8000000000000005,1.3216487169265747,-26.57507128185697,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Corrego_Novo,2022-12-31T00:00:00,1.377777777777778,1.9111753702163696,38.71434138667197,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Cruzilia,2022-12-31T00:00:00,1.5027322404371577,1.4735369682312012,-1.9428126595236528,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Divisopolis,2022-12-31T00:00:00,1.259523809523809,1.231104016304016,-2.256391900248211,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Esmeraldas,2022-12-31T00:00:00,2.111111111111112,1.7362046241760254,-17.75872832850408,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Espirito_Santo_do_Dourado,2022-12-31T00:00:00,1.319148936170213,1.348084807395935,2.193525721949923,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Eugenopolis,2022-12-31T00:00:00,1.8000000000000005,1.4211766719818115,-21.04574044545493,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Felicio_dos_Santos,2022-12-31T00:00:00,3.306397306397306,4.022303104400635,21.65214073390924,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Formiga,2022-12-31T00:00:00,1.8000000000000005,1.3406777381896973,-25.51790343390572,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Guaraciaba,2022-12-31T00:00:00,1.8000000000000005,2.332019090652466,29.5566161473592,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Guiricema,2022-12-31T00:00:00,2.4,2.205573081970215,-8.101121584574377,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Iapu,2022-12-31T00:00:00,1.5614035087719298,1.740839600563049,11.491974418082936,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Ijaci,2022-12-31T00:00:00,1.173913043478261,1.7501319646835327,49.0853155100787,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Imbe_de_Minas,2022-12-31T00:00:00,1.3799999999999997,1.1978715658187866,-13.197712621827035,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Inhapim,2022-12-31T00:00:00,1.8000000000000005,1.8933758735656736,5.187548531426308,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Irai_de_Minas,2022-12-31T00:00:00,2.386666666666667,2.7033815383911133,13.270176189571762,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Itamarati_de_Minas,2022-12-31T00:00:00,1.261538461538461,1.3732845783233645,8.857923891486234,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Jequeri,2022-12-31T00:00:00,1.8000000000000005,1.3929396867752075,-22.61446184582181,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Lamim,2022-12-31T00:00:00,2.4,1.299877643585205,-45.83843151728312,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Mar_de_Espanha,2022-12-31T00:00:00,2.1,1.4835574626922607,-29.354406538463778,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Mata_Verde,2022-12-31T00:00:00,1.6792452830188678,2.0737287998199463,23.49171504545749,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Monte_Alegre_de_Minas,2022-12-31T00:00:00,2.3296703296703294,2.326433181762696,-0.1389530469786324,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Nova_Era,2022-12-31T00:00:00,1.5,1.4368327856063845,-4.211147626241049,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Nova_Ponte,2022-12-31T00:00:00,2.710714285714286,2.34708023071289,-13.414695046164775,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Paula_Candido,2022-12-31T00:00:00,1.6198529411764713,1.5375516414642334,-5.080788361717815,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Pecanha,2022-12-31T00:00:00,2.287671232876712,1.3359270095825195,-41.60319059908746,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Pedra_Bonita,2022-12-31T00:00:00,1.8000000000000005,1.1041520833969116,-38.65821758906047,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Pedra_Dourada,2022-12-31T00:00:00,1.2,1.039770483970642,-13.352459669113156,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Pedra_do_Anta,2022-12-31T00:00:00,1.438297872340426,1.100513219833374,-23.485027615135262,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Piedade_de_Caratinga,2022-12-31T00:00:00,1.56,1.4656801223754885,-6.046146001571267,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Pocrane,2022-12-31T00:00:00,1.440944881889764,1.1974083185195925,-16.90117133771136,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Presidente_Bernardes,2022-12-31T00:00:00,1.501449275362319,1.767991542816162,17.752332484860204,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sabinopolis,2022-12-31T00:00:00,1.32,0.9394544959068298,-28.8292048555432,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Santa_Barbara,2022-12-31T00:00:00,1.083333333333333,1.036027193069458,-4.366720639742312,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Santo_Antonio_do_Grama,2022-12-31T00:00:00,1.565217391304348,1.320380449295044,-15.64236018392775,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sao_Domingos_do_Prata,2022-12-31T00:00:00,1.078125,0.862396240234375,-20.0096240942029,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sao_Geraldo,2022-12-31T00:00:00,1.561038961038961,2.5108232498168945,60.84308671871954,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sao_Jose_da_Barra,2022-12-31T00:00:00,2.3076923076923066,1.583643913269043,-31.375430425008105,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sao_Jose_do_Alegre,2022-12-31T00:00:00,1.682352941176471,1.9361464977264404,15.085630983739437,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sao_Miguel_do_Anta,2022-12-31T00:00:00,1.5,1.2320597171783447,-17.862685521443687,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sao_Sebastiao_da_Vargem_Alegre,2022-12-31T00:00:00,1.020238095238095,1.4212512969970703,39.305844746504015,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sao_Sebastiao_do_Anta,2022-12-31T00:00:00,1.8000000000000005,1.5553770065307615,-13.590166303846583,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Senhora_dos_Remedios,2022-12-31T00:00:00,1.466666666666667,1.7803937196731567,21.39048088680612,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Teixeiras,2022-12-31T00:00:00,1.5,1.1882948875427246,-20.780340830485024,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Visconde_do_Rio_Branco,2022-12-31T00:00:00,1.6666666666666672,1.4288519620895386,-14.268882274627712,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Abre_Campo,2023-12-31T00:00:00,1.8000000000000005,1.235640048980713,-31.353330612182628,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Aimores,2023-12-31T00:00:00,1.544827586206897,1.904717206954956,23.29642634306632,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Alvarenga,2023-12-31T00:00:00,1.75,0.8434215784072876,-51.80448123386928,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Andrelandia,2023-12-31T00:00:00,1.8000000000000005,1.9079126119613647,5.995145108964692,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Antonio_Dias,2023-12-31T00:00:00,1.477272727272727,1.5745137929916382,6.582472140972445,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Antonio_Prado_de_Minas,2023-12-31T00:00:00,1.4857142857142862,1.1544281244277954,-22.298107009667643,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Bocaiuva,2023-12-31T00:00:00,3.0,2.6383328437805176,-12.055571873982746,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Bom_Jesus_do_Amparo,2023-12-31T00:00:00,1.25,1.4774045944213867,18.192367553710938,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Campo_Belo,2023-12-31T00:00:00,1.5,1.358251929283142,-9.44987138112386,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Capela_Nova,2023-12-31T00:00:00,1.8000000000000005,2.071753740310669,15.09743001725937,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Capitao_Eneas,2023-12-31T00:00:00,3.0,3.5914053916931152,19.71351305643717,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Casa_Grande,2023-12-31T00:00:00,2.523076923076923,2.0101685523986816,-20.328685423222986,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Cataguases,2023-12-31T00:00:00,1.333333333333333,1.942448616027832,45.68364620208744,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Caxambu,2023-12-31T00:00:00,1.316326530612245,2.0123894214630127,52.879196359205615,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Coimbra,2023-12-31T00:00:00,1.8000000000000005,1.781158447265625,-1.0467529296875149,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Conselheiro_Pena,2023-12-31T00:00:00,1.320048602673147,1.600414514541626,21.23905978164195,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Corrego_Novo,2023-12-31T00:00:00,1.8000000000000005,1.501799464225769,-16.566696431901732,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Cruzilia,2023-12-31T00:00:00,1.5,1.6523964405059814,10.159762700398762,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Divisopolis,2023-12-31T00:00:00,1.616883116883117,1.3715604543685913,-15.172566275998776,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Esmeraldas,2023-12-31T00:00:00,2.222222222222222,2.0311756134033203,-8.597097396850572,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Espirito_Santo_do_Dourado,2023-12-31T00:00:00,1.5,1.202921748161316,-19.805216789245605,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Eugenopolis,2023-12-31T00:00:00,1.68,1.7711827754974363,5.427546160561702,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Felicio_dos_Santos,2023-12-31T00:00:00,3.542087542087541,3.3953025341033936,-4.144025415522049,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Formiga,2023-12-31T00:00:00,2.021543985637344,1.3007956743240356,-35.6533578509336,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Guaraciaba,2023-12-31T00:00:00,1.5,2.5063724517822266,67.09149678548178,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Guiricema,2023-12-31T00:00:00,1.5,2.182644844055176,45.50965627034505,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Iapu,2023-12-31T00:00:00,1.559210526315789,1.7621910572052002,13.018160630882075,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Ijaci,2023-12-31T00:00:00,1.971428571428572,1.1775633096694946,-40.26852777038797,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Imbe_de_Minas,2023-12-31T00:00:00,1.3799999999999997,1.3155781030654907,-4.668253401051374,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Inhapim,2023-12-31T00:00:00,2.7000000000000006,1.9210528135299685,-28.849895795186377,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Irai_de_Minas,2023-12-31T00:00:00,2.18825561312608,3.86162781715393,76.4706003261346,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Itamarati_de_Minas,2023-12-31T00:00:00,1.2,1.3894717693328855,15.789314111073816,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Jequeri,2023-12-31T00:00:00,1.5,1.4237171411514282,-5.085523923238119,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Lamim,2023-12-31T00:00:00,1.8000000000000005,1.707075119018555,-5.162493387858087,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Mar_de_Espanha,2023-12-31T00:00:00,1.5625,2.4559595584869385,57.18141174316407,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Mata_Verde,2023-12-31T00:00:00,1.739700374531835,2.127512693405152,22.29190293631336,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Monte_Alegre_de_Minas,2023-12-31T00:00:00,2.338983050847458,3.2170584201812744,37.54090347151823,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Nova_Era,2023-12-31T00:00:00,2.636363636363636,1.5383188724517822,-41.649973803553074,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Nova_Ponte,2023-12-31T00:00:00,2.6142857142857143,3.269411325454712,25.059449607557283,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Paula_Candido,2023-12-31T00:00:00,1.6197080291970802,1.5612988471984863,-3.606154994956,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Pecanha,2023-12-31T00:00:00,2.30379746835443,1.3964102268218994,-39.38658905553292,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Pedra_Bonita,2023-12-31T00:00:00,1.08,1.3358652591705322,23.6912277009752,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Pedra_Dourada,2023-12-31T00:00:00,1.2,1.2949330806732178,7.911090056101486,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Pedra_do_Anta,2023-12-31T00:00:00,1.5,1.091144323348999,-27.25704511006673,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Piedade_de_Caratinga,2023-12-31T00:00:00,2.4,1.5859675407409668,-33.918019135793045,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Pocrane,2023-12-31T00:00:00,1.377952755905512,1.2905728816986084,-6.34128229958672,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Presidente_Bernardes,2023-12-31T00:00:00,1.320588235294118,1.5587010383605957,18.03081359523438,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sabinopolis,2023-12-31T00:00:00,1.2,1.0887329578399658,-9.272253513336178,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Santa_Barbara,2023-12-31T00:00:00,1.3,1.0609232187271118,-18.39052163637601,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Santo_Antonio_do_Grama,2023-12-31T00:00:00,1.521739130434783,1.5437837839126587,1.44864865711755,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sao_Domingos_do_Prata,2023-12-31T00:00:00,1.2,0.9092110991477966,-24.23240840435028,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sao_Geraldo,2023-12-31T00:00:00,1.5,1.6803090572357178,12.02060381571452,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sao_Jose_da_Barra,2023-12-31T00:00:00,2.2413698630136984,2.1297643184661865,-4.979345282953411,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sao_Jose_do_Alegre,2023-12-31T00:00:00,1.592307692307692,2.1101608276367188,32.52217758105,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sao_Miguel_do_Anta,2023-12-31T00:00:00,1.5598870056497178,1.205921649932861,-22.6917305186105,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sao_Sebastiao_da_Vargem_Alegre,2023-12-31T00:00:00,1.5,1.3196550607681274,-12.022995948791504,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Sao_Sebastiao_do_Anta,2023-12-31T00:00:00,1.7998212689901698,1.7462079524993896,-2.978813364110377,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Senhora_dos_Remedios,2023-12-31T00:00:00,1.5,1.813268661499024,20.884577433268227,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Teixeiras,2023-12-31T00:00:00,1.5007092198581562,1.07893705368042,-28.10485606382836,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_2_2023,Visconde_do_Rio_Branco,2023-12-31T00:00:00,1.333333333333333,1.1139624118804932,-16.452819108962995,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:20
V42_cluster_3_2023,Aiuruoca,2022-12-31T00:00:00,1.5,1.448888897895813,-3.407406806945801,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Albertina,2022-12-31T00:00:00,1.44,1.6333048343658447,13.423946830961444,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Baependi,2022-12-31T00:00:00,1.079558011049724,1.4405735731124878,33.44105257592641,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Bandeira_do_Sul,2022-12-31T00:00:00,1.110344827586207,1.6637370586395264,49.83967298306293,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Borda_da_Mata,2022-12-31T00:00:00,1.2,1.639241099357605,36.603424946467086,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Brazopolis,2022-12-31T00:00:00,1.200934579439252,1.3013916015625,8.364903787694576,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Bueno_Brandao,2022-12-31T00:00:00,1.440196078431373,1.6497844457626345,14.552766145533456,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Cachoeira_de_Minas,2022-12-31T00:00:00,1.5,1.819429397583008,21.295293172200523,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Caldas,2022-12-31T00:00:00,1.2,1.5223844051361084,26.865367094675708,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Cambuquira,2022-12-31T00:00:00,1.32,1.598877191543579,21.127059965422653,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Campanha,2022-12-31T00:00:00,1.266469038208169,1.5739396810531616,24.277785999412163,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Careacu,2022-12-31T00:00:00,1.2,1.360215187072754,13.351265589396164,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Carmo_da_Cachoeira,2022-12-31T00:00:00,1.2,1.6430168151855469,36.91806793212891,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Carmo_de_Minas,2022-12-31T00:00:00,1.14,1.4870648384094238,30.4442840710021,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Carrancas,2022-12-31T00:00:00,1.6785714285714293,1.5106371641159058,-10.004594478201396,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Carvalhopolis,2022-12-31T00:00:00,1.44,1.4698591232299805,2.073550224304203,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Conceicao_das_Pedras,2022-12-31T00:00:00,1.375,1.5955160856246948,16.037533499977805,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Conceicao_do_Rio_Verde,2022-12-31T00:00:00,1.5,1.7300219535827637,15.334796905517578,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Conceicao_dos_Ouros,2022-12-31T00:00:00,1.261168384879725,1.4614495038986206,15.880600990326595,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Congonhal,2022-12-31T00:00:00,1.080597014925373,1.688709735870361,56.27562472833457,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Cordislandia,2022-12-31T00:00:00,1.260122699386503,1.610823154449463,27.83065928688534,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Cristina,2022-12-31T00:00:00,1.320183486238532,1.7051103115081787,29.15707015593572,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Datas,2022-12-31T00:00:00,0.75,1.5813257694244385,110.84343592325848,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Dom_Vicoso,2022-12-31T00:00:00,1.320588235294118,1.3833985328674316,4.756236341854478,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Fama,2022-12-31T00:00:00,1.379787234042553,1.5597825050354004,13.045146856844765,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Fortaleza_de_Minas,2022-12-31T00:00:00,1.5,1.4647486209869385,-2.3500919342041016,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Heliodora,2022-12-31T00:00:00,1.080140597539543,1.5510423183441162,43.59633568789491,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Ibitiura_de_Minas,2022-12-31T00:00:00,1.3799999999999997,1.7542636394500732,27.12055358333867,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Inconfidentes,2022-12-31T00:00:00,1.739917695473251,2.082194328308105,19.672001366809287,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Ingai,2022-12-31T00:00:00,1.380228136882129,1.6341025829315186,18.393658212393795,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Itajuba,2022-12-31T00:00:00,1.068965517241379,1.7331056594848633,62.12923911310016,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Itutinga,2022-12-31T00:00:00,1.8000000000000005,1.5226314067840576,-15.40936628977459,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Jesuania,2022-12-31T00:00:00,1.2,1.4827593564987185,23.563279708226524,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Lambari,2022-12-31T00:00:00,1.2,1.4873502254486084,23.94585212071737,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Luminarias,2022-12-31T00:00:00,1.380229885057471,1.7831398248672483,29.191509629789014,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Maria_da_Fe,2022-12-31T00:00:00,1.2,1.6241127252578735,35.3427271048228,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Monsenhor_Paulo,2022-12-31T00:00:00,1.23,1.6506109237670898,34.196010062365026,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Monte_Siao,2022-12-31T00:00:00,1.2,1.635690212249756,36.30751768747966,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Natercia,2022-12-31T00:00:00,1.4399193548387097,1.5474655628204346,7.468904950845076,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Olimpio_Noronha,2022-12-31T00:00:00,1.2,1.4132530689239502,17.771089076995857,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Ouro_Fino,2022-12-31T00:00:00,1.2,1.6120867729187012,34.34056440989177,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Paraisopolis,2022-12-31T00:00:00,1.322222222222222,1.6953943967819214,28.22310563896887,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Pedralva,2022-12-31T00:00:00,1.320091324200913,1.8129749298095703,37.33708392538775,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Pirangucu,2022-12-31T00:00:00,1.333333333333333,1.271665096282959,-4.625117778778055,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Piranguinho,2022-12-31T00:00:00,1.380549682875264,1.4515368938446045,5.14195264754948,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Poco_Fundo,2022-12-31T00:00:00,1.079978925184405,1.2934858798980713,19.76954825088001,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Pocos_de_Caldas,2022-12-31T00:00:00,1.169867549668874,1.59696626663208,36.50829677975892,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Pouso_Alegre,2022-12-31T00:00:00,1.5,1.4485427141189575,-3.430485725402832,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Pouso_Alto,2022-12-31T00:00:00,1.5,1.5254162549972534,1.6944169998168943,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Santa_Rita_de_Caldas,2022-12-31T00:00:00,1.8000000000000005,1.918168544769287,6.564919153849267,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Santa_Rita_do_Sapucai,2022-12-31T00:00:00,1.14,1.6422380208969116,44.05596674534314,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Sao_Bento_Abade,2022-12-31T00:00:00,1.2,1.735795974731445,44.64966456095378,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Sao_Goncalo_do_Sapucai,2022-12-31T00:00:00,1.08,1.809709310531616,67.56567690107556,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Sao_Joao_da_Mata,2022-12-31T00:00:00,0.959748427672956,1.329222321510315,38.49695224124513,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Sao_Joao_del_Rei,2022-12-31T00:00:00,1.836244541484716,1.7062654495239258,-7.078528432585244,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Sao_Lourenco,2022-12-31T00:00:00,1.8000000000000005,1.719196081161499,-4.489106602138957,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Sao_Sebastiao_da_Bela_Vista,2022-12-31T00:00:00,1.5,1.4245694875717163,-5.028700828552246,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Sao_Tome_das_Letras,2022-12-31T00:00:00,1.434375,1.4101996421813965,-1.685427996068216,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Senador_Jose_Bento,2022-12-31T00:00:00,1.32,1.3714271783828735,3.895998362338899,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Serrania,2022-12-31T00:00:00,1.2,1.5153898000717163,26.2824833393097,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Silvianopolis,2022-12-31T00:00:00,0.9,1.2174514532089231,35.272383689880364,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Soledade_de_Minas,2022-12-31T00:00:00,1.5,1.7777533531188965,18.51689020792643,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Tocos_do_Moji,2022-12-31T00:00:00,1.5,1.4253108501434326,-4.979276657104492,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Tres_Coracoes,2022-12-31T00:00:00,1.08,1.8598709106445312,72.21026950412325,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Turvolandia,2022-12-31T00:00:00,1.4398692810457523,1.696506381034851,17.823638809955558,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Virginia,2022-12-31T00:00:00,1.62,1.4938799142837524,-7.785190476311583,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Aiuruoca,2023-12-31T00:00:00,1.6000000000000003,1.5156779289245603,-5.2701294422149845,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Albertina,2023-12-31T00:00:00,1.5,1.5789663791656494,5.264425277709961,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Baependi,2023-12-31T00:00:00,1.439779005524862,1.3130148649215698,-8.804416519261661,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Bandeira_do_Sul,2023-12-31T00:00:00,1.5,1.6010206937789917,6.734712918599446,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Borda_da_Mata,2023-12-31T00:00:00,1.290476190476191,1.46394944190979,13.442576679356378,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Brazopolis,2023-12-31T00:00:00,1.440389294403893,1.2179912328720093,-15.440135690811518,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Bueno_Brandao,2023-12-31T00:00:00,1.379901960784314,1.4628456830978394,6.010841688084968,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Cachoeira_de_Minas,2023-12-31T00:00:00,1.6204081632653062,1.7849804162979126,10.156222164480743,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Caldas,2023-12-31T00:00:00,1.56,1.4538393020629885,-6.805172944680242,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Cambuquira,2023-12-31T00:00:00,1.56,1.4990730285644531,-3.905575092022239,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Campanha,2023-12-31T00:00:00,1.327075098814229,1.5029964447021484,13.25632181970026,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Careacu,2023-12-31T00:00:00,1.5,1.258333444595337,-16.111103693644203,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Carmo_da_Cachoeira,2023-12-31T00:00:00,1.32,1.518380880355835,15.028854572411737,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Carmo_de_Minas,2023-12-31T00:00:00,1.5,1.3723524808883667,-8.509834607442219,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Carrancas,2023-12-31T00:00:00,1.214285714285714,1.6066598892211914,32.31316734762756,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Carvalhopolis,2023-12-31T00:00:00,1.560106382978723,1.3968050479888916,-10.467320483494138,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Conceicao_das_Pedras,2023-12-31T00:00:00,1.5598885793871868,1.5196397304534912,-2.580238708427987,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Conceicao_do_Rio_Verde,2023-12-31T00:00:00,1.44,1.672872543334961,16.17170439826118,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Conceicao_dos_Ouros,2023-12-31T00:00:00,1.422222222222222,1.4049899578094482,-1.211643591523159,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Congonhal,2023-12-31T00:00:00,1.2,1.5315992832183838,27.63327360153199,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Cordislandia,2023-12-31T00:00:00,1.14,1.4965037107467651,31.27225532866362,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Cristina,2023-12-31T00:00:00,1.68,1.56889009475708,-6.613684835888087,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Datas,2023-12-31T00:00:00,0.6666666666666667,1.271213173866272,90.68197607994075,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Dom_Vicoso,2023-12-31T00:00:00,1.32,1.324673771858215,0.3540736256223689,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Fama,2023-12-31T00:00:00,1.559677419354839,1.4650382995605469,-6.06786497129898,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Fortaleza_de_Minas,2023-12-31T00:00:00,1.5,1.428152561187744,-4.789829254150391,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Heliodora,2023-12-31T00:00:00,1.439872408293461,1.380833625793457,-4.100278758030853,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Ibitiura_de_Minas,2023-12-31T00:00:00,1.150344827586207,1.4866615533828735,29.23616621134092,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Inconfidentes,2023-12-31T00:00:00,1.7401360544217692,1.7653391361236572,1.4483397225088102,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Ingai,2023-12-31T00:00:00,1.6197718631178712,1.4965689182281494,-7.606191198590806,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Itajuba,2023-12-31T00:00:00,1.5862068965517242,1.4953079223632812,-5.730587503184446,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Itutinga,2023-12-31T00:00:00,1.5,1.4655485153198242,-2.296765645345052,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Jesuania,2023-12-31T00:00:00,1.26,1.3663127422332764,8.437519224863204,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Lambari,2023-12-31T00:00:00,1.56,1.405847430229187,-9.881574985308529,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Luminarias,2023-12-31T00:00:00,1.260224719101124,1.6257410049438477,29.00405620542296,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Maria_da_Fe,2023-12-31T00:00:00,1.422222222222222,1.447786569595337,1.797493174672139,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Monsenhor_Paulo,2023-12-31T00:00:00,1.389931972789116,1.5249451398849487,9.713652879349752,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Monte_Siao,2023-12-31T00:00:00,1.5,1.4881995916366575,-0.7866938908894857,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Natercia,2023-12-31T00:00:00,1.5,1.4831969738006592,-1.1202017466227212,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Olimpio_Noronha,2023-12-31T00:00:00,1.2,1.31490159034729,9.57513252894084,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Ouro_Fino,2023-12-31T00:00:00,1.68,1.4230797290802002,-15.2928732690357,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Paraisopolis,2023-12-31T00:00:00,1.2,1.6167654991149902,34.73045825958253,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Pedralva,2023-12-31T00:00:00,1.67998417721519,1.493443727493286,-11.103702776005957,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Pirangucu,2023-12-31T00:00:00,1.333333333333333,1.2419341802597046,-6.854936480522135,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Piranguinho,2023-12-31T00:00:00,1.460887949260042,1.422050714492798,-2.6584677344293,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Poco_Fundo,2023-12-31T00:00:00,1.380160799652325,1.1890547275543213,-13.846652661497496,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Pocos_de_Caldas,2023-12-31T00:00:00,1.5,1.4686566591262815,-2.089556058247884,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Pouso_Alegre,2023-12-31T00:00:00,1.566666666666667,1.4301778078079224,-8.712054820770938,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Pouso_Alto,2023-12-31T00:00:00,1.8000000000000005,1.518485188484192,-15.639711750878242,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Santa_Rita_de_Caldas,2023-12-31T00:00:00,1.68,1.818453907966613,8.241304045631775,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Santa_Rita_do_Sapucai,2023-12-31T00:00:00,1.3799999999999997,1.4831454753875732,7.474309810693739,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Sao_Bento_Abade,2023-12-31T00:00:00,1.5,1.5332159996032717,2.2143999735514326,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Sao_Goncalo_do_Sapucai,2023-12-31T00:00:00,1.2,1.618488073348999,34.87400611241659,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Sao_Joao_da_Mata,2023-12-31T00:00:00,1.08,1.214057683944702,12.41274851339834,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Sao_Joao_del_Rei,2023-12-31T00:00:00,1.799568965517241,1.6749016046524048,-6.927623406141804,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Sao_Lourenco,2023-12-31T00:00:00,1.5,1.6617591381072998,10.783942540486652,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Sao_Sebastiao_da_Bela_Vista,2023-12-31T00:00:00,1.5,1.3869597911834717,-7.536013921101888,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Sao_Tome_das_Letras,2023-12-31T00:00:00,1.09812734082397,1.373185634613037,25.047941487612864,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Senador_Jose_Bento,2023-12-31T00:00:00,1.2,1.2905361652374268,7.544680436452234,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Serrania,2023-12-31T00:00:00,0.9,1.3944480419158936,54.93867132398817,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Silvianopolis,2023-12-31T00:00:00,1.13974358974359,1.0698418617248535,-6.133109994894756,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Soledade_de_Minas,2023-12-31T00:00:00,1.5,1.6815969944000244,12.10646629333496,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Tocos_do_Moji,2023-12-31T00:00:00,1.5,1.4554319381713867,-2.971204121907552,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Tres_Coracoes,2023-12-31T00:00:00,1.44,1.637047529220581,13.68385619587369,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Turvolandia,2023-12-31T00:00:00,1.259748427672956,1.631622552871704,29.519713383225632,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_3_2023,Virginia,2023-12-31T00:00:00,1.5,1.5026988983154297,0.1799265543619791,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:42:54
V42_cluster_4_2023,Abadia_dos_Dourados,2022-12-31T00:00:00,2.0384615384615383,1.7385684251785278,-14.71173763275146,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Acucena,2022-12-31T00:00:00,1.0,0.9503825902938844,-4.961740970611572,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Agua_Boa,2022-12-31T00:00:00,0.9136363636363636,1.0616989135742188,16.205851236979175,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Aguas_Vermelhas,2022-12-31T00:00:00,3.0,3.2697503566741943,8.991678555806477,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Angelandia,2022-12-31T00:00:00,1.9799126637554585,1.3803372383117676,-30.282922899560035,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Aricanduva,2022-12-31T00:00:00,1.2,1.1641504764556885,-2.98746029535929,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Ataleia,2022-12-31T00:00:00,0.7142857142857143,0.8267641067504883,15.746974945068356,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Bandeira,2022-12-31T00:00:00,1.5,1.0633492469787598,-29.110050201416016,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Berilo,2022-12-31T00:00:00,1.5625,1.2299604415893557,-21.28253173828125,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Berizal,2022-12-31T00:00:00,3.570967741935484,3.536240816116333,-0.9724794041496586,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Bonfinopolis_de_Minas,2022-12-31T00:00:00,3.3,2.7655739784240723,-16.19472792654326,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Buritis,2022-12-31T00:00:00,3.0,2.623012065887451,-12.566264470418297,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Buritizeiro,2022-12-31T00:00:00,3.0,2.7294065952301025,-9.019780158996582,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Capelinha,2022-12-31T00:00:00,1.576751592356688,1.3061017990112305,-17.16502425984117,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Carai,2022-12-31T00:00:00,1.2,1.5904372930526731,32.536441087722785,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Caranaiba,2022-12-31T00:00:00,3.405405405405405,1.2147011756896973,-64.33020357101682,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Catuji,2022-12-31T00:00:00,1.3230769230769233,1.51422917842865,14.447554183560731,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Congonhas_do_Norte,2022-12-31T00:00:00,0.9,1.0842288732528689,20.46987480587429,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Coroaci,2022-12-31T00:00:00,2.1,1.6489202976226809,-21.4799858274914,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Cuparaque,2022-12-31T00:00:00,1.02,1.1586403846740725,13.592194575889437,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Diamantina,2022-12-31T00:00:00,2.965183752417796,2.266912937164306,-23.54898965988608,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Formoso,2022-12-31T00:00:00,3.3607142857142858,2.959552764892578,-11.936793393207028,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Franciscopolis,2022-12-31T00:00:00,1.255555555555556,1.6592860221862793,32.155523890942554,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Frei_Gaspar,2022-12-31T00:00:00,1.133333333333333,1.543049693107605,36.15144350949458,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Grao_Mogol,2022-12-31T00:00:00,1.177777777777778,1.0075201988220217,-14.455832175488762,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Indaiabira,2022-12-31T00:00:00,2.1723404255319148,2.471852302551269,13.787520293741112,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Itabirinha,2022-12-31T00:00:00,0.8674242424242423,0.8891890645027161,2.5091323269506853,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Itacambira,2022-12-31T00:00:00,0.75,0.9716812372207642,29.557498296101887,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Itaipe,2022-12-31T00:00:00,0.9604651162790696,0.894899308681488,-6.826464229288145,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Itamarandiba,2022-12-31T00:00:00,2.136,1.823657751083374,-14.622764462388863,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Itambacuri,2022-12-31T00:00:00,0.9620253164556964,0.9757269024848938,1.4242438109297364,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Jequitinhonha,2022-12-31T00:00:00,2.4,2.156951427459717,-10.12702385584513,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Joao_Pinheiro,2022-12-31T00:00:00,2.4,2.4938385486602783,3.9099395275116007,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Jose_Goncalves_de_Minas,2022-12-31T00:00:00,1.5,1.5867090225219729,5.780601501464844,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Juiz_de_Fora,2022-12-31T00:00:00,1.5714285714285707,0.831212043762207,-47.104688124223166,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Ladainha,2022-12-31T00:00:00,0.6000000000000001,0.932758331298828,55.459721883138,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Malacacheta,2022-12-31T00:00:00,1.5595238095238098,1.8952901363372805,21.53005454376452,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Mantena,2022-12-31T00:00:00,0.75,0.6095238924026489,-18.73014767964681,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Minas_Novas,2022-12-31T00:00:00,1.7848101265822778,1.5123324394226074,-15.266480344407068,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Monte_Formoso,2022-12-31T00:00:00,0.6000000000000001,1.0280094146728516,71.33490244547524,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Ninheira,2022-12-31T00:00:00,2.9401197604790417,2.5364255905151367,-13.730534905085973,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Nova_Belem,2022-12-31T00:00:00,0.96,0.9700958132743835,1.0516472160816348,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Novo_Cruzeiro,2022-12-31T00:00:00,1.180722891566265,1.1648261547088623,-1.346356284861664,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Novorizonte,2022-12-31T00:00:00,1.461538461538461,1.1914054155349731,-18.482787358133383,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Ouro_Verde_de_Minas,2022-12-31T00:00:00,2.0,1.2707993984222412,-36.460030078887925,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Padre_Paraiso,2022-12-31T00:00:00,0.6000000000000001,0.6460976004600525,7.682933410008733,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Paracatu,2022-12-31T00:00:00,2.4,2.5645103454589844,6.854597727457687,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Pedra_Azul,2022-12-31T00:00:00,1.5333333333333332,1.1381118297576904,-25.775315450585403,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Pirapora,2022-12-31T00:00:00,3.3595505617977524,2.95188045501709,-12.134662041297316,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Pote,2022-12-31T00:00:00,1.2,0.7149316072463989,-40.42236606280009,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Rio_Pardo_de_Minas,2022-12-31T00:00:00,2.515254237288135,3.388259172439575,34.70841723310983,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Santa_Barbara_do_Monte_Verde,2022-12-31T00:00:00,1.3,0.9370116591453552,-27.92218006574191,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Santa_Maria_do_Suacui,2022-12-31T00:00:00,1.0,0.7935848832130432,-20.64151167869568,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Santo_Antonio_do_Retiro,2022-12-31T00:00:00,1.2,1.5165464878082275,26.37887398401897,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Sao_Goncalo_do_Rio_Preto,2022-12-31T00:00:00,3.0,2.8571009635925293,-4.763301213582357,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Sao_Joao_do_Manteninha,2022-12-31T00:00:00,1.2,0.7496433258056641,-37.52972284952799,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Sao_Joao_do_Paraiso,2022-12-31T00:00:00,3.09016393442623,2.481905460357666,-19.6836959778156,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Sao_Sebastiao_do_Maranhao,2022-12-31T00:00:00,1.141666666666667,0.8936468362808228,-21.72436470532942,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Senador_Modestino_Goncalves,2022-12-31T00:00:00,2.7000000000000006,2.8370680809021,5.076595588966628,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Setubinha,2022-12-31T00:00:00,1.079761904761905,0.9887571334838868,-8.42822578539529,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Taiobeiras,2022-12-31T00:00:00,3.14031971580817,3.142140388488769,0.0579773031209035,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Teofilo_Otoni,2022-12-31T00:00:00,0.8181818181818183,0.8998568058013916,9.98249848683673,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Turmalina,2022-12-31T00:00:00,1.205163043478261,1.6793625354766846,39.34733101587821,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Urucuia,2022-12-31T00:00:00,1.9193245778611627,2.116018772125244,10.248094383456053,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Vargem_Grande_do_Rio_Pardo,2022-12-31T00:00:00,1.8000000000000005,1.9454165697097776,8.078698317209863,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Varzea_da_Palma,2022-12-31T00:00:00,2.5200000000000005,2.6976513862609863,7.049658184959755,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Abadia_dos_Dourados,2023-12-31T00:00:00,2.103030303030303,1.8545526266098025,-11.81522092489413,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Acucena,2023-12-31T00:00:00,1.6666666666666672,0.9516666531562804,-42.90000081062318,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Agua_Boa,2023-12-31T00:00:00,1.213636363636364,1.0479015111923218,-13.656055257561531,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Aguas_Vermelhas,2023-12-31T00:00:00,3.0,3.0191431045532227,0.6381034851074219,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Angelandia,2023-12-31T00:00:00,2.095663265306122,1.943634867668152,-7.254428712609168,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Aricanduva,2023-12-31T00:00:00,1.2,1.3140851259231567,9.507093826929731,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Ataleia,2023-12-31T00:00:00,0.6833333333333333,0.8829647898674011,29.214359492790404,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Bandeira,2023-12-31T00:00:00,0.95,1.2273032665252686,29.189817528975624,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Berilo,2023-12-31T00:00:00,1.7988826815642458,1.427440881729126,-20.64847272375356,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Berizal,2023-12-31T00:00:00,3.570967741935484,2.108643054962158,-40.95037515462791,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Bonfinopolis_de_Minas,2023-12-31T00:00:00,3.0,2.4420175552368164,-18.599414825439453,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Buritis,2023-12-31T00:00:00,3.0,2.597994804382324,-13.40017318725586,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Buritizeiro,2023-12-31T00:00:00,3.0,2.787397146224976,-7.0867617925008135,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Capelinha,2023-12-31T00:00:00,1.541750580945004,1.4774553775787354,-4.170272686186339,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Carai,2023-12-31T00:00:00,1.2,1.3729865550994873,14.415546258290613,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Caranaiba,2023-12-31T00:00:00,1.45,1.3417510986328125,-7.465441473599135,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Catuji,2023-12-31T00:00:00,1.384615384615385,1.2826844453811646,-7.361678944693699,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Congonhas_do_Norte,2023-12-31T00:00:00,1.111111111111111,0.9400603175163268,-15.394571423530564,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Coroaci,2023-12-31T00:00:00,2.4,1.715005874633789,-28.54142189025879,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Cuparaque,2023-12-31T00:00:00,1.318367346938776,1.1526671648025513,-12.56858966667958,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Diamantina,2023-12-31T00:00:00,2.965183752417796,2.44083833694458,-17.683403770362183,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Formoso,2023-12-31T00:00:00,3.3803921568627446,2.70084547996521,-20.10259891054192,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Franciscopolis,2023-12-31T00:00:00,1.2,1.43682599067688,19.73549922307333,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Frei_Gaspar,2023-12-31T00:00:00,1.2,1.5127443075180054,26.06202562650045,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Grao_Mogol,2023-12-31T00:00:00,1.155555555555555,1.1263524293899536,-2.527193610484737,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Indaiabira,2023-12-31T00:00:00,1.9320754716981128,2.3877475261688232,23.584588756784825,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Itabirinha,2023-12-31T00:00:00,1.027027027027027,0.9505560398101808,-7.445859281640299,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Itacambira,2023-12-31T00:00:00,0.75,0.7216992378234863,-3.7734349568684897,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Itaipe,2023-12-31T00:00:00,0.9598214285714286,0.8899445533752441,-7.280195369276893,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Itamarandiba,2023-12-31T00:00:00,2.4,2.1315717697143555,-11.184509595235186,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Itambacuri,2023-12-31T00:00:00,0.8987341772151899,0.9562817215919496,6.403177472907052,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Jequitinhonha,2023-12-31T00:00:00,2.7000000000000006,2.1052284240722656,-22.0285768862124,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Joao_Pinheiro,2023-12-31T00:00:00,3.3,2.6051979064941406,-21.054608894116942,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Jose_Goncalves_de_Minas,2023-12-31T00:00:00,1.3210526315789468,1.6475932598114014,24.71821488612207,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Juiz_de_Fora,2023-12-31T00:00:00,0.7142857142857143,1.2525101900100708,75.35142660140991,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Ladainha,2023-12-31T00:00:00,1.081818181818182,0.806676983833313,-25.43321998179461,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Malacacheta,2023-12-31T00:00:00,1.619230769230769,1.8474607467651367,14.094962983120096,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Mantena,2023-12-31T00:00:00,1.2,0.6831340789794922,-43.07216008504231,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Minas_Novas,2023-12-31T00:00:00,1.26,1.5933454036712646,26.455984418354333,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Monte_Formoso,2023-12-31T00:00:00,0.6000000000000001,0.7897535562515259,31.625592708587625,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Ninheira,2023-12-31T00:00:00,3.0,2.594670534133911,-13.510982195536297,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Nova_Belem,2023-12-31T00:00:00,0.96,0.9986382722854614,4.024820029735581,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Novo_Cruzeiro,2023-12-31T00:00:00,1.209891435464415,1.1675119400024414,-3.502751918043482,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Novorizonte,2023-12-31T00:00:00,1.333333333333333,1.2475619316101074,-6.432855129241923,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Ouro_Verde_de_Minas,2023-12-31T00:00:00,2.5,1.6682860851287842,-33.26855659484863,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Padre_Paraiso,2023-12-31T00:00:00,0.7212121212121212,0.5938489437103271,-17.659600241845393,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Paracatu,2023-12-31T00:00:00,2.4,2.4852874279022217,3.5536428292592404,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Pedra_Azul,2023-12-31T00:00:00,1.166666666666667,1.2593823671340942,7.947060040065193,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Pirapora,2023-12-31T00:00:00,3.6000000000000005,3.0188119411468506,-16.14411274592083,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Pote,2023-12-31T00:00:00,1.125,0.8811450004577637,-21.67599995930989,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Rio_Pardo_de_Minas,2023-12-31T00:00:00,2.568224299065421,2.631309509277344,2.4563746334336667,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Santa_Barbara_do_Monte_Verde,2023-12-31T00:00:00,1.4,1.0829689502716064,-22.64507498059953,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Santa_Maria_do_Suacui,2023-12-31T00:00:00,0.6666666666666667,0.9056019186973572,35.84028780460356,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Santo_Antonio_do_Retiro,2023-12-31T00:00:00,1.02,1.2011477947235107,17.759587717991245,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Sao_Goncalo_do_Rio_Preto,2023-12-31T00:00:00,2.868421052631579,2.974000930786133,3.680766394378933,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Sao_Joao_do_Manteninha,2023-12-31T00:00:00,1.2,0.9704285860061646,-19.13095116615295,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Sao_Joao_do_Paraiso,2023-12-31T00:00:00,3.601190476190476,2.362663507461548,-34.39215384239007,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Sao_Sebastiao_do_Maranhao,2023-12-31T00:00:00,1.2,1.0403170585632324,-13.306911786397295,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Senador_Modestino_Goncalves,2023-12-31T00:00:00,2.7000000000000006,3.5485637187957764,31.428285881325017,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Setubinha,2023-12-31T00:00:00,1.2,0.9798867702484132,-18.342769145965573,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Taiobeiras,2023-12-31T00:00:00,3.0,3.27030873298645,9.01029109954834,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Teofilo_Otoni,2023-12-31T00:00:00,1.2,0.8996374607086182,-25.03021160761515,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Turmalina,2023-12-31T00:00:00,1.360606060606061,1.31953227519989,-3.0187860098076604,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Urucuia,2023-12-31T00:00:00,2.0994575045207946,1.872857451438904,-10.793266955580162,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Vargem_Grande_do_Rio_Pardo,2023-12-31T00:00:00,2.1,1.7525501251220703,-16.545232137044273,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_4_2023,Varzea_da_Palma,2023-12-31T00:00:00,3.3,2.6590163707733154,-19.42374634020256,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2023_(2023)
    Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:43:29
V42_cluster_0_2024,Aguanil,2023-12-31T00:00:00,1.717774762550882,1.7172870635986328,-0.0283913213118153,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Alfenas,2023-12-31T00:00:00,1.698430922311519,1.3703253269195557,-19.31815954842723,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Alto_Caparao,2023-12-31T00:00:00,1.32,1.223313331604004,-7.324747605757284,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Alto_Jequitiba,2023-12-31T00:00:00,1.08,1.220629096031189,13.021212595480453,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Amparo_do_Serra,2023-12-31T00:00:00,1.319444444444444,1.3789297342300415,4.508358804803183,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Araponga,2023-12-31T00:00:00,1.5,1.2358171939849854,-17.61218706766764,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Astolfo_Dutra,2023-12-31T00:00:00,0.625,1.1301769018173218,80.82830429077148,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Bambui,2023-12-31T00:00:00,1.5,1.4775238037109375,-1.4984130859375,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Boa_Esperanca,2023-12-31T00:00:00,1.08,1.353520750999451,25.325995462912093,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Bom_Jesus_do_Galho,2023-12-31T00:00:00,1.2003192338387871,0.9579294323921204,-20.1937780061618,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Bom_Sucesso,2023-12-31T00:00:00,1.5,1.441301703453064,-3.913219769795736,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Caete,2023-12-31T00:00:00,1.166666666666667,1.3158334493637085,12.785724231174983,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Caiana,2023-12-31T00:00:00,1.08,1.0729141235351562,-0.6560996726707241,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Cajuri,2023-12-31T00:00:00,0.8200000000000001,1.2890533208847046,57.2016244981347,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Camacho,2023-12-31T00:00:00,1.680155642023346,1.553020715713501,-7.566854113392821,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Campo_do_Meio,2023-12-31T00:00:00,0.8771626297577856,1.51216721534729,72.39302770625909,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Campos_Altos,2023-12-31T00:00:00,1.380040526849037,1.437603235244751,4.171088259787807,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Campos_Gerais,2023-12-31T00:00:00,1.560242401107029,1.5386154651641846,-1.3861266638760528,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Cana_Verde,2023-12-31T00:00:00,1.3196428571428571,1.184066891670227,-10.273686152188477,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Canaa,2023-12-31T00:00:00,1.560135135135135,1.3671724796295166,-12.36832958632807,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Candeias,2023-12-31T00:00:00,1.080032206119163,1.33363139629364,23.48070629168783,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Caparao,2023-12-31T00:00:00,1.32,1.25149405002594,-5.189844695004554,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Capitolio,2023-12-31T00:00:00,1.32,1.4019997119903564,6.212099393208816,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Caputira,2023-12-31T00:00:00,1.380065005417118,1.1911360025405884,-13.689862588713831,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Carangola,2023-12-31T00:00:00,1.140118343195266,1.1428898572921753,0.2430900365256795,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Caratinga,2023-12-31T00:00:00,1.3799999999999997,1.3418173789978027,-2.766856594362097,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Carmo_da_Mata,2023-12-31T00:00:00,1.2,1.3074665069580078,8.955542246500656,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Carmo_do_Rio_Claro,2023-12-31T00:00:00,1.816856256463288,1.760012149810791,-3.128706877623346,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Carmopolis_de_Minas,2023-12-31T00:00:00,1.5,1.774369239807129,18.2912826538086,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Cassia,2023-12-31T00:00:00,1.6377649325626198,1.6264289617538452,-0.6921610411475303,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Catas_Altas_da_Noruega,2023-12-31T00:00:00,1.1,0.8364169001579285,-23.962099985642872,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Chale,2023-12-31T00:00:00,1.3799999999999997,1.2050408124923706,-12.678201993306455,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Claudio,2023-12-31T00:00:00,2.398692810457516,1.4554786682128906,-39.32200647504842,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Conceicao_da_Barra_de_Minas,2023-12-31T00:00:00,1.680769230769231,1.6054505109786987,-4.481205296461875,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Conceicao_de_Ipanema,2023-12-31T00:00:00,1.5,1.350532293319702,-9.964513778686523,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Coqueiral,2023-12-31T00:00:00,1.32,1.3651024103164673,3.416849266399032,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Corrego_Danta,2023-12-31T00:00:00,1.44,1.3941999673843384,-3.1805578205320533,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Cristais,2023-12-31T00:00:00,1.5631111111111111,1.4518567323684692,-7.117496507561678,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Desterro_de_Entre_Rios,2023-12-31T00:00:00,1.375,1.81827449798584,32.23814530806108,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Divinesia,2023-12-31T00:00:00,2.1,2.157079696655273,2.7180807931082547,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Divino,2023-12-31T00:00:00,1.02,1.173384428024292,15.037689021989408,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Doresopolis,2023-12-31T00:00:00,1.501960784313725,1.447683572769165,-3.6137569044028206,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Durande,2023-12-31T00:00:00,1.3799999999999997,1.4735218286514282,6.776944105175983,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Eloi_Mendes,2023-12-31T00:00:00,1.577412806790921,1.2776410579681396,-19.004013884776,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Entre_Folhas,2023-12-31T00:00:00,1.3210526315789468,0.994763195514679,-24.69920033952626,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Entre_Rios_de_Minas,2023-12-31T00:00:00,1.56,1.8446698188781736,18.24806531270345,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ervalia,2023-12-31T00:00:00,1.380044843049327,1.26285719871521,-8.491582351424247,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Espera_Feliz,2023-12-31T00:00:00,1.08,1.1005728244781494,1.9048911553841985,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Faria_Lemos,2023-12-31T00:00:00,1.380152671755725,0.982911229133606,-28.78242753512036,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ferros,2023-12-31T00:00:00,1.17948717948718,0.8540142178535461,-27.59444674719938,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Fervedouro,2023-12-31T00:00:00,1.2,1.1976430416107178,-0.1964131991068485,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Guape,2023-12-31T00:00:00,1.560674157303371,1.5374248027801514,-1.4896994618909558,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ibituruna,2023-12-31T00:00:00,1.8000000000000005,1.4369738101959229,-20.16812165578208,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ilicinea,2023-12-31T00:00:00,1.560032362459547,1.5186172723770142,-2.654758393424462,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Itapecerica,2023-12-31T00:00:00,0.9037974683544304,1.3217988014221191,46.249447216172825,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ituiutaba,2023-12-31T00:00:00,1.0,2.1678247451782227,116.78247451782228,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Itumirim,2023-12-31T00:00:00,1.532033426183844,1.3079127073287964,-14.628970557993108,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Lajinha,2023-12-31T00:00:00,1.4399536768963517,1.2273147106170654,-14.767069919749373,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Lavras,2023-12-31T00:00:00,1.5,1.3007993698120115,-13.280042012532553,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Luisburgo,2023-12-31T00:00:00,1.44,1.5916215181350708,10.529272092713256,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Machado,2023-12-31T00:00:00,1.500560931145702,1.309012532234192,-12.765119691958118,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Manhuacu,2023-12-31T00:00:00,1.2,1.2520569562911987,4.338079690933232,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Manhumirim,2023-12-31T00:00:00,1.44,1.403836965560913,-2.5113218360476988,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Martins_Soares,2023-12-31T00:00:00,1.2,1.4521712064743042,21.014267206192024,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Matipo,2023-12-31T00:00:00,1.25990675990676,1.189055562019348,-5.623527084865802,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Medeiros,2023-12-31T00:00:00,1.86,1.5211293697357178,-18.21885108947753,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Miradouro,2023-12-31T00:00:00,1.140084388185654,1.157073736190796,1.490183374248203,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Mirai,2023-12-31T00:00:00,1.08,1.06923246383667,-0.996994089197236,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Moeda,2023-12-31T00:00:00,1.0,1.33962881565094,33.962881565093994,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Muriae,2023-12-31T00:00:00,1.057297297297297,1.0200021266937256,-3.527406217617945,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Mutum,2023-12-31T00:00:00,1.019985196150999,1.3930301666259766,36.57356713437554,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Nazareno,2023-12-31T00:00:00,1.5599056603773582,1.5051231384277344,-3.51191250478387,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Nepomuceno,2023-12-31T00:00:00,1.650045578851413,1.152739405632019,-30.138935529621303,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Oliveira,2023-12-31T00:00:00,1.8240227434257288,1.5411216020584106,-15.509737605354449,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Orizania,2023-12-31T00:00:00,1.2,1.398938536643982,16.578211386998497,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Paraguacu,2023-12-31T00:00:00,1.298291457286432,1.1486698389053345,-11.524501493202678,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Passa_Tempo,2023-12-31T00:00:00,1.625,1.5516037940979004,-4.516689593975361,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Perdoes,2023-12-31T00:00:00,1.6799401197604789,1.454058289527893,-13.44582628723644,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Pimenta,2023-12-31T00:00:00,1.504186046511628,1.6002908945083618,6.38915965346252,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Piranga,2023-12-31T00:00:00,2.1,2.035997152328491,-3.047754651024232,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Piumhi,2023-12-31T00:00:00,1.7399633363886338,1.4116326570510864,-18.86997688232968,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ponte_Nova,2023-12-31T00:00:00,1.333333333333333,1.2370585203170776,-7.220610976219157,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Porto_Firme,2023-12-31T00:00:00,1.200764818355641,1.249842405319214,4.087193946170162,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Raul_Soares,2023-12-31T00:00:00,1.5,1.0415935516357422,-30.56042989095052,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Reduto,2023-12-31T00:00:00,0.9,1.369870901107788,52.20787790086534,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ribeirao_Vermelho,2023-12-31T00:00:00,1.5,1.280620813369751,-14.625279108683268,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ritapolis,2023-12-31T00:00:00,2.102222222222222,1.7109930515289309,-18.6102671048606,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Rosario_da_Limeira,2023-12-31T00:00:00,1.2,0.9701551198959352,-19.15374000867208,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Santa_Barbara_do_Leste,2023-12-31T00:00:00,1.08,1.163867712020874,7.765528890821661,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Santa_Margarida,2023-12-31T00:00:00,1.32,1.3057204484939575,-1.081784205003223,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Santa_Rita_de_Minas,2023-12-31T00:00:00,1.019930675909879,1.2207796573638916,19.69241500407226,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Santa_Rita_do_Itueto,2023-12-31T00:00:00,1.2,1.4180384874343872,18.169873952865608,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Santana_da_Vargem,2023-12-31T00:00:00,1.5593593593593589,1.299329400062561,-16.67543518664149,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Santana_do_Jacare,2023-12-31T00:00:00,0.9011235955056182,1.2510414123535156,38.831278927011056,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Santana_do_Manhuacu,2023-12-31T00:00:00,1.32,1.37238609790802,3.968643780910602,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Domingos_das_Dores,2023-12-31T00:00:00,1.296078431372549,1.30569326877594,0.7418407073720635,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Francisco_de_Paula,2023-12-31T00:00:00,1.56,1.5333373546600342,-1.7091439320490942,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Francisco_do_Gloria,2023-12-31T00:00:00,1.08,0.936914086341858,-13.248695709087237,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Joao_do_Manhuacu,2023-12-31T00:00:00,1.2,1.3627450466156006,13.56208721796672,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Jose_do_Mantimento,2023-12-31T00:00:00,1.5,1.4182816743850708,-5.447888374328613,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Roque_de_Minas,2023-12-31T00:00:00,1.8000000000000005,1.4354088306427002,-20.255064964294444,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Tiago,2023-12-31T00:00:00,1.560526315789474,1.72599196434021,10.603195016741934,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Tomas_de_Aquino,2023-12-31T00:00:00,1.319968346082828,1.413960337638855,7.120776178834898,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Senador_Firmino,2023-12-31T00:00:00,1.436170212765957,1.579782247543335,9.99965279190632,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Senhora_de_Oliveira,2023-12-31T00:00:00,1.2,1.6968183517456057,41.4015293121338,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sericita,2023-12-31T00:00:00,1.26,1.2812005281448364,1.6825815987965411,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Simonesia,2023-12-31T00:00:00,1.5,1.2863082885742188,-14.246114095052084,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Tapirai,2023-12-31T00:00:00,1.5579665220086798,1.5094141960144043,-3.116390840778606,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Tombos,2023-12-31T00:00:00,1.32,0.9869746565818788,-25.229192683191016,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Tres_Pontas,2023-12-31T00:00:00,1.5900000000000003,1.3455116748809814,-15.376624221321938,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ubaporanga,2023-12-31T00:00:00,1.5,1.2111090421676636,-19.259397188822422,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Vargem_Bonita,2023-12-31T00:00:00,1.487234042553192,1.731287956237793,16.409919804257854,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Vermelho_Novo,2023-12-31T00:00:00,1.5,1.073339581489563,-28.4440279006958,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Vicosa,2023-12-31T00:00:00,1.5004329004329,1.1797562837600708,-21.372273067346672,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Vieiras,2023-12-31T00:00:00,1.5,1.0384875535964966,-30.767496426900227,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Aguanil,2024-12-31T00:00:00,2.025039123630673,1.4976216554641724,-26.044803876228272,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Alfenas,2024-12-31T00:00:00,1.635197497066875,1.1201523542404177,-31.497427298556683,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Alto_Caparao,2024-12-31T00:00:00,1.14,1.2774362564086914,12.055811965674694,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Alto_Jequitiba,2024-12-31T00:00:00,1.02,0.8915668725967407,-12.591483078750912,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Amparo_do_Serra,2024-12-31T00:00:00,1.680555555555556,1.2560747861862185,-25.258359830241577,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Araponga,2024-12-31T00:00:00,1.5,1.123197078704834,-25.120194753011067,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Astolfo_Dutra,2024-12-31T00:00:00,0.8749999999999999,0.7562084794044495,-13.576173782348622,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Bambui,2024-12-31T00:00:00,1.8000000000000005,1.392444372177124,-22.64197932349312,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Boa_Esperanca,2024-12-31T00:00:00,1.083893395133256,0.8936373591423035,-17.55302106694378,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Bom_Jesus_do_Galho,2024-12-31T00:00:00,0.9,0.9443230032920836,4.924778143564858,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Bom_Sucesso,2024-12-31T00:00:00,1.6799426934097417,1.299458384513855,-22.6486480990388,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Caete,2024-12-31T00:00:00,1.8000000000000005,1.117273449897766,-37.92925278345744,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Caiana,2024-12-31T00:00:00,1.14,0.8494699001312256,-25.485096479717047,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Cajuri,2024-12-31T00:00:00,1.32,0.8757407665252686,-33.65600253596451,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Camacho,2024-12-31T00:00:00,1.5,1.3104103803634644,-12.639307975769045,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Campo_do_Meio,2024-12-31T00:00:00,1.372340425531915,0.8812739849090576,-35.783135983371,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Campos_Altos,2024-12-31T00:00:00,1.5,1.0724608898162842,-28.50260734558105,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Campos_Gerais,2024-12-31T00:00:00,1.611315789473684,1.245069980621338,-22.729610871123885,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Cana_Verde,2024-12-31T00:00:00,1.2,1.0528564453125,-12.261962890624996,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Canaa,2024-12-31T00:00:00,1.319791666666667,1.2288771867752075,-6.888547805509155,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Candeias,2024-12-31T00:00:00,1.14003294892916,0.885614812374115,-22.316735388571143,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Caparao,2024-12-31T00:00:00,1.6799283154121862,1.124325394630432,-33.07301363305087,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Capitolio,2024-12-31T00:00:00,1.32,1.035311579704285,-21.567304567857228,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Caputira,2024-12-31T00:00:00,1.38008658008658,1.2045695781707764,-12.7178254211263,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Carangola,2024-12-31T00:00:00,1.44,0.9910701513290404,-31.175683935483296,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Caratinga,2024-12-31T00:00:00,1.6200335289186922,1.1162549257278442,-31.09680103516747,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Carmo_da_Mata,2024-12-31T00:00:00,1.62,1.1087422370910645,-31.559121167218247,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Carmo_do_Rio_Claro,2024-12-31T00:00:00,1.7634782608695652,1.4580003023147583,-17.32246806400532,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Carmopolis_de_Minas,2024-12-31T00:00:00,1.98,1.4761523008346558,-25.4468534931992,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Cassia,2024-12-31T00:00:00,1.7162790697674415,1.2980282306671145,-24.369628836468944,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Catas_Altas_da_Noruega,2024-12-31T00:00:00,0.9,0.7937273979187012,-11.808066897922094,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Chale,2024-12-31T00:00:00,1.320168067226891,1.214467167854309,-8.006624459158013,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Claudio,2024-12-31T00:00:00,2.699367088607595,1.199070692062378,-55.57956170085446,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Conceicao_da_Barra_de_Minas,2024-12-31T00:00:00,1.8000000000000005,1.38468599319458,-23.07300037807889,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Conceicao_de_Ipanema,2024-12-31T00:00:00,1.2901734104046243,1.309104561805725,1.46733386755844,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Coqueiral,2024-12-31T00:00:00,0.9,1.0888147354125977,20.97941504584418,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Corrego_Danta,2024-12-31T00:00:00,1.5,1.1809728145599363,-21.268479029337563,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Cristais,2024-12-31T00:00:00,1.439664804469274,1.0434889793395996,-27.51861571525483,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Desterro_de_Entre_Rios,2024-12-31T00:00:00,1.3780487804878048,1.361541748046875,-1.1978554514657005,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Divinesia,2024-12-31T00:00:00,1.316666666666667,1.7012972831726074,29.21245188652708,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Divino,2024-12-31T00:00:00,1.2,0.9148675203323364,-23.76103997230529,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Doresopolis,2024-12-31T00:00:00,1.5615384615384622,1.2204017639160156,-21.846192458580312,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Durande,2024-12-31T00:00:00,1.380095693779904,1.149876356124878,-16.68140395572752,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Eloi_Mendes,2024-12-31T00:00:00,1.421623345558922,0.9755967855453492,-31.37445381766815,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Entre_Folhas,2024-12-31T00:00:00,1.2,0.9323149919509888,-22.307084004084267,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Entre_Rios_de_Minas,2024-12-31T00:00:00,1.88,1.5996021032333374,-14.91478174290758,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ervalia,2024-12-31T00:00:00,1.32,1.066259264945984,-19.222782958637588,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Espera_Feliz,2024-12-31T00:00:00,1.44,0.7482606172561646,-48.03745713498857,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Faria_Lemos,2024-12-31T00:00:00,1.37956204379562,0.8989023566246033,-34.84146938752873,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ferros,2024-12-31T00:00:00,1.205128205128205,0.8585613369941711,-28.757676291973027,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Fervedouro,2024-12-31T00:00:00,1.56,1.1298534870147705,-27.573494422130096,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Guape,2024-12-31T00:00:00,1.7459340659340663,1.2138113975524902,-30.47782151480577,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ibituruna,2024-12-31T00:00:00,1.8000000000000005,1.329936146736145,-26.114658514658625,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ilicinea,2024-12-31T00:00:00,1.5,1.0640612840652466,-29.06258106231689,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Itapecerica,2024-12-31T00:00:00,1.615189873417721,0.8286309838294983,-48.697611502719134,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ituiutaba,2024-12-31T00:00:00,1.0,0.5496963262557983,-45.030367374420166,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Itumirim,2024-12-31T00:00:00,1.62116991643454,1.2236489057540894,-24.520625916543263,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Lajinha,2024-12-31T00:00:00,1.564997053624043,1.0945693254470823,-30.05933634747728,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Lavras,2024-12-31T00:00:00,1.55996015936255,1.150206446647644,-26.266934463399664,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Luisburgo,2024-12-31T00:00:00,1.2,1.365994930267334,13.83291085561117,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Machado,2024-12-31T00:00:00,1.3347736360118658,1.5008316040039062,12.440908593924627,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Manhuacu,2024-12-31T00:00:00,1.5,0.984241545200348,-34.38389698664348,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Manhumirim,2024-12-31T00:00:00,1.56,1.2284337282180786,-21.254248191148807,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Martins_Soares,2024-12-31T00:00:00,1.2,1.1730066537857056,-2.249445517857866,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Matipo,2024-12-31T00:00:00,1.2,1.136294960975647,-5.308753252029415,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Medeiros,2024-12-31T00:00:00,1.8600609756097564,1.286095142364502,-30.8573665471961,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Miradouro,2024-12-31T00:00:00,1.13996383363472,1.0024878978729248,-12.059675202454429,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Mirai,2024-12-31T00:00:00,1.2,1.046925663948059,-12.756194670995075,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Moeda,2024-12-31T00:00:00,1.5,0.9340468645095824,-37.7302090326945,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Muriae,2024-12-31T00:00:00,1.170285714285714,0.8708305358886719,-25.58821104466913,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Mutum,2024-12-31T00:00:00,1.15997150997151,1.1438491344451904,-1.389894095477875,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Nazareno,2024-12-31T00:00:00,1.619811320754717,1.335561990737915,-17.54829876632557,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Nepomuceno,2024-12-31T00:00:00,1.3800383877159308,0.9509443044662476,-31.09290923130528,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Oliveira,2024-12-31T00:00:00,1.8240227434257288,1.454431653022766,-20.262416778248458,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Orizania,2024-12-31T00:00:00,1.5,1.191662311553955,-20.555845896402992,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Paraguacu,2024-12-31T00:00:00,1.323316582914573,0.8618195056915283,-34.87427598062804,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Passa_Tempo,2024-12-31T00:00:00,1.5,1.4459800720214844,-3.601328531901041,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Perdoes,2024-12-31T00:00:00,1.6799401197604789,1.2858431339263916,-23.45899006747196,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Pimenta,2024-12-31T00:00:00,1.715219421101774,1.0903059244155884,-36.43344338328279,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Piranga,2024-12-31T00:00:00,1.8000000000000005,1.7933080196380615,-0.3717766867743746,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Piumhi,2024-12-31T00:00:00,1.3800316957210783,1.2415552139282229,-10.034297199275596,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ponte_Nova,2024-12-31T00:00:00,2.0,1.1723010540008545,-41.38494729995726,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Porto_Firme,2024-12-31T00:00:00,1.3810526315789473,1.113183856010437,-19.39598603582964,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Raul_Soares,2024-12-31T00:00:00,1.5,1.033875584602356,-31.0749610265096,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Reduto,2024-12-31T00:00:00,1.2,0.9486318230628968,-20.94734807809193,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ribeirao_Vermelho,2024-12-31T00:00:00,1.43859649122807,1.1714448928833008,-18.57029403128274,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ritapolis,2024-12-31T00:00:00,1.8000000000000005,1.701365947723389,-5.479669570922866,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Rosario_da_Limeira,2024-12-31T00:00:00,1.259493670886076,0.9563111066818236,-24.0717814795336,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Santa_Barbara_do_Leste,2024-12-31T00:00:00,1.4101226993865028,1.0445849895477295,-25.92240448280185,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Santa_Margarida,2024-12-31T00:00:00,1.5,1.0610495805740356,-29.26336129506429,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Santa_Rita_de_Minas,2024-12-31T00:00:00,1.200173310225303,1.026755452156067,-14.449401314938518,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Santa_Rita_do_Itueto,2024-12-31T00:00:00,1.365238095238095,1.2421462535858154,-9.016144662357412,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Santana_da_Vargem,2024-12-31T00:00:00,1.44,1.0500930547714231,-27.07687119642893,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Santana_do_Jacare,2024-12-31T00:00:00,1.2,0.8829083442687988,-26.42430464426676,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Santana_do_Manhuacu,2024-12-31T00:00:00,1.08,1.239319920539856,14.7518444944311,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Domingos_das_Dores,2024-12-31T00:00:00,1.41,1.1951544284820557,-15.237274575740742,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Francisco_de_Paula,2024-12-31T00:00:00,1.62,1.3999696969985962,-13.582117469222462,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Francisco_do_Gloria,2024-12-31T00:00:00,1.25,0.7707993388175964,-38.336052894592285,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Joao_do_Manhuacu,2024-12-31T00:00:00,1.32,1.0181511640548706,-22.8673360564492,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Jose_do_Mantimento,2024-12-31T00:00:00,1.44,1.41404128074646,-1.802688837051388,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Roque_de_Minas,2024-12-31T00:00:00,1.58,1.2185626029968262,-22.87578462045404,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Tiago,2024-12-31T00:00:00,1.621052631578947,1.183126449584961,-27.01492681131733,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sao_Tomas_de_Aquino,2024-12-31T00:00:00,1.619975932611312,1.1442254781723022,-29.367748301798912,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Senador_Firmino,2024-12-31T00:00:00,1.804347826086957,1.308121919631958,-27.501676743289096,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Senhora_de_Oliveira,2024-12-31T00:00:00,1.8000000000000005,1.2828633785247805,-28.72981230417889,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Sericita,2024-12-31T00:00:00,1.3799999999999997,0.8614134192466736,-37.57873773574828,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Simonesia,2024-12-31T00:00:00,1.2,1.1981918811798096,-0.1506765683491988,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Tapirai,2024-12-31T00:00:00,1.9163050216986977,1.3142913579940796,-31.415336122793576,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Tombos,2024-12-31T00:00:00,1.5,0.9886654019355774,-34.088973204294845,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Tres_Pontas,2024-12-31T00:00:00,1.387228956803825,1.0735870599746704,-22.609238027425945,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Ubaporanga,2024-12-31T00:00:00,1.440056417489422,1.3093093633651731,-9.079300820185331,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Vargem_Bonita,2024-12-31T00:00:00,2.187804878048781,1.2196624279022217,-44.251773083622005,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Vermelho_Novo,2024-12-31T00:00:00,1.8000000000000005,1.041327953338623,-42.14844703674318,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Vicosa,2024-12-31T00:00:00,1.32,0.9020501971244812,-31.662863854205973,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_0_2024,Vieiras,2024-12-31T00:00:00,1.44,0.8967115879058838,-37.728361950980286,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:03
V42_cluster_1_2024,Almenara,2023-12-31T00:00:00,0.78,0.6456568837165833,-17.223476446591892,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Alpinopolis,2023-12-31T00:00:00,1.8000000000000005,1.280978441238403,-28.83453104231093,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Alterosa,2023-12-31T00:00:00,1.2603960396039595,1.01345956325531,-19.59197494989288,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Andradas,2023-12-31T00:00:00,1.22,1.0865310430526731,-10.94007843830546,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Araguari,2023-12-31T00:00:00,3.0,2.213229179382324,-26.22569402058919,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Arapua,2023-12-31T00:00:00,0.9,1.1696375608444214,29.95972898271349,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Araxa,2023-12-31T00:00:00,1.638401296246287,1.0600879192352295,-35.29741940121882,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Arceburgo,2023-12-31T00:00:00,1.8482558139534884,0.7556077241897583,-59.11779535682968,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Areado,2023-12-31T00:00:00,1.590034364261168,1.0314278602600098,-35.13172523543054,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Bom_Jesus_da_Penha,2023-12-31T00:00:00,1.2,1.3997178077697754,16.643150647481285,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Botelhos,2023-12-31T00:00:00,1.080052666227781,1.0470428466796875,-3.056315731656344,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Botumirim,2023-12-31T00:00:00,0.59375,0.6172776222229004,3.962546900699013,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Cabo_Verde,2023-12-31T00:00:00,1.319976635514019,1.2359330654144287,-6.367049827882934,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Campestre,2023-12-31T00:00:00,1.319964428634949,1.1008068323135376,-16.60329563184175,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Capetinga,2023-12-31T00:00:00,1.4484046164290565,1.462744116783142,0.9900203431717092,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Carmo_do_Paranaiba,2023-12-31T00:00:00,2.369801007771799,1.4995368719100952,-36.72308911202498,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Cascalho_Rico,2023-12-31T00:00:00,2.4,2.42920446395874,1.2168526649475135,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Claraval,2023-12-31T00:00:00,1.8600214362272245,1.4960178136825562,-19.56986169379828,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Conceicao_da_Aparecida,2023-12-31T00:00:00,1.679948420373952,1.4657137393951416,-12.752455871896496,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Coromandel,2023-12-31T00:00:00,2.0926575541308825,1.3499677181243896,-35.49027094951257,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Corrego_Fundo,2023-12-31T00:00:00,1.320158102766798,1.3237732648849487,0.2738431185305867,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Cruzeiro_da_Fortaleza,2023-12-31T00:00:00,1.5,1.6616255044937134,10.775033632914226,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Delfinopolis,2023-12-31T00:00:00,1.501818181818182,1.3517630100250244,-9.991567129084338,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Divisa_Nova,2023-12-31T00:00:00,1.680092592592593,1.2409420013427734,-26.13847553319399,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Dom_Cavati,2023-12-31T00:00:00,0.6000000000000001,0.3769008219242096,-37.18319634596507,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Dores_do_Turvo,2023-12-31T00:00:00,1.5,0.7961935997009277,-46.92042668660481,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Estrela_do_Indaia,2023-12-31T00:00:00,1.8000000000000005,1.7737619876861572,-1.4576673507690576,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Estrela_do_Sul,2023-12-31T00:00:00,2.43015873015873,1.3469505310058594,-44.573557509229815,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Guaranesia,2023-12-31T00:00:00,1.1961439588688951,0.8794438242912292,-26.47675743621577,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Guarani,2023-12-31T00:00:00,0.9655172413793104,0.5000646710395813,-48.20758764232908,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Guarda-Mor,2023-12-31T00:00:00,1.7994350282485878,1.7283848524093628,-3.9484713103745137,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Guaxupe,2023-12-31T00:00:00,1.2,0.8679072260856628,-27.674397826194763,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Guimarania,2023-12-31T00:00:00,1.7704600484261497,1.225148320198059,-30.800566706537403,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Ibia,2023-12-31T00:00:00,1.668,1.1020090579986572,-33.932310671543334,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Ibiraci,2023-12-31T00:00:00,1.804169884169884,1.3021917343139648,-27.82321965688304,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Indianopolis,2023-12-31T00:00:00,2.7000000000000006,1.7381114959716797,-35.625500149197066,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Itamogi,2023-12-31T00:00:00,1.740011254924029,1.137293815612793,-34.63870923855325,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Itanhomi,2023-12-31T00:00:00,1.078787878787879,0.7792990207672119,-27.761607625511274,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Jacui,2023-12-31T00:00:00,1.32,0.865382194519043,-34.440742839466445,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Jacutinga,2023-12-31T00:00:00,1.160053262316911,1.1771152019500732,1.4707893324730004,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Juruaia,2023-12-31T00:00:00,1.5,1.399946928024292,-6.670204798380534,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Lagoa_Dourada,2023-12-31T00:00:00,1.8000000000000005,1.1407277584075928,-36.62623564402263,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Lagoa_Formosa,2023-12-31T00:00:00,1.978723404255319,2.4432876110076904,23.47797604017361,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Lagoa_Grande,2023-12-31T00:00:00,2.4,0.4241918921470642,-82.32533782720566,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Matutina,2023-12-31T00:00:00,1.502024291497976,1.7927483320236206,19.35548194335154,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Monte_Belo,2023-12-31T00:00:00,1.3799999999999997,0.9973406791687012,-27.728936292123084,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Monte_Carmelo,2023-12-31T00:00:00,2.4900284900284904,1.8999712467193604,-23.6968069109273,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Monte_Santo_de_Minas,2023-12-31T00:00:00,1.620050377833753,0.9203519821166992,-43.18991589956937,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Muzambinho,2023-12-31T00:00:00,1.68,1.0730208158493042,-36.12971334230332,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Nova_Resende,2023-12-31T00:00:00,1.56,1.2154422998428345,-22.08703206135677,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Passos,2023-12-31T00:00:00,1.829801324503311,0.8474311828613281,-53.68725710746993,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Patis,2023-12-31T00:00:00,1.2,1.5814659595489502,31.788829962412525,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Patos_de_Minas,2023-12-31T00:00:00,1.782452316076294,1.7604703903198242,-1.2332406066748798,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Patrocinio,2023-12-31T00:00:00,2.4370275910039414,0.9450619220733644,-61.220713070218345,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Pedrinopolis,2023-12-31T00:00:00,2.187804878048781,2.097585201263428,-4.123753342474363,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Perdizes,2023-12-31T00:00:00,2.106451612903226,1.1763912439346311,-44.15294247783527,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Piracema,2023-12-31T00:00:00,1.2,1.0393991470336914,-13.383404413859044,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Ponto_dos_Volantes,2023-12-31T00:00:00,0.6000000000000001,0.5579929351806641,-7.001177469889337,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Pratapolis,2023-12-31T00:00:00,1.8011363636363642,0.8566186428070068,-52.44010058863308,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Pratinha,2023-12-31T00:00:00,2.13,1.4132459163665771,-33.65042646166304,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Presidente_Kubitschek,2023-12-31T00:00:00,0.8947368421052632,0.5619125366210938,-37.198010612936585,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Presidente_Olegario,2023-12-31T00:00:00,2.160074626865672,1.913296222686768,-11.4245314078332,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Quartel_Geral,2023-12-31T00:00:00,3.0,2.719879388809204,-9.337353706359863,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Rio_Paranaiba,2023-12-31T00:00:00,1.8000000000000005,1.1312005519866943,-37.1555248896281,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Rio_Vermelho,2023-12-31T00:00:00,0.9,0.782867968082428,-13.01467021306356,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Romaria,2023-12-31T00:00:00,2.131578947368421,2.3432743549346924,9.93138949076336,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Sacramento,2023-12-31T00:00:00,1.75,1.4190618991851809,-18.910748617989675,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Santa_Rosa_da_Serra,2023-12-31T00:00:00,1.68,1.9071156978607176,13.518791539328442,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Santo_Antonio_do_Amparo,2023-12-31T00:00:00,1.8000000000000005,1.1186658143997192,-37.85189920001561,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Sao_Goncalo_do_Abaete,2023-12-31T00:00:00,1.619047619047619,1.4312164783477783,-11.601335160872516,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Sao_Gotardo,2023-12-31T00:00:00,1.5,1.199964165687561,-20.0023889541626,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Sao_Joao_Batista_do_Gloria,2023-12-31T00:00:00,1.5232974910394272,1.0395870208740234,-31.75416968850532,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Sao_Pedro_da_Uniao,2023-12-31T00:00:00,1.5,1.2890304327011108,-14.064637819925943,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Sao_Sebastiao_do_Paraiso,2023-12-31T00:00:00,1.440017746228926,1.140647292137146,-20.789358664208272,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Serra_do_Salitre,2023-12-31T00:00:00,1.489636363636364,1.2289777994155884,-17.498133811964678,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Tapira,2023-12-31T00:00:00,1.92,0.8395504355430603,-56.27341481546561,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Tiros,2023-12-31T00:00:00,2.382038834951457,1.2275971174240112,-48.46443729583325,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Tupaciguara,2023-12-31T00:00:00,2.622222222222222,2.052818536758423,-21.714547327009285,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Uberaba,2023-12-31T00:00:00,2.1200980392156863,2.014268159866333,-4.991744598212271,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Uberlandia,2023-12-31T00:00:00,1.8000000000000005,1.6117539405822754,-10.458114412095824,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Unai,2023-12-31T00:00:00,2.520095187731359,2.595787763595581,3.003560192199017,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Varginha,2023-12-31T00:00:00,1.679990280646337,0.9869694113731384,-41.25148087205451,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Varjao_de_Minas,2023-12-31T00:00:00,2.3892857142857142,2.348015785217285,-1.727291500621845,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Virginopolis,2023-12-31T00:00:00,1.3258426966292132,1.3792455196380615,4.027840040497885,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Almenara,2024-12-31T00:00:00,0.78,0.9664465188980104,23.90339985871926,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Alpinopolis,2024-12-31T00:00:00,2.08,1.4174556732177734,-31.85309263376089,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Alterosa,2024-12-31T00:00:00,1.110028116213683,1.2456984519958496,12.222243184698742,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Andradas,2024-12-31T00:00:00,1.26,0.8312771320343018,-34.02562444172209,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Araguari,2024-12-31T00:00:00,1.500037950664137,2.525150775909424,68.3391260062068,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Arapua,2024-12-31T00:00:00,1.0805194805194802,1.5435068607330322,42.84859167841768,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Araxa,2024-12-31T00:00:00,1.412283279459301,1.6773260831832886,18.76697172435975,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Arceburgo,2024-12-31T00:00:00,1.7586206896551722,1.109229564666748,-36.92616200914569,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Areado,2024-12-31T00:00:00,1.162105263157895,1.56141197681427,34.36063206282212,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Bom_Jesus_da_Penha,2024-12-31T00:00:00,1.56,1.274317979812622,-18.31295001201141,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Botelhos,2024-12-31T00:00:00,1.08,0.8602069020271301,-20.351212775265733,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Botumirim,2024-12-31T00:00:00,0.7250000000000001,0.6399440765380859,-11.731851511988156,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Cabo_Verde,2024-12-31T00:00:00,1.32,1.27147376537323,-3.67622989596743,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Campestre,2024-12-31T00:00:00,1.320032051282051,1.1295537948608398,-14.429820566569838,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Capetinga,2024-12-31T00:00:00,1.437691001697793,1.51262629032135,5.212197094860072,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Carmo_do_Paranaiba,2024-12-31T00:00:00,2.103206239168111,2.1690497398376465,3.130625016383511,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Cascalho_Rico,2024-12-31T00:00:00,1.8000000000000005,2.4795403480529785,37.75224155849878,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Claraval,2024-12-31T00:00:00,1.92,1.4941104650497437,-22.18174661199252,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Conceicao_da_Aparecida,2024-12-31T00:00:00,1.5,1.5998291969299316,6.655279795328776,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Coromandel,2024-12-31T00:00:00,1.6144686299615878,2.262571334838867,40.14340649608653,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Corrego_Fundo,2024-12-31T00:00:00,1.501960784313725,1.020533323287964,-32.05326437638881,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Cruzeiro_da_Fortaleza,2024-12-31T00:00:00,2.22,1.9429807662963867,-12.478343860523111,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Delfinopolis,2024-12-31T00:00:00,1.5,1.310033082962036,-12.664461135864258,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Divisa_Nova,2024-12-31T00:00:00,1.679831932773109,1.4864859580993652,-11.509840413294404,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Dom_Cavati,2024-12-31T00:00:00,0.6000000000000001,1.145787000656128,90.96450010935465,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Dores_do_Turvo,2024-12-31T00:00:00,1.2,1.6787757873535156,39.89798227945964,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Estrela_do_Indaia,2024-12-31T00:00:00,1.56,1.869820237159729,19.86027161280314,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Estrela_do_Sul,2024-12-31T00:00:00,1.7099999999999995,2.300461769104004,34.529928017778026,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Guaranesia,2024-12-31T00:00:00,1.205714285714286,0.9773988127708436,-18.936117424219155,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Guarani,2024-12-31T00:00:00,0.9655172413793104,1.777066707611084,84.05333757400511,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Guarda-Mor,2024-12-31T00:00:00,1.740112994350282,2.080869197845459,19.58241818787219,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Guaxupe,2024-12-31T00:00:00,1.32,0.9864122867584229,-25.271796457695245,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Guimarania,2024-12-31T00:00:00,1.8219512195121947,1.8095901012420648,-0.6784549519080346,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Ibia,2024-12-31T00:00:00,1.681621621621622,1.7735682725906372,5.467737200021797,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Ibiraci,2024-12-31T00:00:00,1.978101265822785,1.7600446939468384,-11.023529262302285,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Indianopolis,2024-12-31T00:00:00,1.8000000000000005,2.315596342086792,28.64424122704398,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Itamogi,2024-12-31T00:00:00,1.44,1.4282658100128174,-0.8148743046654559,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Itanhomi,2024-12-31T00:00:00,1.02089552238806,1.4391653537750244,40.970875296676326,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Jacui,2024-12-31T00:00:00,1.3799999999999997,1.03623366355896,-24.91060408993042,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Jacutinga,2024-12-31T00:00:00,1.319874804381847,1.008864402770996,-23.56362895771089,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Juruaia,2024-12-31T00:00:00,1.68,1.3970191478729248,-16.84409834089733,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Lagoa_Dourada,2024-12-31T00:00:00,1.5,1.7289661169052124,15.264407793680826,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Lagoa_Formosa,2024-12-31T00:00:00,1.8142857142857145,2.1630196571350098,19.221555905079267,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Lagoa_Grande,2024-12-31T00:00:00,2.4,0.8232060670852661,-65.69974720478058,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Matutina,2024-12-31T00:00:00,1.380566801619433,1.910905361175537,38.41455255435711,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Monte_Belo,2024-12-31T00:00:00,1.5,1.1566212177276611,-22.89191881815592,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Monte_Carmelo,2024-12-31T00:00:00,1.56,2.252030849456787,44.36095188825558,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Monte_Santo_de_Minas,2024-12-31T00:00:00,1.5,1.2592840194702148,-16.047732035319008,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Muzambinho,2024-12-31T00:00:00,1.26,1.2921597957611084,2.55236474294511,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Nova_Resende,2024-12-31T00:00:00,1.32,1.541696310043335,16.795175003282946,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Passos,2024-12-31T00:00:00,1.95459940652819,1.028187274932861,-47.39652168629509,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Patis,2024-12-31T00:00:00,0.9,1.924362063407898,113.818007045322,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Patos_de_Minas,2024-12-31T00:00:00,1.6624523160762943,1.8614705801010127,11.971366763435364,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Patrocinio,2024-12-31T00:00:00,1.4995330375904743,1.5948607921600342,6.357162675304397,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Pedrinopolis,2024-12-31T00:00:00,1.8511627906976744,2.038271427154541,10.107627346288032,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Perdizes,2024-12-31T00:00:00,1.142806076854334,2.0796542167663574,81.97787523940838,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Piracema,2024-12-31T00:00:00,0.8,1.322456121444702,65.30701518058775,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Ponto_dos_Volantes,2024-12-31T00:00:00,0.5,0.6170865297317505,23.417305946350098,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Pratapolis,2024-12-31T00:00:00,1.1988636363636362,1.229063630104065,2.519051610576044,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Pratinha,2024-12-31T00:00:00,1.849285714285714,1.6044824123382568,-13.23772200565624,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Presidente_Kubitschek,2024-12-31T00:00:00,0.7368421052631579,1.4113036394119265,91.53406534876144,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Presidente_Olegario,2024-12-31T00:00:00,1.9199630314232905,2.150693416595459,12.017438950432584,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Quartel_Geral,2024-12-31T00:00:00,3.0,3.7706217765808105,25.687392552693684,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Rio_Paranaiba,2024-12-31T00:00:00,1.680014776505357,1.890257835388184,12.514357720124266,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Rio_Vermelho,2024-12-31T00:00:00,0.9230769230769232,1.1756842136383057,27.36578981081644,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Romaria,2024-12-31T00:00:00,2.052631578947369,2.2193074226379395,8.120105205438044,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Sacramento,2024-12-31T00:00:00,1.737021276595745,1.5297105312347412,-11.93484202837722,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Santa_Rosa_da_Serra,2024-12-31T00:00:00,1.92,2.016773700714112,5.040296912193303,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Santo_Antonio_do_Amparo,2024-12-31T00:00:00,1.68,1.5665791034698486,-6.751243841080436,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Sao_Goncalo_do_Abaete,2024-12-31T00:00:00,1.5,2.135162353515625,42.34415690104167,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Sao_Gotardo,2024-12-31T00:00:00,1.668148148148148,1.7539047002792358,5.14082350697019,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Sao_Joao_Batista_do_Gloria,2024-12-31T00:00:00,1.548387096774194,1.1528068780899048,-25.54788912336033,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Sao_Pedro_da_Uniao,2024-12-31T00:00:00,2.04,1.3324007987976074,-34.68623535305846,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Sao_Sebastiao_do_Paraiso,2024-12-31T00:00:00,1.26,1.3002610206604004,3.1953191000317767,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Serra_do_Salitre,2024-12-31T00:00:00,1.5168750000000002,1.458033561706543,-3.879122425609046,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Tapira,2024-12-31T00:00:00,1.98,1.773134469985962,-10.447754041113024,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Tiros,2024-12-31T00:00:00,1.95,2.046483039855957,4.947848197741401,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Tupaciguara,2024-12-31T00:00:00,2.8219895287958106,2.35644006729126,-16.49720726296275,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Uberaba,2024-12-31T00:00:00,2.102941176470588,2.227922916412353,5.943187633594452,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Uberlandia,2024-12-31T00:00:00,1.919565217391304,1.9066202640533447,-0.6743690300635539,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Unai,2024-12-31T00:00:00,2.2801061007957566,2.705917358398437,18.67506329876811,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Varginha,2024-12-31T00:00:00,1.3799999999999997,1.0494272708892822,-23.95454558773316,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Varjao_de_Minas,2024-12-31T00:00:00,2.6876337184424486,2.484867572784424,-7.544411437713798,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_1_2024,Virginopolis,2024-12-31T00:00:00,1.325,1.501043677330017,13.286315270189972,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:44:38
V42_cluster_2_2024,Abre_Campo,2023-12-31T00:00:00,1.8000000000000005,1.3974261283874512,-22.36521508958606,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Aimores,2023-12-31T00:00:00,1.544827586206897,2.048988103866577,32.635390652077504,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Alvarenga,2023-12-31T00:00:00,1.75,1.0049450397491455,-42.57456915719168,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Andrelandia,2023-12-31T00:00:00,1.8000000000000005,1.6333388090133667,-9.258955054812974,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Antonio_Dias,2023-12-31T00:00:00,1.477272727272727,1.874089241027832,26.86142554649942,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Antonio_Prado_de_Minas,2023-12-31T00:00:00,1.4857142857142862,1.5746114253997805,5.983461324985175,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Bocaiuva,2023-12-31T00:00:00,3.0,2.9033260345458984,-3.2224655151367188,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Bom_Jesus_do_Amparo,2023-12-31T00:00:00,1.25,1.7769067287445068,42.15253829956055,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Campo_Belo,2023-12-31T00:00:00,1.5,1.3093736171722412,-12.708425521850586,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Capela_Nova,2023-12-31T00:00:00,1.8000000000000005,2.245108127593994,24.72822931077744,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Capitao_Eneas,2023-12-31T00:00:00,3.0,3.4584994316101074,15.28331438700358,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Casa_Grande,2023-12-31T00:00:00,2.523076923076923,2.7954158782958984,10.793922005630124,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Cataguases,2023-12-31T00:00:00,1.333333333333333,2.864118814468384,114.80891108512884,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Caxambu,2023-12-31T00:00:00,1.316326530612245,1.5954596996307373,21.20546555334283,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Coimbra,2023-12-31T00:00:00,1.8000000000000005,2.0620641708374023,14.55912060207789,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Conselheiro_Pena,2023-12-31T00:00:00,1.320048602673147,1.8639819622039795,41.20555549464977,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Corrego_Novo,2023-12-31T00:00:00,1.8000000000000005,2.109757900238037,17.20877223544649,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Cruzilia,2023-12-31T00:00:00,1.5,1.5184173583984375,1.2278238932291663,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Divisopolis,2023-12-31T00:00:00,1.616883116883117,1.3895100355148315,-14.062431538440146,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Esmeraldas,2023-12-31T00:00:00,2.222222222222222,2.231236219406128,0.4056298732757724,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Espirito_Santo_do_Dourado,2023-12-31T00:00:00,1.5,1.266621708869934,-15.558552742004396,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Eugenopolis,2023-12-31T00:00:00,1.68,2.41477632522583,43.73668602534703,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Felicio_dos_Santos,2023-12-31T00:00:00,3.542087542087541,4.889473915100098,38.039330112616845,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Formiga,2023-12-31T00:00:00,2.021543985637344,1.5050102472305298,-25.55144691763723,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Guaraciaba,2023-12-31T00:00:00,1.5,2.748237133026123,83.2158088684082,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Guiricema,2023-12-31T00:00:00,1.5,2.555654287338257,70.37695248921713,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Iapu,2023-12-31T00:00:00,1.559210526315789,1.9189887046813965,23.07438105973517,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Ijaci,2023-12-31T00:00:00,1.971428571428572,1.3005069494247437,-34.03225618859998,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Imbe_de_Minas,2023-12-31T00:00:00,1.3799999999999997,1.7533361911773682,27.05334718676584,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Inhapim,2023-12-31T00:00:00,2.7000000000000006,2.1608686447143555,-19.96782797354241,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Irai_de_Minas,2023-12-31T00:00:00,2.18825561312608,2.4893836975097656,13.761101883042931,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Itamarati_de_Minas,2023-12-31T00:00:00,1.2,1.4617071151733398,21.80892626444499,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Jequeri,2023-12-31T00:00:00,1.5,1.8532826900482176,23.55217933654785,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Lamim,2023-12-31T00:00:00,1.8000000000000005,3.37247896194458,87.35994233025443,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Mar_de_Espanha,2023-12-31T00:00:00,1.5625,2.640753507614136,69.00822448730469,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Mata_Verde,2023-12-31T00:00:00,1.739700374531835,2.008120536804199,15.42910297668919,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Monte_Alegre_de_Minas,2023-12-31T00:00:00,2.338983050847458,2.5747437477111816,10.0796239963476,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Nova_Era,2023-12-31T00:00:00,2.636363636363636,1.6029919385910034,-39.196857501720544,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Nova_Ponte,2023-12-31T00:00:00,2.6142857142857143,1.7637308835983276,-32.53488423394375,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Paula_Candido,2023-12-31T00:00:00,1.6197080291970802,1.7484073638916016,7.945835445312944,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Pecanha,2023-12-31T00:00:00,2.30379746835443,2.069791316986084,-10.15740986708754,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Pedra_Bonita,2023-12-31T00:00:00,1.08,1.9520384073257449,80.74429697460597,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Pedra_Dourada,2023-12-31T00:00:00,1.2,1.5695244073867798,30.79370061556499,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Pedra_do_Anta,2023-12-31T00:00:00,1.5,1.4505972862243652,-3.2935142517089844,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Piedade_de_Caratinga,2023-12-31T00:00:00,2.4,1.8565682172775269,-22.64299094676971,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Pocrane,2023-12-31T00:00:00,1.377952755905512,1.6119661331176758,16.98268508911131,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Presidente_Bernardes,2023-12-31T00:00:00,1.320588235294118,1.601926565170288,21.30401607080129,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sabinopolis,2023-12-31T00:00:00,1.2,1.23541522026062,2.9512683550516803,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Santa_Barbara,2023-12-31T00:00:00,1.3,1.2323254346847534,-5.2057357934805095,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Santo_Antonio_do_Grama,2023-12-31T00:00:00,1.521739130434783,2.637887477874756,73.34689140319821,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sao_Domingos_do_Prata,2023-12-31T00:00:00,1.2,1.1967926025390625,-0.2672831217447879,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sao_Geraldo,2023-12-31T00:00:00,1.5,1.8704029321670528,24.69352881113688,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sao_Jose_da_Barra,2023-12-31T00:00:00,2.2413698630136984,2.177574396133423,-2.8462712885100347,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sao_Jose_do_Alegre,2023-12-31T00:00:00,1.592307692307692,1.54182767868042,-3.170242401712738,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sao_Miguel_do_Anta,2023-12-31T00:00:00,1.5598870056497178,1.4613357782363892,-6.317843988467646,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sao_Sebastiao_da_Vargem_Alegre,2023-12-31T00:00:00,1.5,1.5867812633514404,5.785417556762695,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sao_Sebastiao_do_Anta,2023-12-31T00:00:00,1.7998212689901698,1.9114502668380733,6.202226841698336,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Senhora_dos_Remedios,2023-12-31T00:00:00,1.5,1.7739694118499756,18.26462745666504,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Teixeiras,2023-12-31T00:00:00,1.5007092198581562,1.5765239000320437,5.051923395329916,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Visconde_do_Rio_Branco,2023-12-31T00:00:00,1.333333333333333,1.6500859260559082,23.756444454193144,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Abre_Campo,2024-12-31T00:00:00,1.32,1.9594688415527344,48.44460920854047,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Aimores,2024-12-31T00:00:00,0.9455882352941176,2.765413761138916,192.45433243770808,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Alvarenga,2024-12-31T00:00:00,1.25,2.424288272857666,93.94306182861328,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Andrelandia,2024-12-31T00:00:00,1.5,2.2281816005706787,48.54544003804524,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Antonio_Dias,2024-12-31T00:00:00,1.5,2.479714870452881,65.31432469685873,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Antonio_Prado_de_Minas,2024-12-31T00:00:00,1.933333333333333,2.48228120803833,28.39385558818952,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Bocaiuva,2024-12-31T00:00:00,2.0989010989010994,2.667483329772949,27.089519900177127,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Bom_Jesus_do_Amparo,2024-12-31T00:00:00,0.75,2.016921281814575,168.92283757527667,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Campo_Belo,2024-12-31T00:00:00,1.2,2.1349098682403564,77.90915568669638,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Capela_Nova,2024-12-31T00:00:00,2.101234567901236,2.3596231937408447,12.296991006467875,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Capitao_Eneas,2024-12-31T00:00:00,3.0,6.173920631408691,105.79735438028972,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Casa_Grande,2024-12-31T00:00:00,2.522388059701492,3.57962703704834,41.914207977656105,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Cataguases,2024-12-31T00:00:00,1.0,3.0361242294311523,203.61242294311523,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Caxambu,2024-12-31T00:00:00,1.795918367346939,2.17240047454834,20.96320824189618,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Coimbra,2024-12-31T00:00:00,1.5012658227848097,2.208735942840576,47.12490681653083,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Conselheiro_Pena,2024-12-31T00:00:00,1.2,2.0393452644348145,69.94543870290121,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Corrego_Novo,2024-12-31T00:00:00,1.2,3.241631507873535,170.13595898946124,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Cruzilia,2024-12-31T00:00:00,1.502793296089385,1.9529422521591189,29.95414986486332,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Divisopolis,2024-12-31T00:00:00,1.4902200488997548,1.834457874298096,23.09979829170164,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Esmeraldas,2024-12-31T00:00:00,3.0,2.632731914520264,-12.242269515991213,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Espirito_Santo_do_Dourado,2024-12-31T00:00:00,1.4391752577319588,1.7779403924942017,23.53883816041373,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Eugenopolis,2024-12-31T00:00:00,1.68,3.195972204208374,90.23644072668894,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Felicio_dos_Santos,2024-12-31T00:00:00,2.663299663299664,5.999191761016846,125.25410278407114,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Formiga,2024-12-31T00:00:00,1.9475763016157988,2.5373635292053223,30.283138437257065,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Guaraciaba,2024-12-31T00:00:00,1.8000000000000005,3.291435956954956,82.8575531641642,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Guiricema,2024-12-31T00:00:00,1.56,3.434883117675781,120.18481523562698,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Iapu,2024-12-31T00:00:00,1.355263157894737,2.3498687744140625,73.38837558783372,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Ijaci,2024-12-31T00:00:00,2.19,2.4838876724243164,13.419528421201663,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Imbe_de_Minas,2024-12-31T00:00:00,1.26,2.065248966217041,63.90864811246357,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Inhapim,2024-12-31T00:00:00,1.2,3.8787262439727774,223.2271869977316,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Irai_de_Minas,2024-12-31T00:00:00,2.607944732297064,4.392857551574707,68.44135909680502,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Itamarati_de_Minas,2024-12-31T00:00:00,1.196969696969697,1.747846603393555,46.02262762528432,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Jequeri,2024-12-31T00:00:00,1.8000000000000005,2.332599878311157,29.588882128397604,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Lamim,2024-12-31T00:00:00,1.777777777777778,3.8334884643554688,115.63372611999507,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Mar_de_Espanha,2024-12-31T00:00:00,1.2625,2.582592964172364,104.56181894434562,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Mata_Verde,2024-12-31T00:00:00,1.5,2.063953399658203,37.59689331054688,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Monte_Alegre_de_Minas,2024-12-31T00:00:00,2.378787878787879,3.750356435775757,57.658296026242,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Nova_Era,2024-12-31T00:00:00,1.5,2.652690887451172,76.84605916341147,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Nova_Ponte,2024-12-31T00:00:00,2.306557377049181,3.974042654037476,72.29324939323806,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Paula_Candido,2024-12-31T00:00:00,1.31970802919708,1.9379794597625728,46.84910729395608,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Pecanha,2024-12-31T00:00:00,1.8855421686746991,2.616363763809204,38.759228368155846,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Pedra_Bonita,2024-12-31T00:00:00,1.2,2.283540964126587,90.29508034388223,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Pedra_Dourada,2024-12-31T00:00:00,1.2800000000000002,1.7155048847198486,34.023819118738146,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Pedra_do_Anta,2024-12-31T00:00:00,1.379166666666667,2.0399210453033447,47.90968304314278,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Piedade_de_Caratinga,2024-12-31T00:00:00,1.68,3.151896715164185,87.61289971215385,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Pocrane,2024-12-31T00:00:00,1.7968750000000002,1.7127506732940674,-4.6817016601562615,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Presidente_Bernardes,2024-12-31T00:00:00,1.110144927536232,1.8401086330413816,65.75391080921061,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sabinopolis,2024-12-31T00:00:00,1.2,1.2877638339996338,7.31365283330282,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Santa_Barbara,2024-12-31T00:00:00,1.333333333333333,1.4556915760040283,9.176868200302147,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Santo_Antonio_do_Grama,2024-12-31T00:00:00,1.565217391304348,2.36267638206482,50.94876885414122,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sao_Domingos_do_Prata,2024-12-31T00:00:00,1.05,1.5650076866149902,49.04835110618954,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sao_Geraldo,2024-12-31T00:00:00,1.3189189189189188,2.507391452789306,90.10959785492696,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sao_Jose_da_Barra,2024-12-31T00:00:00,1.709523809523809,3.3247621059417725,94.48469143391988,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sao_Jose_do_Alegre,2024-12-31T00:00:00,1.8000000000000005,2.059150695800781,14.397260877821164,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sao_Miguel_do_Anta,2024-12-31T00:00:00,1.55974025974026,1.802284479141236,15.55029549864704,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sao_Sebastiao_da_Vargem_Alegre,2024-12-31T00:00:00,1.2800000000000002,2.328317165374756,81.89977854490277,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Sao_Sebastiao_do_Anta,2024-12-31T00:00:00,1.8000000000000005,2.186509847640991,21.472769313388383,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Senhora_dos_Remedios,2024-12-31T00:00:00,1.8000000000000005,2.035145998001098,13.063666555616576,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Teixeiras,2024-12-31T00:00:00,1.5,2.131412982940674,42.09419886271159,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_2_2024,Visconde_do_Rio_Branco,2024-12-31T00:00:00,1.333333333333333,2.8236076831817627,111.77057623863224,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:13
V42_cluster_3_2024,Aiuruoca,2023-12-31T00:00:00,1.6000000000000003,1.4890289306640625,-6.935691833496112,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Albertina,2023-12-31T00:00:00,1.5,1.541714429855347,2.7809619903564453,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Baependi,2023-12-31T00:00:00,1.439779005524862,1.2831732034683228,-10.877072207303764,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Bandeira_do_Sul,2023-12-31T00:00:00,1.5,1.5362961292266846,2.4197419484456377,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Borda_da_Mata,2023-12-31T00:00:00,1.290476190476191,1.3916501998901367,7.84005239001055,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Brazopolis,2023-12-31T00:00:00,1.440389294403893,1.1861051321029663,-17.653849781365007,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Bueno_Brandao,2023-12-31T00:00:00,1.379901960784314,1.393879532814026,1.0129395005546136,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Cachoeira_de_Minas,2023-12-31T00:00:00,1.6204081632653062,1.7442409992218018,7.642076778171638,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Caldas,2023-12-31T00:00:00,1.56,1.3672704696655271,-12.354457072722608,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Cambuquira,2023-12-31T00:00:00,1.56,1.46294367313385,-6.221559414496792,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Campanha,2023-12-31T00:00:00,1.327075098814229,1.455744385719299,9.695705014737984,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Careacu,2023-12-31T00:00:00,1.5,1.2239177227020264,-18.40548515319824,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Carmo_da_Cachoeira,2023-12-31T00:00:00,1.32,1.465492844581604,11.022185195576055,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Carmo_de_Minas,2023-12-31T00:00:00,1.5,1.3365331888198853,-10.897787412007649,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Carrancas,2023-12-31T00:00:00,1.214285714285714,1.5506720542907717,27.702404471004744,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Carvalhopolis,2023-12-31T00:00:00,1.560106382978723,1.3794095516204834,-11.58234036663793,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Conceicao_das_Pedras,2023-12-31T00:00:00,1.5598885793871868,1.499999761581421,-3.839300998619637,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Conceicao_do_Rio_Verde,2023-12-31T00:00:00,1.44,1.6643683910369873,15.581138266457456,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Conceicao_dos_Ouros,2023-12-31T00:00:00,1.422222222222222,1.3897719383239746,-2.281660586595524,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Congonhal,2023-12-31T00:00:00,1.2,1.4375343322753906,19.79452768961589,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Cordislandia,2023-12-31T00:00:00,1.14,1.439172387123108,26.24319185290421,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Cristina,2023-12-31T00:00:00,1.68,1.523522138595581,-9.314158416929695,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Datas,2023-12-31T00:00:00,0.6666666666666667,1.1096221208572388,66.44331812858579,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Dom_Vicoso,2023-12-31T00:00:00,1.32,1.280550241470337,-2.9886180704290264,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Fama,2023-12-31T00:00:00,1.559677419354839,1.4296925067901611,-8.334089533619473,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Fortaleza_de_Minas,2023-12-31T00:00:00,1.5,1.3965868949890137,-6.894207000732422,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Heliodora,2023-12-31T00:00:00,1.439872408293461,1.3162314891815186,-8.586935786795298,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Ibitiura_de_Minas,2023-12-31T00:00:00,1.150344827586207,1.4458074569702148,25.6847009956122,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Inconfidentes,2023-12-31T00:00:00,1.7401360544217692,1.6276915073394775,-6.46182502779393,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Ingai,2023-12-31T00:00:00,1.6197718631178712,1.4581913948059082,-9.97550778545687,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Itajuba,2023-12-31T00:00:00,1.5862068965517242,1.4198453426361084,-10.488011007723603,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Itutinga,2023-12-31T00:00:00,1.5,1.4296401739120483,-4.690655072530111,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Jesuania,2023-12-31T00:00:00,1.26,1.318506956100464,4.643409214322528,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Lambari,2023-12-31T00:00:00,1.56,1.367715835571289,-12.325907976199424,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Luminarias,2023-12-31T00:00:00,1.260224719101124,1.6004105806350708,26.994063548966885,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Maria_da_Fe,2023-12-31T00:00:00,1.422222222222222,1.412409424781799,-0.6899623200297239,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Monsenhor_Paulo,2023-12-31T00:00:00,1.389931972789116,1.4608992338180542,5.1058082279042285,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Monte_Siao,2023-12-31T00:00:00,1.5,1.397313356399536,-6.845776240030925,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Natercia,2023-12-31T00:00:00,1.5,1.4566280841827393,-2.8914610544840498,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Olimpio_Noronha,2023-12-31T00:00:00,1.2,1.2854762077331543,7.123017311096196,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Ouro_Fino,2023-12-31T00:00:00,1.68,1.338814377784729,-20.308667989004224,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Paraisopolis,2023-12-31T00:00:00,1.2,1.5674824714660645,30.623539288838707,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Pedralva,2023-12-31T00:00:00,1.67998417721519,1.4616358280181885,-12.99704795785307,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Pirangucu,2023-12-31T00:00:00,1.333333333333333,1.237376689910889,-7.196748256683329,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Piranguinho,2023-12-31T00:00:00,1.460887949260042,1.4140307903289795,-3.207443730013394,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Poco_Fundo,2023-12-31T00:00:00,1.380160799652325,1.15221905708313,-16.515593156001515,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Pocos_de_Caldas,2023-12-31T00:00:00,1.5,1.419598937034607,-5.360070864359537,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Pouso_Alegre,2023-12-31T00:00:00,1.566666666666667,1.3977516889572144,-10.781807087837404,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Pouso_Alto,2023-12-31T00:00:00,1.8000000000000005,1.5018761157989502,-16.562438011169444,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Santa_Rita_de_Caldas,2023-12-31T00:00:00,1.68,1.779996633529663,5.952180567241854,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Santa_Rita_do_Sapucai,2023-12-31T00:00:00,1.3799999999999997,1.433039903640747,3.8434712783150298,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Sao_Bento_Abade,2023-12-31T00:00:00,1.5,1.5120577812194824,0.8038520812988281,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Sao_Goncalo_do_Sapucai,2023-12-31T00:00:00,1.2,1.5262655019760132,27.188791831334434,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Sao_Joao_da_Mata,2023-12-31T00:00:00,1.08,1.153849959373474,6.837959201247597,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Sao_Joao_del_Rei,2023-12-31T00:00:00,1.799568965517241,1.6094627380371094,-10.563986772548638,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Sao_Lourenco,2023-12-31T00:00:00,1.5,1.5970501899719238,6.470012664794922,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Sao_Sebastiao_da_Bela_Vista,2023-12-31T00:00:00,1.5,1.3602505922317505,-9.316627184549969,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Sao_Tome_das_Letras,2023-12-31T00:00:00,1.09812734082397,1.3641797304153442,24.2278267465542,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Senador_Jose_Bento,2023-12-31T00:00:00,1.2,1.2453861236572266,3.782176971435551,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Serrania,2023-12-31T00:00:00,0.9,1.3453744649887085,49.48605166541205,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Silvianopolis,2023-12-31T00:00:00,1.13974358974359,1.0235209465026855,-10.19726228660353,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Soledade_de_Minas,2023-12-31T00:00:00,1.5,1.6257519721984863,8.383464813232422,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Tocos_do_Moji,2023-12-31T00:00:00,1.5,1.4044958353042605,-6.366944313049316,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Tres_Coracoes,2023-12-31T00:00:00,1.44,1.5634088516235352,8.570059140523279,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Turvolandia,2023-12-31T00:00:00,1.259748427672956,1.5709803104400637,24.70587586618576,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Virginia,2023-12-31T00:00:00,1.5,1.4730117321014404,-1.7992178599039714,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Aiuruoca,2024-12-31T00:00:00,1.8000000000000005,1.439936876296997,-20.003506872389064,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Albertina,2024-12-31T00:00:00,1.56,1.5228216648101809,-2.383226614732012,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Baependi,2024-12-31T00:00:00,1.439779005524862,1.224965214729309,-14.919914095930576,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Bandeira_do_Sul,2024-12-31T00:00:00,1.5,1.583355188369751,5.557012557983398,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Borda_da_Mata,2024-12-31T00:00:00,1.5,1.3798125982284546,-8.01249345143636,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Brazopolis,2024-12-31T00:00:00,1.501216545012166,1.349252700805664,-10.122713122993884,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Bueno_Brandao,2024-12-31T00:00:00,1.8000000000000005,1.435893177986145,-20.22815677854751,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Cachoeira_de_Minas,2024-12-31T00:00:00,1.73984375,1.623473882675171,-6.688524031242979,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Caldas,2024-12-31T00:00:00,1.68,1.4504055976867676,-13.666333471025736,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Cambuquira,2024-12-31T00:00:00,1.7400000000000002,1.415937900543213,-18.62425858947053,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Campanha,2024-12-31T00:00:00,1.5689448441247,1.374378681182861,-12.401083675467596,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Careacu,2024-12-31T00:00:00,1.560169491525424,1.2528506517410278,-19.69778549405689,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Carmo_da_Cachoeira,2024-12-31T00:00:00,1.44,1.4820634126663208,2.921070324050059,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Carmo_de_Minas,2024-12-31T00:00:00,1.56,1.3313734531402588,-14.65554787562444,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Carrancas,2024-12-31T00:00:00,1.6428571428571432,1.448328137397766,-11.840895984483822,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Carvalhopolis,2024-12-31T00:00:00,1.62,1.4089887142181396,-13.025388011225951,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Conceicao_das_Pedras,2024-12-31T00:00:00,1.5598885793871868,1.553535223007202,-0.4072955250740183,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Conceicao_do_Rio_Verde,2024-12-31T00:00:00,1.5,1.5331873893737793,2.21249262491862,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Conceicao_dos_Ouros,2024-12-31T00:00:00,1.8000000000000005,1.362372875213623,-24.31261804368762,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Congonhal,2024-12-31T00:00:00,1.8000000000000005,1.3504681587219238,-24.97399118211536,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Cordislandia,2024-12-31T00:00:00,1.439887640449438,1.3171405792236328,-8.524766639950581,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Cristina,2024-12-31T00:00:00,1.619736842105263,1.4807149171829224,-8.58299455247594,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Datas,2024-12-31T00:00:00,1.0,0.8321160674095154,-16.788393259048462,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Dom_Vicoso,2024-12-31T00:00:00,1.2,1.317014455795288,9.751204649607343,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Fama,2024-12-31T00:00:00,1.679838709677419,1.5389299392700195,-8.388232131789504,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Fortaleza_de_Minas,2024-12-31T00:00:00,1.5598425196850392,1.549484133720398,-0.6640661370567571,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Heliodora,2024-12-31T00:00:00,1.5601255886970171,1.3249797821044922,-15.072235741541396,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Ibitiura_de_Minas,2024-12-31T00:00:00,1.360264900662252,1.4232640266418457,4.631386573962338,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Inconfidentes,2024-12-31T00:00:00,1.8000000000000005,1.7203574180603027,-4.424587885538752,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Ingai,2024-12-31T00:00:00,1.621004566210046,1.4931113719940186,-7.889749164312682,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Itajuba,2024-12-31T00:00:00,1.068965517241379,1.337859272956848,25.154577147576152,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Itutinga,2024-12-31T00:00:00,1.8000000000000005,1.4573525190353394,-19.03597116470338,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Jesuania,2024-12-31T00:00:00,1.5,1.2758694887161257,-14.942034085591636,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Lambari,2024-12-31T00:00:00,1.56,1.3828949928283691,-11.352885075104544,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Luminarias,2024-12-31T00:00:00,1.380229885057471,1.45895516872406,5.703780545464063,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Maria_da_Fe,2024-12-31T00:00:00,1.477777777777778,1.392362356185913,-5.779990934787856,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Monsenhor_Paulo,2024-12-31T00:00:00,1.5899728997289972,1.415487289428711,-10.974124799864606,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Monte_Siao,2024-12-31T00:00:00,1.5,1.4452170133590698,-3.652199109395345,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Natercia,2024-12-31T00:00:00,1.8000000000000005,1.4689807891845703,-18.38995615641277,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Olimpio_Noronha,2024-12-31T00:00:00,1.5,1.2803986072540283,-14.640092849731444,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Ouro_Fino,2024-12-31T00:00:00,1.5,1.4329140186309814,-4.47239875793457,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Paraisopolis,2024-12-31T00:00:00,1.5066666666666668,1.4215476512908936,-5.649492170958402,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Pedralva,2024-12-31T00:00:00,1.5600790513833993,1.5340681076049805,-1.667283703050409,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Pirangucu,2024-12-31T00:00:00,1.333333333333333,1.2623122930526731,-5.326578021049479,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Piranguinho,2024-12-31T00:00:00,1.6807610993657498,1.4181876182556152,-15.622296423282227,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Poco_Fundo,2024-12-31T00:00:00,1.380192991366176,1.1982107162475586,-13.185277439967525,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Pocos_de_Caldas,2024-12-31T00:00:00,1.7400000000000002,1.5521397590637207,-10.796565571050545,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Pouso_Alegre,2024-12-31T00:00:00,1.7878787878787883,1.448625564575195,-18.975180286472145,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Pouso_Alto,2024-12-31T00:00:00,1.53,1.5162992477416992,-0.8954740038105103,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Santa_Rita_de_Caldas,2024-12-31T00:00:00,1.8000000000000005,1.739426851272583,-3.3651749293009585,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Santa_Rita_do_Sapucai,2024-12-31T00:00:00,1.5,1.4016873836517334,-6.554174423217773,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Sao_Bento_Abade,2024-12-31T00:00:00,1.5,1.452545404434204,-3.163639704386393,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Sao_Goncalo_do_Sapucai,2024-12-31T00:00:00,1.5,1.4126195907592771,-5.825360616048178,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Sao_Joao_da_Mata,2024-12-31T00:00:00,1.32,1.1004055738449097,-16.635941375385634,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Sao_Joao_del_Rei,2024-12-31T00:00:00,1.80168776371308,1.6866934299468994,-6.382589485382842,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Sao_Lourenco,2024-12-31T00:00:00,1.3799999999999997,1.4148502349853516,2.5253793467646304,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Sao_Sebastiao_da_Bela_Vista,2024-12-31T00:00:00,1.44,1.3770267963409424,-4.373139142990109,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Sao_Tome_das_Letras,2024-12-31T00:00:00,2.1,1.3436827659606934,-36.01510638282413,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Senador_Jose_Bento,2024-12-31T00:00:00,0.9,1.3056715726852417,45.07461918724908,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Serrania,2024-12-31T00:00:00,1.32,1.278362274169922,-3.1543731689453174,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Silvianopolis,2024-12-31T00:00:00,1.56025641025641,1.05032479763031,-32.682552000686776,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Soledade_de_Minas,2024-12-31T00:00:00,1.5,1.4853438138961792,-0.9770790735880532,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Tocos_do_Moji,2024-12-31T00:00:00,1.5,1.4098981618881226,-6.006789207458496,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Tres_Coracoes,2024-12-31T00:00:00,1.56,1.3922418355941772,-10.753728487552744,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Turvolandia,2024-12-31T00:00:00,1.44,1.462243914604187,1.544716291957435,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_3_2024,Virginia,2024-12-31T00:00:00,1.6599999999999997,1.4694445133209229,-11.47924618548656,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:45:48
V42_cluster_4_2024,Abadia_dos_Dourados,2023-12-31T00:00:00,2.103030303030303,2.090414047241211,-0.599908416484206,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Acucena,2023-12-31T00:00:00,1.6666666666666672,1.101953387260437,-33.8827967643738,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Agua_Boa,2023-12-31T00:00:00,1.213636363636364,1.0869084596633911,-10.441999578297391,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Aguas_Vermelhas,2023-12-31T00:00:00,3.0,3.0027971267700195,0.093237559000651,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Angelandia,2023-12-31T00:00:00,2.095663265306122,2.250311374664306,7.379435041802604,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Aricanduva,2023-12-31T00:00:00,1.2,1.371193528175354,14.266127347946172,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Ataleia,2023-12-31T00:00:00,0.6833333333333333,0.915876805782318,34.03075206570509,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Bandeira,2023-12-31T00:00:00,0.95,1.2914835214614868,35.94563383805124,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Berilo,2023-12-31T00:00:00,1.7988826815642458,1.6510794162750244,-8.216392697754854,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Berizal,2023-12-31T00:00:00,3.570967741935484,2.6783666610717773,-24.996055561675607,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Bonfinopolis_de_Minas,2023-12-31T00:00:00,3.0,2.7956879138946533,-6.810402870178223,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Buritis,2023-12-31T00:00:00,3.0,2.7244105339050293,-9.186315536499023,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Buritizeiro,2023-12-31T00:00:00,3.0,3.097373962402344,3.2457987467447915,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Capelinha,2023-12-31T00:00:00,1.541750580945004,1.618022084236145,4.947071480549788,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Carai,2023-12-31T00:00:00,1.2,1.4642544984817505,22.021208206812545,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Caranaiba,2023-12-31T00:00:00,1.45,2.5224738121032715,73.96371117953598,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Catuji,2023-12-31T00:00:00,1.384615384615385,1.366584539413452,-1.302227709028485,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Congonhas_do_Norte,2023-12-31T00:00:00,1.111111111111111,0.9500687122344972,-14.493815898895251,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Coroaci,2023-12-31T00:00:00,2.4,1.9095968008041384,-20.433466633160908,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Cuparaque,2023-12-31T00:00:00,1.318367346938776,1.2709650993347168,-3.595526521050928,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Diamantina,2023-12-31T00:00:00,2.965183752417796,3.3049418926239014,11.458249085881071,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Formoso,2023-12-31T00:00:00,3.3803921568627446,2.8692874908447266,-15.119685595660629,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Franciscopolis,2023-12-31T00:00:00,1.2,1.5914732217788696,32.62276848157247,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Frei_Gaspar,2023-12-31T00:00:00,1.2,1.4650983810424805,22.09153175354005,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Grao_Mogol,2023-12-31T00:00:00,1.155555555555555,1.357617974281311,17.486170851267357,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Indaiabira,2023-12-31T00:00:00,1.9320754716981128,2.431762456893921,25.86270528845492,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Itabirinha,2023-12-31T00:00:00,1.027027027027027,1.043247938156128,1.5794045046756195,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Itacambira,2023-12-31T00:00:00,0.75,0.813347578048706,8.446343739827473,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Itaipe,2023-12-31T00:00:00,0.9598214285714286,0.900737464427948,-6.155724636344026,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Itamarandiba,2023-12-31T00:00:00,2.4,2.201451301574707,-8.272862434387203,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Itambacuri,2023-12-31T00:00:00,0.8987341772151899,0.997677743434906,11.009213706137428,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Jequitinhonha,2023-12-31T00:00:00,2.7000000000000006,2.2266318798065186,-17.53215259975859,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Joao_Pinheiro,2023-12-31T00:00:00,3.3,2.8568356037139893,-13.429224129879108,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Jose_Goncalves_de_Minas,2023-12-31T00:00:00,1.3210526315789468,1.8395856618881223,39.25150428635197,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Juiz_de_Fora,2023-12-31T00:00:00,0.7142857142857143,1.501833200454712,110.25664806365968,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Ladainha,2023-12-31T00:00:00,1.081818181818182,0.874623715877533,-19.15242962476588,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Malacacheta,2023-12-31T00:00:00,1.619230769230769,1.9041223526000977,17.594254554875405,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Mantena,2023-12-31T00:00:00,1.2,0.7573450207710266,-36.88791493574778,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Minas_Novas,2023-12-31T00:00:00,1.26,1.8085339069366453,43.53443705846393,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Monte_Formoso,2023-12-31T00:00:00,0.6000000000000001,0.8427003622055054,40.45006036758421,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Ninheira,2023-12-31T00:00:00,3.0,2.6358962059021,-12.13679313659668,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Nova_Belem,2023-12-31T00:00:00,0.96,1.175127387046814,22.40910281737647,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Novo_Cruzeiro,2023-12-31T00:00:00,1.209891435464415,1.1863787174224854,-1.943374203066769,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Novorizonte,2023-12-31T00:00:00,1.333333333333333,1.5290765762329102,14.680743217468288,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Ouro_Verde_de_Minas,2023-12-31T00:00:00,2.5,1.988277792930603,-20.46888828277588,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Padre_Paraiso,2023-12-31T00:00:00,0.7212121212121212,0.6032611131668091,-16.354551535694537,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Paracatu,2023-12-31T00:00:00,2.4,2.48939061164856,3.724608818689986,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Pedra_Azul,2023-12-31T00:00:00,1.166666666666667,1.3071799278259275,12.04399381365092,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Pirapora,2023-12-31T00:00:00,3.6000000000000005,3.394566774368286,-5.706478489769844,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Pote,2023-12-31T00:00:00,1.125,0.956972062587738,-14.93581665886773,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Rio_Pardo_de_Minas,2023-12-31T00:00:00,2.568224299065421,2.716036319732666,5.755417107494613,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Santa_Barbara_do_Monte_Verde,2023-12-31T00:00:00,1.4,1.4093844890594482,0.6703206471034523,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Santa_Maria_do_Suacui,2023-12-31T00:00:00,0.6666666666666667,0.9732178449630736,45.982676744461045,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Santo_Antonio_do_Retiro,2023-12-31T00:00:00,1.02,1.2314211130142212,20.727560099433447,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Sao_Goncalo_do_Rio_Preto,2023-12-31T00:00:00,2.868421052631579,3.502007484436035,22.088334319788373,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Sao_Joao_do_Manteninha,2023-12-31T00:00:00,1.2,1.090038776397705,-9.16343530019124,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Sao_Joao_do_Paraiso,2023-12-31T00:00:00,3.601190476190476,2.689448118209839,-25.31780432078463,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Sao_Sebastiao_do_Maranhao,2023-12-31T00:00:00,1.2,1.0979084968566897,-8.507625261942541,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Senador_Modestino_Goncalves,2023-12-31T00:00:00,2.7000000000000006,3.988853931427002,47.73533079359263,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Setubinha,2023-12-31T00:00:00,1.2,1.0676292181015017,-11.03089849154154,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Taiobeiras,2023-12-31T00:00:00,3.0,3.661612749099731,22.05375830332438,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Teofilo_Otoni,2023-12-31T00:00:00,1.2,0.9288981556892396,-22.59182035923004,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Turmalina,2023-12-31T00:00:00,1.360606060606061,1.4319788217544556,5.245659505338572,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Urucuia,2023-12-31T00:00:00,2.0994575045207946,1.914075255393982,-8.830007215084192,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Vargem_Grande_do_Rio_Pardo,2023-12-31T00:00:00,2.1,1.7713185548782349,-15.651497386750725,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Varzea_da_Palma,2023-12-31T00:00:00,3.3,2.7842581272125244,-15.628541599620467,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Abadia_dos_Dourados,2024-12-31T00:00:00,2.6116071428571423,2.138800621032715,-18.10404459635415,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Acucena,2024-12-31T00:00:00,1.6666666666666672,1.5131118297576904,-9.213290214538604,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Agua_Boa,2024-12-31T00:00:00,1.7064935064935067,1.1411306858062744,-33.13008918791239,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Aguas_Vermelhas,2024-12-31T00:00:00,3.0,3.1345107555389404,4.483691851298014,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Angelandia,2024-12-31T00:00:00,1.870967741935484,2.185688734054565,16.821294406364697,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Aricanduva,2024-12-31T00:00:00,1.320245398773006,1.2884738445281982,-2.4064885417768007,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Ataleia,2024-12-31T00:00:00,0.7529411764705883,0.8554447293281555,13.61375311389564,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Bandeira,2024-12-31T00:00:00,1.15,1.281835675239563,11.463971759962009,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Berilo,2024-12-31T00:00:00,1.681818181818182,2.029911518096924,20.697441616573844,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Berizal,2024-12-31T00:00:00,3.5516129032258075,3.7048768997192383,4.31533505113202,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Bonfinopolis_de_Minas,2024-12-31T00:00:00,2.1,3.108275413513184,48.01311492919921,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Buritis,2024-12-31T00:00:00,2.69967707212056,3.1162428855896,15.430208959838026,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Buritizeiro,2024-12-31T00:00:00,2.7000000000000006,3.284480571746826,21.64742858321576,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Capelinha,2024-12-31T00:00:00,1.62007678089009,1.6768712997436523,3.505668343839775,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Carai,2024-12-31T00:00:00,1.2,1.279545545578003,6.628795464833582,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Caranaiba,2024-12-31T00:00:00,1.5,2.7801499366760254,85.34332911173502,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Catuji,2024-12-31T00:00:00,1.138461538461538,1.388163924217224,21.9333176677292,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Congonhas_do_Norte,2024-12-31T00:00:00,1.2,1.0383819341659546,-13.468172152837116,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Coroaci,2024-12-31T00:00:00,2.1,2.4659881591796875,17.428007579985113,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Cuparaque,2024-12-31T00:00:00,1.020408163265306,1.265690803527832,24.03769874572756,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Diamantina,2024-12-31T00:00:00,2.967637540453075,3.553689479827881,19.74809697566139,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Formoso,2024-12-31T00:00:00,3.6000000000000005,3.278247356414795,-8.937573432922377,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Franciscopolis,2024-12-31T00:00:00,0.9,1.6924129724502563,88.04588582780626,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Frei_Gaspar,2024-12-31T00:00:00,1.2,1.246485471725464,3.8737893104553263,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Grao_Mogol,2024-12-31T00:00:00,2.0333333333333337,1.3582407236099243,-33.20127588803652,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Indaiabira,2024-12-31T00:00:00,2.511111111111111,2.4051852226257324,-4.218287594550477,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Itabirinha,2024-12-31T00:00:00,0.8918918918918919,0.9630056619644164,7.973362099040639,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Itacambira,2024-12-31T00:00:00,0.5,0.7635653018951416,52.71306037902832,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Itaipe,2024-12-31T00:00:00,0.9,0.9477823972702026,5.309155252244735,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Itamarandiba,2024-12-31T00:00:00,2.1352380952380954,2.1794538497924805,2.0707645977745024,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Itambacuri,2024-12-31T00:00:00,1.084388185654009,1.0111839771270752,-6.750738296063538,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Jequitinhonha,2024-12-31T00:00:00,2.4,2.486168384552002,3.5903493563334186,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Joao_Pinheiro,2024-12-31T00:00:00,3.0,3.1386282444000244,4.620941480000814,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Jose_Goncalves_de_Minas,2024-12-31T00:00:00,1.5,1.569547176361084,4.636478424072266,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Juiz_de_Fora,2024-12-31T00:00:00,1.166666666666667,1.4139598608016968,21.196559497288263,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Ladainha,2024-12-31T00:00:00,1.12,1.0044724941253662,-10.31495588166374,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Malacacheta,2024-12-31T00:00:00,1.5,1.753563642501831,16.904242833455406,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Mantena,2024-12-31T00:00:00,1.2,0.893601655960083,-25.533195336659748,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Minas_Novas,2024-12-31T00:00:00,1.6204545454545451,1.6079113483428955,-0.7740542397090943,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Monte_Formoso,2024-12-31T00:00:00,0.6800000000000002,0.7352749705314636,8.128672136979919,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Ninheira,2024-12-31T00:00:00,2.819758276405675,2.644702196121216,-6.208194572890903,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Nova_Belem,2024-12-31T00:00:00,0.9,1.0214147567749023,13.4905285305447,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Novo_Cruzeiro,2024-12-31T00:00:00,1.7997542997542997,1.2080812454223633,-32.875212711685755,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Novorizonte,2024-12-31T00:00:00,1.333333333333333,1.6533334255218506,24.000006914138822,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Ouro_Verde_de_Minas,2024-12-31T00:00:00,2.5,2.3977532386779785,-4.089870452880859,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Padre_Paraiso,2024-12-31T00:00:00,0.7212121212121212,0.6571097373962402,-8.888145655143157,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Paracatu,2024-12-31T00:00:00,2.7003236245954687,2.5496296882629395,-5.580588006561786,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Pedra_Azul,2024-12-31T00:00:00,0.9,1.463743805885315,62.63820065392388,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Pirapora,2024-12-31T00:00:00,2.7000000000000006,3.7264633178710938,38.01715992115159,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Pote,2024-12-31T00:00:00,1.125,1.1773561239242554,4.6538776821560335,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Rio_Pardo_de_Minas,2024-12-31T00:00:00,3.064285714285715,2.785177707672119,-9.10841979624788,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Santa_Barbara_do_Monte_Verde,2024-12-31T00:00:00,1.3,1.673039436340332,28.695341256948616,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Santa_Maria_do_Suacui,2024-12-31T00:00:00,1.333333333333333,0.8687739372253418,-34.84195470809935,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Santo_Antonio_do_Retiro,2024-12-31T00:00:00,1.9666666666666672,1.074836254119873,-45.34730911254884,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Sao_Goncalo_do_Rio_Preto,2024-12-31T00:00:00,2.5238095238095246,3.951211452484131,56.55743490974853,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Sao_Joao_do_Manteninha,2024-12-31T00:00:00,0.9,1.123062014579773,24.78466828664144,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Sao_Joao_do_Paraiso,2024-12-31T00:00:00,3.5396226415094336,2.9653687477111816,-16.22359081626192,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Sao_Sebastiao_do_Maranhao,2024-12-31T00:00:00,1.380952380952381,1.1721675395965576,-15.118902305076862,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Senador_Modestino_Goncalves,2024-12-31T00:00:00,2.706666666666667,3.472592353820801,28.29774706234484,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Setubinha,2024-12-31T00:00:00,1.020238095238095,1.141677737236023,11.90306876059038,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Taiobeiras,2024-12-31T00:00:00,3.1206349206349207,3.709479808807373,18.86939366981917,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Teofilo_Otoni,2024-12-31T00:00:00,1.05,1.065235257148743,1.450976871308822,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Turmalina,2024-12-31T00:00:00,1.8507692307692305,1.3866270780563354,-25.07833742837754,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Urucuia,2024-12-31T00:00:00,1.920433996383363,2.222333669662476,15.72038788355455,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Vargem_Grande_do_Rio_Pardo,2024-12-31T00:00:00,1.8000000000000005,1.9333617687225344,7.408987151251884,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_4_2024,Varzea_da_Palma,2024-12-31T00:00:00,3.0,3.298236131668091,9.94120438893636,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2024_(2024)
    Modelo LSTM treinado com dados de 2012 a 2022, validado com os dados de 2023 e testado com os dados de 2024.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:22
V42_cluster_0_2025,Aguanil,2024-12-31T00:00:00,2.025039123630673,1.5618162155151367,-22.874763391485903,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Alfenas,2024-12-31T00:00:00,1.635197497066875,1.294376254081726,-20.842818289319474,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Alto_Caparao,2024-12-31T00:00:00,1.14,1.312943458557129,15.170478820800792,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Alto_Jequitiba,2024-12-31T00:00:00,1.02,0.9874908328056335,-3.187173254349654,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Amparo_do_Serra,2024-12-31T00:00:00,1.680555555555556,1.3148860931396484,-21.75884404458293,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Araponga,2024-12-31T00:00:00,1.5,1.2140512466430664,-19.06325022379557,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Astolfo_Dutra,2024-12-31T00:00:00,0.8749999999999999,0.8887052536010742,1.566314697265638,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Bambui,2024-12-31T00:00:00,1.8000000000000005,1.457882046699524,-19.00655296113757,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Boa_Esperanca,2024-12-31T00:00:00,1.083893395133256,1.0176050662994385,-6.11576093474285,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Bom_Jesus_do_Galho,2024-12-31T00:00:00,0.9,1.0198521614074707,13.316906823052296,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Bom_Sucesso,2024-12-31T00:00:00,1.6799426934097417,1.3713548183441162,-18.368952481307065,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Caete,2024-12-31T00:00:00,1.8000000000000005,1.1591193675994873,-35.604479577806266,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Caiana,2024-12-31T00:00:00,1.14,0.9264516830444336,-18.73230850487424,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Cajuri,2024-12-31T00:00:00,1.32,0.9623221158981324,-27.09680940165665,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Camacho,2024-12-31T00:00:00,1.5,1.386826992034912,-7.544867197672526,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Campo_do_Meio,2024-12-31T00:00:00,1.372340425531915,1.0257799625396729,-25.25324303974477,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Campos_Altos,2024-12-31T00:00:00,1.5,1.1694960594177246,-22.03359603881836,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Campos_Gerais,2024-12-31T00:00:00,1.611315789473684,1.3755409717559814,-14.632440100069736,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Cana_Verde,2024-12-31T00:00:00,1.2,1.1081122159957886,-7.657315333684282,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Canaa,2024-12-31T00:00:00,1.319791666666667,1.298128366470337,-1.6414181679934334,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Candeias,2024-12-31T00:00:00,1.14003294892916,0.9852154850959778,-13.580086784211218,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Caparao,2024-12-31T00:00:00,1.6799283154121862,1.180748701095581,-29.71434017374288,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Capitolio,2024-12-31T00:00:00,1.32,1.1615972518920898,-12.000208189993195,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Caputira,2024-12-31T00:00:00,1.38008658008658,1.2716703414916992,-7.855756309729445,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Carangola,2024-12-31T00:00:00,1.44,1.046930909156799,-27.29646464188893,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Caratinga,2024-12-31T00:00:00,1.6200335289186922,1.2089881896972656,-25.372643953596626,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Carmo_da_Mata,2024-12-31T00:00:00,1.62,1.1712608337402344,-27.699948534553435,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Carmo_do_Rio_Claro,2024-12-31T00:00:00,1.7634782608695652,1.5807247161865234,-10.363243411513707,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Carmopolis_de_Minas,2024-12-31T00:00:00,1.98,1.5420091152191162,-22.12075175661028,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Cassia,2024-12-31T00:00:00,1.7162790697674415,1.4000346660614014,-18.42616444357688,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Catas_Altas_da_Noruega,2024-12-31T00:00:00,0.9,0.8350232839584351,-7.21963511572944,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Chale,2024-12-31T00:00:00,1.320168067226891,1.27803373336792,-3.191588624581509,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Claudio,2024-12-31T00:00:00,2.699367088607595,1.387192964553833,-48.61043648311709,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Conceicao_da_Barra_de_Minas,2024-12-31T00:00:00,1.8000000000000005,1.4638036489486694,-18.677575058407268,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Conceicao_de_Ipanema,2024-12-31T00:00:00,1.2901734104046243,1.3572508096694946,5.19909949499219,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Coqueiral,2024-12-31T00:00:00,0.9,1.187704086303711,31.967120700412323,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Corrego_Danta,2024-12-31T00:00:00,1.5,1.2907774448394775,-13.94817034403483,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Cristais,2024-12-31T00:00:00,1.439664804469274,1.1440727710723877,-20.532003871960665,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Desterro_de_Entre_Rios,2024-12-31T00:00:00,1.3780487804878048,1.4853675365447998,7.787732740419109,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Divinesia,2024-12-31T00:00:00,1.316666666666667,1.855213284492493,40.90227477158165,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Divino,2024-12-31T00:00:00,1.2,1.0080621242523191,-15.994822978973383,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Doresopolis,2024-12-31T00:00:00,1.5615384615384622,1.323391318321228,-15.250802274995287,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Durande,2024-12-31T00:00:00,1.380095693779904,1.282118558883667,-7.099300094755772,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Eloi_Mendes,2024-12-31T00:00:00,1.421623345558922,1.1264914274215698,-20.76020480806882,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Entre_Folhas,2024-12-31T00:00:00,1.2,0.9932640194892884,-17.227998375892636,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Entre_Rios_de_Minas,2024-12-31T00:00:00,1.88,1.6499550342559814,-12.236434348086089,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ervalia,2024-12-31T00:00:00,1.32,1.1623294353485107,-11.944739746324949,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Espera_Feliz,2024-12-31T00:00:00,1.44,0.8821625709533691,-38.73871035046047,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Faria_Lemos,2024-12-31T00:00:00,1.37956204379562,0.9729526042938232,-29.473805932140827,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ferros,2024-12-31T00:00:00,1.205128205128205,0.9030677676200868,-25.064589495354504,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Fervedouro,2024-12-31T00:00:00,1.56,1.197543382644653,-23.23439854841966,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Guape,2024-12-31T00:00:00,1.7459340659340663,1.3127872943878174,-24.8088848254712,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ibituruna,2024-12-31T00:00:00,1.8000000000000005,1.394446611404419,-22.530743810865623,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ilicinea,2024-12-31T00:00:00,1.5,1.186691403388977,-20.8872397740682,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Itapecerica,2024-12-31T00:00:00,1.615189873417721,0.937678039073944,-41.94626560592351,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ituiutaba,2024-12-31T00:00:00,1.0,0.6218308806419373,-37.816911935806274,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Itumirim,2024-12-31T00:00:00,1.62116991643454,1.3021082878112793,-19.68094925700182,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Lajinha,2024-12-31T00:00:00,1.564997053624043,1.1917724609375,-23.848261683449927,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Lavras,2024-12-31T00:00:00,1.55996015936255,1.2202439308166504,-21.77723748308538,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Luisburgo,2024-12-31T00:00:00,1.2,1.442596197128296,20.21634976069133,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Machado,2024-12-31T00:00:00,1.3347736360118658,1.5726726055145264,17.82316964346647,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Manhuacu,2024-12-31T00:00:00,1.5,1.064133644104004,-29.05775705973308,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Manhumirim,2024-12-31T00:00:00,1.56,1.2733569145202637,-18.374556761521564,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Martins_Soares,2024-12-31T00:00:00,1.2,1.2571172714233398,4.759772618611658,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Matipo,2024-12-31T00:00:00,1.2,1.1858065128326416,-1.182790597279863,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Medeiros,2024-12-31T00:00:00,1.8600609756097564,1.432908535003662,-22.96443214535304,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Miradouro,2024-12-31T00:00:00,1.13996383363472,1.071707844734192,-5.98755740196574,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Mirai,2024-12-31T00:00:00,1.2,1.094631910324097,-8.780674139658606,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Moeda,2024-12-31T00:00:00,1.5,1.007186770439148,-32.85421530405681,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Muriae,2024-12-31T00:00:00,1.170285714285714,0.947613000869751,-19.027209398336684,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Mutum,2024-12-31T00:00:00,1.15997150997151,1.2056188583374023,3.9352128887211615,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Nazareno,2024-12-31T00:00:00,1.619811320754717,1.4146811962127686,-12.663828306025932,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Nepomuceno,2024-12-31T00:00:00,1.3800383877159308,1.0696600675582886,-22.49055699612401,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Oliveira,2024-12-31T00:00:00,1.8240227434257288,1.5133233070373535,-17.03374793478974,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Orizania,2024-12-31T00:00:00,1.5,1.2416688203811646,-17.2220786412557,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Paraguacu,2024-12-31T00:00:00,1.323316582914573,1.078802466392517,-18.47737115056168,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Passa_Tempo,2024-12-31T00:00:00,1.5,1.482692837715149,-1.1538108189900718,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Perdoes,2024-12-31T00:00:00,1.6799401197604789,1.3562625646591189,-19.267207878070632,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Pimenta,2024-12-31T00:00:00,1.715219421101774,1.2141109704971311,-29.215413750548297,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Piranga,2024-12-31T00:00:00,1.8000000000000005,1.8667234182357788,3.7068565686543633,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Piumhi,2024-12-31T00:00:00,1.3800316957210783,1.3295934200286863,-3.654863569349932,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ponte_Nova,2024-12-31T00:00:00,2.0,1.2119696140289309,-39.40151929855345,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Porto_Firme,2024-12-31T00:00:00,1.3810526315789473,1.1627570390701294,-15.80646439659886,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Raul_Soares,2024-12-31T00:00:00,1.5,1.1159359216690063,-25.60427188873291,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Reduto,2024-12-31T00:00:00,1.2,1.059502124786377,-11.708156267801916,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ribeirao_Vermelho,2024-12-31T00:00:00,1.43859649122807,1.241837501525879,-13.677149284176698,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ritapolis,2024-12-31T00:00:00,1.8000000000000005,1.7650728225708008,-1.9403987460666376,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Rosario_da_Limeira,2024-12-31T00:00:00,1.259493670886076,1.015860676765442,-19.343725161336778,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Santa_Barbara_do_Leste,2024-12-31T00:00:00,1.4101226993865028,1.141315221786499,-19.0627012611706,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Santa_Margarida,2024-12-31T00:00:00,1.5,1.14325213432312,-23.783191045125328,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Santa_Rita_de_Minas,2024-12-31T00:00:00,1.200173310225303,1.0966116189956665,-8.628894706065017,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Santa_Rita_do_Itueto,2024-12-31T00:00:00,1.365238095238095,1.2748091220855713,-6.623677838168805,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Santana_da_Vargem,2024-12-31T00:00:00,1.44,1.1648588180541992,-19.10702652401394,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Santana_do_Jacare,2024-12-31T00:00:00,1.2,0.9712558388710022,-19.06201342741648,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Santana_do_Manhuacu,2024-12-31T00:00:00,1.08,1.3157893419265747,21.83234647468284,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Domingos_das_Dores,2024-12-31T00:00:00,1.41,1.2306187152862549,-12.722076930052854,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Francisco_de_Paula,2024-12-31T00:00:00,1.62,1.435519456863403,-11.387687847938071,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Francisco_do_Gloria,2024-12-31T00:00:00,1.25,0.8660178184509277,-30.718574523925778,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Joao_do_Manhuacu,2024-12-31T00:00:00,1.32,1.093890905380249,-17.129476865132652,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Jose_do_Mantimento,2024-12-31T00:00:00,1.44,1.462382197380066,1.5543192625045814,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Roque_de_Minas,2024-12-31T00:00:00,1.58,1.3015862703323364,-17.621122130864787,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Tiago,2024-12-31T00:00:00,1.621052631578947,1.3325343132019043,-17.798207951830562,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Tomas_de_Aquino,2024-12-31T00:00:00,1.619975932611312,1.2622621059417725,-22.081428462515763,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Senador_Firmino,2024-12-31T00:00:00,1.804347826086957,1.3587993383407593,-24.693048718463963,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Senhora_de_Oliveira,2024-12-31T00:00:00,1.8000000000000005,1.359175682067871,-24.490239885118285,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sericita,2024-12-31T00:00:00,1.3799999999999997,1.033012628555298,-25.144012423529123,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Simonesia,2024-12-31T00:00:00,1.2,1.2999539375305176,8.329494794209802,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Tapirai,2024-12-31T00:00:00,1.9163050216986977,1.3982514142990112,-27.03398475366207,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Tombos,2024-12-31T00:00:00,1.5,1.07726788520813,-28.18214098612468,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Tres_Pontas,2024-12-31T00:00:00,1.387228956803825,1.206443428993225,-13.03213337091302,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ubaporanga,2024-12-31T00:00:00,1.440056417489422,1.3471033573150637,-6.454820731010787,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Vargem_Bonita,2024-12-31T00:00:00,2.187804878048781,1.3278052806854248,-39.308788731212495,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Vermelho_Novo,2024-12-31T00:00:00,1.8000000000000005,1.1190155744552612,-37.83246808581883,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Vicosa,2024-12-31T00:00:00,1.32,0.984789788722992,-25.394712975530915,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Vieiras,2024-12-31T00:00:00,1.44,1.0152347087860107,-29.49758966763814,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Aguanil,2025-12-31T00:00:00,0.0,1.7537977695465088,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Alfenas,2025-12-31T00:00:00,0.0,1.3999184370040894,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Alto_Caparao,2025-12-31T00:00:00,0.0,1.5877227783203125,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Alto_Jequitiba,2025-12-31T00:00:00,0.0,1.3790717124938965,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Amparo_do_Serra,2025-12-31T00:00:00,0.0,1.6112847328186035,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Araponga,2025-12-31T00:00:00,0.0,1.4889874458312988,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Astolfo_Dutra,2025-12-31T00:00:00,0.0,0.7413367033004761,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Bambui,2025-12-31T00:00:00,0.0,1.5335146188735962,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Boa_Esperanca,2025-12-31T00:00:00,0.0,1.141883134841919,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Bom_Jesus_do_Galho,2025-12-31T00:00:00,0.0,0.9806966185569764,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Bom_Sucesso,2025-12-31T00:00:00,0.0,1.5197081565856934,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Caete,2025-12-31T00:00:00,0.0,1.324859380722046,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Caiana,2025-12-31T00:00:00,0.0,1.1168792247772217,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Cajuri,2025-12-31T00:00:00,0.0,1.1962791681289673,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Camacho,2025-12-31T00:00:00,0.0,1.595232367515564,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Campo_do_Meio,2025-12-31T00:00:00,0.0,1.2473117113113403,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Campos_Altos,2025-12-31T00:00:00,0.0,1.289076328277588,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Campos_Gerais,2025-12-31T00:00:00,0.0,1.5068302154541016,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Cana_Verde,2025-12-31T00:00:00,0.0,1.182304620742798,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Canaa,2025-12-31T00:00:00,0.0,1.473986029624939,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Candeias,2025-12-31T00:00:00,0.0,1.2534514665603638,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Caparao,2025-12-31T00:00:00,0.0,1.6632671356201172,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Capitolio,2025-12-31T00:00:00,0.0,1.370943307876587,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Caputira,2025-12-31T00:00:00,0.0,1.4589418172836304,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Carangola,2025-12-31T00:00:00,0.0,1.227463960647583,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Caratinga,2025-12-31T00:00:00,0.0,1.3177634477615356,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Carmo_da_Mata,2025-12-31T00:00:00,0.0,1.4334017038345337,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Carmo_do_Rio_Claro,2025-12-31T00:00:00,0.0,1.8032302856445312,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Carmopolis_de_Minas,2025-12-31T00:00:00,0.0,1.7720904350280762,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Cassia,2025-12-31T00:00:00,0.0,1.6930100917816162,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Catas_Altas_da_Noruega,2025-12-31T00:00:00,0.0,0.9983649849891664,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Chale,2025-12-31T00:00:00,0.0,1.3860405683517456,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Claudio,2025-12-31T00:00:00,0.0,2.1512842178344727,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Conceicao_da_Barra_de_Minas,2025-12-31T00:00:00,0.0,1.5823163986206057,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Conceicao_de_Ipanema,2025-12-31T00:00:00,0.0,1.395849108695984,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Coqueiral,2025-12-31T00:00:00,0.0,1.156179904937744,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Corrego_Danta,2025-12-31T00:00:00,0.0,1.3559885025024414,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Cristais,2025-12-31T00:00:00,0.0,1.4354760646820068,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Desterro_de_Entre_Rios,2025-12-31T00:00:00,0.0,1.6352949142456057,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Divinesia,2025-12-31T00:00:00,0.0,1.8417654037475584,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Divino,2025-12-31T00:00:00,0.0,1.264933705329895,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Doresopolis,2025-12-31T00:00:00,0.0,1.481673002243042,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Durande,2025-12-31T00:00:00,0.0,1.5355507135391235,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Eloi_Mendes,2025-12-31T00:00:00,0.0,1.239292025566101,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Entre_Folhas,2025-12-31T00:00:00,0.0,1.236465573310852,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Entre_Rios_de_Minas,2025-12-31T00:00:00,0.0,1.717434287071228,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ervalia,2025-12-31T00:00:00,0.0,1.3181633949279783,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Espera_Feliz,2025-12-31T00:00:00,0.0,1.3310683965682983,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Faria_Lemos,2025-12-31T00:00:00,0.0,1.204521894454956,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ferros,2025-12-31T00:00:00,0.0,1.0637072324752808,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Fervedouro,2025-12-31T00:00:00,0.0,1.3651398420333862,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Guape,2025-12-31T00:00:00,0.0,1.5902611017227173,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ibituruna,2025-12-31T00:00:00,0.0,1.5519278049468994,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ilicinea,2025-12-31T00:00:00,0.0,1.4795749187469482,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Itapecerica,2025-12-31T00:00:00,0.0,1.2505528926849363,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ituiutaba,2025-12-31T00:00:00,0.0,0.9749863147735596,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Itumirim,2025-12-31T00:00:00,0.0,1.3768422603607178,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Lajinha,2025-12-31T00:00:00,0.0,1.448035717010498,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Lavras,2025-12-31T00:00:00,0.0,1.344376802444458,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Luisburgo,2025-12-31T00:00:00,0.0,1.6409337520599363,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Machado,2025-12-31T00:00:00,0.0,1.4982855319976809,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Manhuacu,2025-12-31T00:00:00,0.0,1.3773586750030518,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Manhumirim,2025-12-31T00:00:00,0.0,1.5009043216705322,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Martins_Soares,2025-12-31T00:00:00,0.0,1.417008876800537,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Matipo,2025-12-31T00:00:00,0.0,1.295647382736206,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Medeiros,2025-12-31T00:00:00,0.0,1.5497745275497437,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Miradouro,2025-12-31T00:00:00,0.0,1.0970337390899658,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Mirai,2025-12-31T00:00:00,0.0,1.0897605419158936,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Moeda,2025-12-31T00:00:00,0.0,1.2058751583099363,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Muriae,2025-12-31T00:00:00,0.0,0.969991147518158,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Mutum,2025-12-31T00:00:00,0.0,1.2020701169967651,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Nazareno,2025-12-31T00:00:00,0.0,1.496693730354309,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Nepomuceno,2025-12-31T00:00:00,0.0,1.2048680782318115,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Oliveira,2025-12-31T00:00:00,0.0,1.747866153717041,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Orizania,2025-12-31T00:00:00,0.0,1.5295212268829346,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Paraguacu,2025-12-31T00:00:00,0.0,1.245720148086548,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Passa_Tempo,2025-12-31T00:00:00,0.0,1.5463405847549438,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Perdoes,2025-12-31T00:00:00,0.0,1.502574443817139,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Pimenta,2025-12-31T00:00:00,0.0,1.534569501876831,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Piranga,2025-12-31T00:00:00,0.0,2.036865234375,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Piumhi,2025-12-31T00:00:00,0.0,1.4808852672576904,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ponte_Nova,2025-12-31T00:00:00,0.0,1.428943395614624,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Porto_Firme,2025-12-31T00:00:00,0.0,1.3078632354736328,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Raul_Soares,2025-12-31T00:00:00,0.0,1.3541432619094849,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Reduto,2025-12-31T00:00:00,0.0,1.2680054903030396,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ribeirao_Vermelho,2025-12-31T00:00:00,0.0,1.3456352949142456,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ritapolis,2025-12-31T00:00:00,0.0,1.8217060565948489,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Rosario_da_Limeira,2025-12-31T00:00:00,0.0,1.083329439163208,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Santa_Barbara_do_Leste,2025-12-31T00:00:00,0.0,1.2243441343307495,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Santa_Margarida,2025-12-31T00:00:00,0.0,1.4084115028381348,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Santa_Rita_de_Minas,2025-12-31T00:00:00,0.0,1.1011412143707275,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Santa_Rita_do_Itueto,2025-12-31T00:00:00,0.0,1.2950851917266846,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Santana_da_Vargem,2025-12-31T00:00:00,0.0,1.2904481887817385,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Santana_do_Jacare,2025-12-31T00:00:00,0.0,1.1643774509429932,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Santana_do_Manhuacu,2025-12-31T00:00:00,0.0,1.3754253387451172,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Domingos_das_Dores,2025-12-31T00:00:00,0.0,1.2279295921325684,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Francisco_de_Paula,2025-12-31T00:00:00,0.0,1.60509192943573,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Francisco_do_Gloria,2025-12-31T00:00:00,0.0,1.1041157245635986,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Joao_do_Manhuacu,2025-12-31T00:00:00,0.0,1.3220192193984983,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Jose_do_Mantimento,2025-12-31T00:00:00,0.0,1.5169610977172852,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Roque_de_Minas,2025-12-31T00:00:00,0.0,1.49891459941864,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Tiago,2025-12-31T00:00:00,0.0,1.591808795928955,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sao_Tomas_de_Aquino,2025-12-31T00:00:00,0.0,1.4409551620483398,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Senador_Firmino,2025-12-31T00:00:00,0.0,1.4951133728027344,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Senhora_de_Oliveira,2025-12-31T00:00:00,0.0,1.5670920610427856,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Sericita,2025-12-31T00:00:00,0.0,1.503601312637329,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Simonesia,2025-12-31T00:00:00,0.0,1.5205105543136597,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Tapirai,2025-12-31T00:00:00,0.0,1.5181480646133425,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Tombos,2025-12-31T00:00:00,0.0,1.095708250999451,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Tres_Pontas,2025-12-31T00:00:00,0.0,1.248404026031494,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Ubaporanga,2025-12-31T00:00:00,0.0,1.3189349174499512,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Vargem_Bonita,2025-12-31T00:00:00,0.0,1.6660006046295166,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Vermelho_Novo,2025-12-31T00:00:00,0.0,1.4692153930664062,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Vicosa,2025-12-31T00:00:00,0.0,1.385124921798706,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_0_2025,Vieiras,2025-12-31T00:00:00,0.0,1.282588005065918,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_0_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:46:57
V42_cluster_1_2025,Almenara,2024-12-31T00:00:00,0.78,0.9768451452255248,25.2365570801955,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Alpinopolis,2024-12-31T00:00:00,2.08,2.247289657592773,8.042771999652565,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Alterosa,2024-12-31T00:00:00,1.110028116213683,2.062556743621826,85.81121626515443,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Andradas,2024-12-31T00:00:00,1.26,1.261276364326477,0.1012987560696065,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Araguari,2024-12-31T00:00:00,1.500037950664137,2.366596221923828,57.76908983376222,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Arapua,2024-12-31T00:00:00,1.0805194805194802,1.6121729612350464,49.20350722968583,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Araxa,2024-12-31T00:00:00,1.412283279459301,1.8265119791030884,29.33042581955489,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Arceburgo,2024-12-31T00:00:00,1.7586206896551722,1.929603576660156,9.722556319891254,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Areado,2024-12-31T00:00:00,1.162105263157895,2.3192813396453857,99.57583991513732,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Bom_Jesus_da_Penha,2024-12-31T00:00:00,1.56,2.1456892490386963,37.54418263068565,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Botelhos,2024-12-31T00:00:00,1.08,1.5116429328918457,39.96693823072645,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Botumirim,2024-12-31T00:00:00,0.7250000000000001,0.6920537948608398,-4.54430415712555,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Cabo_Verde,2024-12-31T00:00:00,1.32,1.8874359130859373,42.98756917317708,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Campestre,2024-12-31T00:00:00,1.320032051282051,1.7633569240570068,33.58440216238589,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Capetinga,2024-12-31T00:00:00,1.437691001697793,2.1068687438964844,46.545310599318505,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Carmo_do_Paranaiba,2024-12-31T00:00:00,2.103206239168111,2.2902867794036865,8.895016415963678,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Cascalho_Rico,2024-12-31T00:00:00,1.8000000000000005,2.274381399154663,26.35452217525904,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Claraval,2024-12-31T00:00:00,1.92,2.53847074508667,32.21201797326406,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Conceicao_da_Aparecida,2024-12-31T00:00:00,1.5,2.163656711578369,44.24378077189128,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Coromandel,2024-12-31T00:00:00,1.6144686299615878,2.09354829788208,29.674139158212743,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Corrego_Fundo,2024-12-31T00:00:00,1.501960784313725,1.867803931236267,24.3577029935374,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Cruzeiro_da_Fortaleza,2024-12-31T00:00:00,2.22,2.015430927276612,-9.214823095648129,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Delfinopolis,2024-12-31T00:00:00,1.5,1.8069276809692385,20.46184539794922,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Divisa_Nova,2024-12-31T00:00:00,1.679831932773109,1.9952173233032229,18.774818145614567,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Dom_Cavati,2024-12-31T00:00:00,0.6000000000000001,1.1471697092056274,91.1949515342712,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Dores_do_Turvo,2024-12-31T00:00:00,1.2,1.536827564239502,28.06896368662517,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Estrela_do_Indaia,2024-12-31T00:00:00,1.56,1.8527803421020508,18.76797064756736,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Estrela_do_Sul,2024-12-31T00:00:00,1.7099999999999995,2.2458977699279785,31.339050872981225,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Guaranesia,2024-12-31T00:00:00,1.205714285714286,1.48498272895813,23.16207467662212,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Guarani,2024-12-31T00:00:00,0.9655172413793104,1.681172251701355,74.12141178335462,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Guarda-Mor,2024-12-31T00:00:00,1.740112994350282,2.077526092529297,19.390298174573264,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Guaxupe,2024-12-31T00:00:00,1.32,1.544602870941162,17.015369010694094,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Guimarania,2024-12-31T00:00:00,1.8219512195121947,1.9171780347824097,5.226639124603498,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Ibia,2024-12-31T00:00:00,1.681621621621622,1.6441144943237305,-2.2304141916136064,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Ibiraci,2024-12-31T00:00:00,1.978101265822785,2.604156255722046,31.64928918029155,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Indianopolis,2024-12-31T00:00:00,1.8000000000000005,2.4026033878326416,33.47796599070229,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Itamogi,2024-12-31T00:00:00,1.44,2.30957579612732,60.38720806439718,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Itanhomi,2024-12-31T00:00:00,1.02089552238806,1.4798320531845093,44.95430930316095,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Jacui,2024-12-31T00:00:00,1.3799999999999997,1.79655122756958,30.184871563013083,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Jacutinga,2024-12-31T00:00:00,1.319874804381847,1.2442758083343506,-5.727739918704071,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Juruaia,2024-12-31T00:00:00,1.68,1.7025607824325562,1.3429037162235842,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Lagoa_Dourada,2024-12-31T00:00:00,1.5,2.618535041809082,74.56900278727213,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Lagoa_Formosa,2024-12-31T00:00:00,1.8142857142857145,2.1343555450439453,17.641644215020595,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Lagoa_Grande,2024-12-31T00:00:00,2.4,2.540068864822388,5.836202700932825,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Matutina,2024-12-31T00:00:00,1.380566801619433,1.931529641151428,39.90845201302136,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Monte_Belo,2024-12-31T00:00:00,1.5,1.7536334991455078,16.908899943033852,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Monte_Carmelo,2024-12-31T00:00:00,1.56,2.1040966510772705,34.87799045367118,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Monte_Santo_de_Minas,2024-12-31T00:00:00,1.5,1.960413098335266,30.694206555684406,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Muzambinho,2024-12-31T00:00:00,1.26,1.872824549674988,48.63686902182442,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Nova_Resende,2024-12-31T00:00:00,1.32,2.276700496673584,72.47731035405938,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Passos,2024-12-31T00:00:00,1.95459940652819,1.927870869636536,-1.3674687919367707,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Patis,2024-12-31T00:00:00,0.9,3.283882141113281,264.87579345703125,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Patos_de_Minas,2024-12-31T00:00:00,1.6624523160762943,1.82262659072876,9.63481918269436,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Patrocinio,2024-12-31T00:00:00,1.4995330375904743,2.03159236907959,35.481667836012164,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Pedrinopolis,2024-12-31T00:00:00,1.8511627906976744,1.879093050956726,1.508795466255315,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Perdizes,2024-12-31T00:00:00,1.142806076854334,2.07896089553833,81.91720692112855,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Piracema,2024-12-31T00:00:00,0.8,1.3861541748046875,73.26927185058592,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Ponto_dos_Volantes,2024-12-31T00:00:00,0.5,0.6040345430374146,20.80690860748291,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Pratapolis,2024-12-31T00:00:00,1.1988636363636362,2.179118394851685,81.76532582649124,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Pratinha,2024-12-31T00:00:00,1.849285714285714,1.995515942573548,7.907389710427454,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Presidente_Kubitschek,2024-12-31T00:00:00,0.7368421052631579,1.349215388298035,83.10780269759043,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Presidente_Olegario,2024-12-31T00:00:00,1.9199630314232905,2.1966443061828613,14.410760531907954,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Quartel_Geral,2024-12-31T00:00:00,3.0,3.801464557647705,26.715485254923504,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Rio_Paranaiba,2024-12-31T00:00:00,1.680014776505357,1.9305369853973389,14.911905085329051,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Rio_Vermelho,2024-12-31T00:00:00,0.9230769230769232,1.1473661661148071,24.2980013291041,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Romaria,2024-12-31T00:00:00,2.052631578947369,2.17875075340271,6.144267473465326,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Sacramento,2024-12-31T00:00:00,1.737021276595745,1.875145673751831,7.951796504576261,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Santa_Rosa_da_Serra,2024-12-31T00:00:00,1.92,1.8742974996566768,-2.3803385595480564,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Santo_Antonio_do_Amparo,2024-12-31T00:00:00,1.68,2.112438201904297,25.74036916097005,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Sao_Goncalo_do_Abaete,2024-12-31T00:00:00,1.5,2.069013357162476,37.93422381083171,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Sao_Gotardo,2024-12-31T00:00:00,1.668148148148148,1.7737815380096436,6.332374614254845,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Sao_Joao_Batista_do_Gloria,2024-12-31T00:00:00,1.548387096774194,1.6384527683258057,5.816741287708255,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Sao_Pedro_da_Uniao,2024-12-31T00:00:00,2.04,1.77381694316864,-13.048189060360778,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Sao_Sebastiao_do_Paraiso,2024-12-31T00:00:00,1.26,1.8418376445770264,46.17759083944654,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Serra_do_Salitre,2024-12-31T00:00:00,1.5168750000000002,2.032118797302246,33.96745264456504,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Tapira,2024-12-31T00:00:00,1.98,2.1944353580474854,10.83006858825685,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Tiros,2024-12-31T00:00:00,1.95,2.0430588722229004,4.77224985758465,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Tupaciguara,2024-12-31T00:00:00,2.8219895287958106,2.451174020767212,-13.140215590623816,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Uberaba,2024-12-31T00:00:00,2.102941176470588,2.456840991973877,16.828802415541023,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Uberlandia,2024-12-31T00:00:00,1.919565217391304,2.265660047531128,18.02985525077227,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Unai,2024-12-31T00:00:00,2.2801061007957566,2.6178057193756104,14.810697557538951,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Varginha,2024-12-31T00:00:00,1.3799999999999997,1.778203368186951,28.85531653528632,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Varjao_de_Minas,2024-12-31T00:00:00,2.6876337184424486,2.479742765426636,-7.7350924565826205,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Virginopolis,2024-12-31T00:00:00,1.325,1.5048506259918213,13.57363215032614,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Almenara,2025-12-31T00:00:00,0.0,0.7822378873825073,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Alpinopolis,2025-12-31T00:00:00,0.0,2.2532362937927246,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Alterosa,2025-12-31T00:00:00,0.0,1.6312384605407717,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Andradas,2025-12-31T00:00:00,0.0,2.0763461589813232,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Araguari,2025-12-31T00:00:00,0.0,2.259718179702759,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Arapua,2025-12-31T00:00:00,0.0,1.3333063125610352,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Araxa,2025-12-31T00:00:00,0.0,1.620309352874756,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Arceburgo,2025-12-31T00:00:00,0.0,1.812364101409912,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Areado,2025-12-31T00:00:00,0.0,1.8598361015319824,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Bom_Jesus_da_Penha,2025-12-31T00:00:00,0.0,1.933518648147583,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Botelhos,2025-12-31T00:00:00,0.0,1.648836374282837,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Botumirim,2025-12-31T00:00:00,0.0,0.8455942869186401,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Cabo_Verde,2025-12-31T00:00:00,0.0,1.7385520935058594,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Campestre,2025-12-31T00:00:00,0.0,1.7900230884552002,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Capetinga,2025-12-31T00:00:00,0.0,1.8198966979980469,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Carmo_do_Paranaiba,2025-12-31T00:00:00,0.0,2.163297176361084,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Cascalho_Rico,2025-12-31T00:00:00,0.0,2.3984804153442383,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Claraval,2025-12-31T00:00:00,0.0,1.9744914770126345,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Conceicao_da_Aparecida,2025-12-31T00:00:00,0.0,1.9616930484771729,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Coromandel,2025-12-31T00:00:00,0.0,1.8234436511993408,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Corrego_Fundo,2025-12-31T00:00:00,0.0,1.5812697410583496,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Cruzeiro_da_Fortaleza,2025-12-31T00:00:00,0.0,2.0206522941589355,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Delfinopolis,2025-12-31T00:00:00,0.0,1.493724346160889,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Divisa_Nova,2025-12-31T00:00:00,0.0,1.806287288665772,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Dom_Cavati,2025-12-31T00:00:00,0.0,0.748273491859436,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Dores_do_Turvo,2025-12-31T00:00:00,0.0,1.5522514581680298,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Estrela_do_Indaia,2025-12-31T00:00:00,0.0,1.840494990348816,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Estrela_do_Sul,2025-12-31T00:00:00,0.0,2.1699531078338623,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Guaranesia,2025-12-31T00:00:00,0.0,1.3485814332962036,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Guarani,2025-12-31T00:00:00,0.0,1.3521521091461182,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Guarda-Mor,2025-12-31T00:00:00,0.0,1.830199837684632,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Guaxupe,2025-12-31T00:00:00,0.0,1.4567493200302124,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Guimarania,2025-12-31T00:00:00,0.0,1.922958254814148,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Ibia,2025-12-31T00:00:00,0.0,1.6508331298828125,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Ibiraci,2025-12-31T00:00:00,0.0,2.0810141563415527,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Indianopolis,2025-12-31T00:00:00,0.0,2.1505489349365234,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Itamogi,2025-12-31T00:00:00,0.0,1.97031319141388,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Itanhomi,2025-12-31T00:00:00,0.0,1.1779210567474363,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Jacui,2025-12-31T00:00:00,0.0,1.5329664945602417,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Jacutinga,2025-12-31T00:00:00,0.0,1.6309911012649536,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Juruaia,2025-12-31T00:00:00,0.0,1.7414579391479492,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Lagoa_Dourada,2025-12-31T00:00:00,0.0,2.2594449520111084,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Lagoa_Formosa,2025-12-31T00:00:00,0.0,2.423455238342285,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Lagoa_Grande,2025-12-31T00:00:00,0.0,2.9421980381011963,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Matutina,2025-12-31T00:00:00,0.0,1.7708160877227783,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Monte_Belo,2025-12-31T00:00:00,0.0,1.6637117862701416,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Monte_Carmelo,2025-12-31T00:00:00,0.0,2.086240291595459,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Monte_Santo_de_Minas,2025-12-31T00:00:00,0.0,1.8095510005950928,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Muzambinho,2025-12-31T00:00:00,0.0,1.712045669555664,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Nova_Resende,2025-12-31T00:00:00,0.0,2.008319854736328,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Passos,2025-12-31T00:00:00,0.0,1.8599073886871336,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Patis,2025-12-31T00:00:00,0.0,3.797404527664185,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Patos_de_Minas,2025-12-31T00:00:00,0.0,1.8702075481414795,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Patrocinio,2025-12-31T00:00:00,0.0,1.8125293254852293,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Pedrinopolis,2025-12-31T00:00:00,0.0,2.06144118309021,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Perdizes,2025-12-31T00:00:00,0.0,1.8183242082595823,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Piracema,2025-12-31T00:00:00,0.0,1.280815839767456,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Ponto_dos_Volantes,2025-12-31T00:00:00,0.0,0.5902988314628601,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Pratapolis,2025-12-31T00:00:00,0.0,1.915435552597046,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Pratinha,2025-12-31T00:00:00,0.0,2.026983499526977,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Presidente_Kubitschek,2025-12-31T00:00:00,0.0,1.5236283540725708,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Presidente_Olegario,2025-12-31T00:00:00,0.0,2.075795412063598,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Quartel_Geral,2025-12-31T00:00:00,0.0,3.30514931678772,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Rio_Paranaiba,2025-12-31T00:00:00,0.0,1.7351150512695312,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Rio_Vermelho,2025-12-31T00:00:00,0.0,1.035994052886963,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Romaria,2025-12-31T00:00:00,0.0,2.194308280944824,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Sacramento,2025-12-31T00:00:00,0.0,1.6213159561157229,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Santa_Rosa_da_Serra,2025-12-31T00:00:00,0.0,2.021538734436035,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Santo_Antonio_do_Amparo,2025-12-31T00:00:00,0.0,1.9187248945236208,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Sao_Goncalo_do_Abaete,2025-12-31T00:00:00,0.0,2.117126226425171,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Sao_Gotardo,2025-12-31T00:00:00,0.0,1.5519649982452393,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Sao_Joao_Batista_do_Gloria,2025-12-31T00:00:00,0.0,1.583521604537964,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Sao_Pedro_da_Uniao,2025-12-31T00:00:00,0.0,1.9150421619415283,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Sao_Sebastiao_do_Paraiso,2025-12-31T00:00:00,0.0,1.6105115413665771,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Serra_do_Salitre,2025-12-31T00:00:00,0.0,1.646639347076416,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Tapira,2025-12-31T00:00:00,0.0,1.9488402605056765,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Tiros,2025-12-31T00:00:00,0.0,2.049423217773437,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Tupaciguara,2025-12-31T00:00:00,0.0,2.638103008270264,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Uberaba,2025-12-31T00:00:00,0.0,2.250186920166016,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Uberlandia,2025-12-31T00:00:00,0.0,1.9449678659439087,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Unai,2025-12-31T00:00:00,0.0,2.619285821914673,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Varginha,2025-12-31T00:00:00,0.0,1.6331241130828855,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Varjao_de_Minas,2025-12-31T00:00:00,0.0,2.5956356525421143,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_1_2025,Virginopolis,2025-12-31T00:00:00,0.0,1.5564417839050293,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_1_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:47:31
V42_cluster_2_2025,Abre_Campo,2024-12-31T00:00:00,1.32,1.6382081508636477,24.106678095730864,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Aimores,2024-12-31T00:00:00,0.9455882352941176,1.853043913841248,95.9673190376436,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Alvarenga,2024-12-31T00:00:00,1.25,2.043684482574463,63.49475860595704,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Andrelandia,2024-12-31T00:00:00,1.5,2.0883212089538574,39.22141393025716,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Antonio_Dias,2024-12-31T00:00:00,1.5,2.0298385620117188,35.32257080078125,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Antonio_Prado_de_Minas,2024-12-31T00:00:00,1.933333333333333,1.5276414155960083,-20.98406471055128,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Bocaiuva,2024-12-31T00:00:00,2.0989010989010994,1.9855235815048216,-5.401756064429981,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Bom_Jesus_do_Amparo,2024-12-31T00:00:00,0.75,1.587667465209961,111.68899536132812,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Campo_Belo,2024-12-31T00:00:00,1.2,2.068144559860229,72.34537998835246,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Capela_Nova,2024-12-31T00:00:00,2.101234567901236,1.924497365951538,-8.411112431213569,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Capitao_Eneas,2024-12-31T00:00:00,3.0,2.483814239501953,-17.206192016601562,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Casa_Grande,2024-12-31T00:00:00,2.522388059701492,3.164734125137329,25.4657907598823,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Cataguases,2024-12-31T00:00:00,1.0,1.533786416053772,53.3786416053772,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Caxambu,2024-12-31T00:00:00,1.795918367346939,2.1674699783325195,20.68866924806073,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Coimbra,2024-12-31T00:00:00,1.5012658227848097,1.888233661651612,25.77610393800787,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Conselheiro_Pena,2024-12-31T00:00:00,1.2,1.6924830675125122,41.040255626042686,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Corrego_Novo,2024-12-31T00:00:00,1.2,2.9187042713165283,143.22535594304404,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Cruzilia,2024-12-31T00:00:00,1.502793296089385,1.930989027023316,28.493321872555228,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Divisopolis,2024-12-31T00:00:00,1.4902200488997548,1.9401968717575075,30.19532740751778,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Esmeraldas,2024-12-31T00:00:00,3.0,2.538358211517334,-15.388059616088867,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Espirito_Santo_do_Dourado,2024-12-31T00:00:00,1.4391752577319588,1.5375406742095947,6.834846273875852,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Eugenopolis,2024-12-31T00:00:00,1.68,2.00549578666687,19.37474920636132,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Felicio_dos_Santos,2024-12-31T00:00:00,2.663299663299664,3.7767276763916016,41.80633626906516,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Formiga,2024-12-31T00:00:00,1.9475763016157988,2.374669551849365,21.92947459256052,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Guaraciaba,2024-12-31T00:00:00,1.8000000000000005,1.9467328786849976,8.15182659361096,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Guiricema,2024-12-31T00:00:00,1.56,2.212571859359741,41.83152944613725,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Iapu,2024-12-31T00:00:00,1.355263157894737,2.052742481231689,51.46449376078485,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Ijaci,2024-12-31T00:00:00,2.19,2.082578182220459,-4.905105834682236,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Imbe_de_Minas,2024-12-31T00:00:00,1.26,1.8961095809936523,50.4848873804486,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Inhapim,2024-12-31T00:00:00,1.2,3.604872465133667,200.4060387611389,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Irai_de_Minas,2024-12-31T00:00:00,2.607944732297064,4.377129554748535,67.83827895360278,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Itamarati_de_Minas,2024-12-31T00:00:00,1.196969696969697,1.5420169830322266,28.826735291299936,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Jequeri,2024-12-31T00:00:00,1.8000000000000005,1.9445947408676147,8.033041159311914,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Lamim,2024-12-31T00:00:00,1.777777777777778,3.116023063659668,75.2762973308563,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Mar_de_Espanha,2024-12-31T00:00:00,1.2625,2.245844841003418,77.88870027749846,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Mata_Verde,2024-12-31T00:00:00,1.5,2.2184042930603027,47.89361953735352,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Monte_Alegre_de_Minas,2024-12-31T00:00:00,2.378787878787879,3.502375602722168,47.23362406347967,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Nova_Era,2024-12-31T00:00:00,1.5,2.736457586288452,82.43050575256348,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Nova_Ponte,2024-12-31T00:00:00,2.306557377049181,3.9955978393554688,73.22776702251849,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Paula_Candido,2024-12-31T00:00:00,1.31970802919708,1.778963565826416,34.79978347246629,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Pecanha,2024-12-31T00:00:00,1.8855421686746991,2.1382317543029785,13.401428502969445,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Pedra_Bonita,2024-12-31T00:00:00,1.2,1.5635311603546145,30.29426336288453,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Pedra_Dourada,2024-12-31T00:00:00,1.2800000000000002,1.3862954378128052,8.304331079125383,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Pedra_do_Anta,2024-12-31T00:00:00,1.379166666666667,1.51599383354187,9.921003036268496,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Piedade_de_Caratinga,2024-12-31T00:00:00,1.68,3.108396291732788,85.02358879361834,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Pocrane,2024-12-31T00:00:00,1.7968750000000002,1.4463064670562744,-19.509900963824737,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Presidente_Bernardes,2024-12-31T00:00:00,1.110144927536232,1.5133230686187744,36.31761323067287,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sabinopolis,2024-12-31T00:00:00,1.2,1.175445318222046,-2.046223481496172,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Santa_Barbara,2024-12-31T00:00:00,1.333333333333333,1.492638111114502,11.947858333587671,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Santo_Antonio_do_Grama,2024-12-31T00:00:00,1.565217391304348,1.826898455619812,16.718512442376873,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sao_Domingos_do_Prata,2024-12-31T00:00:00,1.05,1.514621376991272,44.24965495154971,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sao_Geraldo,2024-12-31T00:00:00,1.3189189189189188,1.7979023456573486,36.31636637156129,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sao_Jose_da_Barra,2024-12-31T00:00:00,1.709523809523809,3.2499117851257324,90.1062604112546,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sao_Jose_do_Alegre,2024-12-31T00:00:00,1.8000000000000005,2.042593479156494,13.4774155086941,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sao_Miguel_do_Anta,2024-12-31T00:00:00,1.55974025974026,1.5901494026184082,1.949628644144383,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sao_Sebastiao_da_Vargem_Alegre,2024-12-31T00:00:00,1.2800000000000002,1.4614818096160889,14.17826637625692,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sao_Sebastiao_do_Anta,2024-12-31T00:00:00,1.8000000000000005,2.12835693359375,18.24205186631943,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Senhora_dos_Remedios,2024-12-31T00:00:00,1.8000000000000005,1.7979943752288818,-0.1114235983954683,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Teixeiras,2024-12-31T00:00:00,1.5,1.764678955078125,17.645263671875,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Visconde_do_Rio_Branco,2024-12-31T00:00:00,1.333333333333333,1.2594108581542969,-5.544185638427714,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Abre_Campo,2025-12-31T00:00:00,0.0,1.399628520011902,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Aimores,2025-12-31T00:00:00,0.0,1.5785976648330688,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Alvarenga,2025-12-31T00:00:00,0.0,1.229200839996338,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Andrelandia,2025-12-31T00:00:00,0.0,1.644315481185913,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Antonio_Dias,2025-12-31T00:00:00,0.0,1.4868773221969604,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Antonio_Prado_de_Minas,2025-12-31T00:00:00,0.0,1.186894178390503,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Bocaiuva,2025-12-31T00:00:00,0.0,2.25327205657959,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Bom_Jesus_do_Amparo,2025-12-31T00:00:00,0.0,1.2120869159698486,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Campo_Belo,2025-12-31T00:00:00,0.0,1.4663830995559692,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Capela_Nova,2025-12-31T00:00:00,0.0,1.8865931034088133,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Capitao_Eneas,2025-12-31T00:00:00,0.0,2.3725898265838623,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Casa_Grande,2025-12-31T00:00:00,0.0,1.9297468662261963,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Cataguases,2025-12-31T00:00:00,0.0,0.9996519684791564,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Caxambu,2025-12-31T00:00:00,0.0,1.6566189527511597,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Coimbra,2025-12-31T00:00:00,0.0,1.638941526412964,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Conselheiro_Pena,2025-12-31T00:00:00,0.0,1.5651121139526367,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Corrego_Novo,2025-12-31T00:00:00,0.0,1.8531343936920168,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Cruzilia,2025-12-31T00:00:00,0.0,1.3934283256530762,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Divisopolis,2025-12-31T00:00:00,0.0,2.206334114074707,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Esmeraldas,2025-12-31T00:00:00,0.0,2.3124840259552,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Espirito_Santo_do_Dourado,2025-12-31T00:00:00,0.0,1.3708409070968628,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Eugenopolis,2025-12-31T00:00:00,0.0,1.4498281478881836,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Felicio_dos_Santos,2025-12-31T00:00:00,0.0,5.080653190612793,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Formiga,2025-12-31T00:00:00,0.0,1.579846739768982,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Guaraciaba,2025-12-31T00:00:00,0.0,1.5100376605987549,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Guiricema,2025-12-31T00:00:00,0.0,1.812080144882202,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Iapu,2025-12-31T00:00:00,0.0,1.4853605031967163,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Ijaci,2025-12-31T00:00:00,0.0,1.7872545719146729,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Imbe_de_Minas,2025-12-31T00:00:00,0.0,1.4794728755950928,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Inhapim,2025-12-31T00:00:00,0.0,2.294912099838257,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Irai_de_Minas,2025-12-31T00:00:00,0.0,4.598679542541504,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Itamarati_de_Minas,2025-12-31T00:00:00,0.0,1.1246511936187744,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Jequeri,2025-12-31T00:00:00,0.0,1.6684248447418213,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Lamim,2025-12-31T00:00:00,0.0,1.8202251195907595,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Mar_de_Espanha,2025-12-31T00:00:00,0.0,1.706060528755188,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Mata_Verde,2025-12-31T00:00:00,0.0,2.099862098693848,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Monte_Alegre_de_Minas,2025-12-31T00:00:00,0.0,3.9578583240509033,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Nova_Era,2025-12-31T00:00:00,0.0,1.925630569458008,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Nova_Ponte,2025-12-31T00:00:00,0.0,3.727222442626953,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Paula_Candido,2025-12-31T00:00:00,0.0,1.5034465789794922,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Pecanha,2025-12-31T00:00:00,0.0,1.968393921852112,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Pedra_Bonita,2025-12-31T00:00:00,0.0,1.2987957000732422,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Pedra_Dourada,2025-12-31T00:00:00,0.0,1.2393810749053955,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Pedra_do_Anta,2025-12-31T00:00:00,0.0,1.2941222190856934,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Piedade_de_Caratinga,2025-12-31T00:00:00,0.0,1.84223735332489,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Pocrane,2025-12-31T00:00:00,0.0,1.3256828784942627,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Presidente_Bernardes,2025-12-31T00:00:00,0.0,1.3403757810592651,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sabinopolis,2025-12-31T00:00:00,0.0,1.1789653301239014,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Santa_Barbara,2025-12-31T00:00:00,0.0,1.3521759510040283,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Santo_Antonio_do_Grama,2025-12-31T00:00:00,0.0,1.604980230331421,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sao_Domingos_do_Prata,2025-12-31T00:00:00,0.0,1.2709484100341797,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sao_Geraldo,2025-12-31T00:00:00,0.0,1.5739383697509766,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sao_Jose_da_Barra,2025-12-31T00:00:00,0.0,2.149350166320801,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sao_Jose_do_Alegre,2025-12-31T00:00:00,0.0,1.745322823524475,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sao_Miguel_do_Anta,2025-12-31T00:00:00,0.0,1.4399335384368896,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sao_Sebastiao_da_Vargem_Alegre,2025-12-31T00:00:00,0.0,1.2086174488067627,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Sao_Sebastiao_do_Anta,2025-12-31T00:00:00,0.0,1.9151687622070312,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Senhora_dos_Remedios,2025-12-31T00:00:00,0.0,1.6275752782821655,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Teixeiras,2025-12-31T00:00:00,0.0,1.3986880779266355,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_2_2025,Visconde_do_Rio_Branco,2025-12-31T00:00:00,0.0,1.0608047246932983,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_2_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:06
V42_cluster_3_2025,Aiuruoca,2024-12-31T00:00:00,1.8000000000000005,1.4278053045272827,-20.67748308181764,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Albertina,2024-12-31T00:00:00,1.56,1.5134382247924805,-2.984729179969204,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Baependi,2024-12-31T00:00:00,1.439779005524862,1.19935405254364,-16.698740019033444,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Bandeira_do_Sul,2024-12-31T00:00:00,1.5,1.5246106386184692,1.6407092412312825,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Borda_da_Mata,2024-12-31T00:00:00,1.5,1.34916889667511,-10.055406888326008,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Brazopolis,2024-12-31T00:00:00,1.501216545012166,1.3481626510620115,-10.195324216128585,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Bueno_Brandao,2024-12-31T00:00:00,1.8000000000000005,1.409595251083374,-21.689152717590343,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Cachoeira_de_Minas,2024-12-31T00:00:00,1.73984375,1.6126294136047363,-7.311825351860682,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Caldas,2024-12-31T00:00:00,1.68,1.3757529258728027,-18.109944888523643,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Cambuquira,2024-12-31T00:00:00,1.7400000000000002,1.399250626564026,-19.583297323906567,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Campanha,2024-12-31T00:00:00,1.5689448441247,1.35558819770813,-13.598734666520407,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Careacu,2024-12-31T00:00:00,1.560169491525424,1.2477495670318604,-20.02474258025014,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Carmo_da_Cachoeira,2024-12-31T00:00:00,1.44,1.464072823524475,1.67172385586633,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Carmo_de_Minas,2024-12-31T00:00:00,1.56,1.3241082429885864,-15.121266475090616,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Carrancas,2024-12-31T00:00:00,1.6428571428571432,1.4495395421981812,-11.7671583009803,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Carvalhopolis,2024-12-31T00:00:00,1.62,1.4057531356811523,-13.225115081410356,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Conceicao_das_Pedras,2024-12-31T00:00:00,1.5598885793871868,1.548720121383667,-0.715977932725647,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Conceicao_do_Rio_Verde,2024-12-31T00:00:00,1.5,1.504791498184204,0.3194332122802734,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Conceicao_dos_Ouros,2024-12-31T00:00:00,1.8000000000000005,1.3548014163970947,-24.73325464460586,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Congonhal,2024-12-31T00:00:00,1.8000000000000005,1.324326992034912,-26.426278220282672,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Cordislandia,2024-12-31T00:00:00,1.439887640449438,1.2989020347595217,-9.79143106235082,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Cristina,2024-12-31T00:00:00,1.619736842105263,1.467146635055542,-9.420678908025014,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Datas,2024-12-31T00:00:00,1.0,0.7707545757293701,-22.92454242706299,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Dom_Vicoso,2024-12-31T00:00:00,1.2,1.314803123474121,9.56692695617676,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Fama,2024-12-31T00:00:00,1.679838709677419,1.5372322797775269,-8.489292994520707,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Fortaleza_de_Minas,2024-12-31T00:00:00,1.5598425196850392,1.5417375564575195,-1.1606917364437128,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Heliodora,2024-12-31T00:00:00,1.5601255886970171,1.2985080480575562,-16.76900517079258,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Ibitiura_de_Minas,2024-12-31T00:00:00,1.360264900662252,1.3478983640670776,-0.9091270817289838,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Inconfidentes,2024-12-31T00:00:00,1.8000000000000005,1.687200665473938,-6.266629695892348,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Ingai,2024-12-31T00:00:00,1.621004566210046,1.4846129417419434,-8.414018523525208,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Itajuba,2024-12-31T00:00:00,1.068965517241379,1.330210566520691,24.43905299709693,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Itutinga,2024-12-31T00:00:00,1.8000000000000005,1.461988925933838,-18.778393003675685,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Jesuania,2024-12-31T00:00:00,1.5,1.260836482048035,-15.944234530131022,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Lambari,2024-12-31T00:00:00,1.56,1.3746360540390017,-11.88230422826914,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Luminarias,2024-12-31T00:00:00,1.380229885057471,1.427495002746582,3.424438073744719,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Maria_da_Fe,2024-12-31T00:00:00,1.477777777777778,1.387373447418213,-6.1175862649329815,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Monsenhor_Paulo,2024-12-31T00:00:00,1.5899728997289972,1.3939476013183594,-12.32884525541595,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Monte_Siao,2024-12-31T00:00:00,1.5,1.4175989627838137,-5.493402481079102,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Natercia,2024-12-31T00:00:00,1.8000000000000005,1.465862274169922,-18.56320699055991,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Olimpio_Noronha,2024-12-31T00:00:00,1.5,1.27020001411438,-15.319999059041342,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Ouro_Fino,2024-12-31T00:00:00,1.5,1.4040803909301758,-6.394640604654948,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Paraisopolis,2024-12-31T00:00:00,1.5066666666666668,1.3934552669525146,-7.514030954479126,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Pedralva,2024-12-31T00:00:00,1.5600790513833993,1.5322754383087158,-1.7821925786407198,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Pirangucu,2024-12-31T00:00:00,1.333333333333333,1.2747399806976318,-4.394501447677591,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Piranguinho,2024-12-31T00:00:00,1.6807610993657498,1.4164292812347412,-15.726911946662533,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Poco_Fundo,2024-12-31T00:00:00,1.380192991366176,1.184861421585083,-14.152482370436116,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Pocos_de_Caldas,2024-12-31T00:00:00,1.7400000000000002,1.494503140449524,-14.109014916694038,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Pouso_Alegre,2024-12-31T00:00:00,1.7878787878787883,1.443695902824402,-19.250907130160595,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Pouso_Alto,2024-12-31T00:00:00,1.53,1.516221523284912,-0.9005540336658769,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Santa_Rita_de_Caldas,2024-12-31T00:00:00,1.8000000000000005,1.7201581001281738,-4.435661103990357,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Santa_Rita_do_Sapucai,2024-12-31T00:00:00,1.5,1.3887144327163696,-7.419037818908691,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Sao_Bento_Abade,2024-12-31T00:00:00,1.5,1.405739188194275,-6.284054120381673,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Sao_Goncalo_do_Sapucai,2024-12-31T00:00:00,1.5,1.3770606517791748,-8.195956548055012,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Sao_Joao_da_Mata,2024-12-31T00:00:00,1.32,1.093336820602417,-17.171452984665383,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Sao_Joao_del_Rei,2024-12-31T00:00:00,1.80168776371308,1.682809829711914,-6.598142941048306,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Sao_Lourenco,2024-12-31T00:00:00,1.3799999999999997,1.3946818113327026,1.0638993719349978,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Sao_Sebastiao_da_Bela_Vista,2024-12-31T00:00:00,1.44,1.3782994747161863,-4.284758700264821,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Sao_Tome_das_Letras,2024-12-31T00:00:00,2.1,1.3088493347167969,-37.673841203962056,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Senador_Jose_Bento,2024-12-31T00:00:00,0.9,1.2954297065734863,43.936634063720696,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Serrania,2024-12-31T00:00:00,1.32,1.25137460231781,-5.198893763802273,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Silvianopolis,2024-12-31T00:00:00,1.56025641025641,1.037119746208191,-33.52889054705103,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Soledade_de_Minas,2024-12-31T00:00:00,1.5,1.451061487197876,-3.2625675201416016,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Tocos_do_Moji,2024-12-31T00:00:00,1.5,1.4056367874145508,-6.290880839029948,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Tres_Coracoes,2024-12-31T00:00:00,1.56,1.3569304943084717,-13.017276005867204,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Turvolandia,2024-12-31T00:00:00,1.44,1.4304099082946775,-0.6659785906473759,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Virginia,2024-12-31T00:00:00,1.6599999999999997,1.4824140071868896,-10.69795137428374,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Aiuruoca,2025-12-31T00:00:00,0.0,1.4311916828155518,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Albertina,2025-12-31T00:00:00,0.0,1.5034542083740234,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Baependi,2025-12-31T00:00:00,0.0,1.1795415878295898,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Bandeira_do_Sul,2025-12-31T00:00:00,0.0,1.4137881994247437,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Borda_da_Mata,2025-12-31T00:00:00,0.0,1.2836472988128662,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Brazopolis,2025-12-31T00:00:00,0.0,1.309775471687317,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Bueno_Brandao,2025-12-31T00:00:00,0.0,1.3978960514068604,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Cachoeira_de_Minas,2025-12-31T00:00:00,0.0,1.5860304832458496,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Caldas,2025-12-31T00:00:00,0.0,1.4289079904556274,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Cambuquira,2025-12-31T00:00:00,0.0,1.362555980682373,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Campanha,2025-12-31T00:00:00,0.0,1.3286499977111816,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Careacu,2025-12-31T00:00:00,0.0,1.2604308128356934,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Carmo_da_Cachoeira,2025-12-31T00:00:00,0.0,1.2906485795974731,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Carmo_de_Minas,2025-12-31T00:00:00,0.0,1.323975920677185,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Carrancas,2025-12-31T00:00:00,0.0,1.35906720161438,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Carvalhopolis,2025-12-31T00:00:00,0.0,1.375190496444702,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Conceicao_das_Pedras,2025-12-31T00:00:00,0.0,1.483464002609253,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Conceicao_do_Rio_Verde,2025-12-31T00:00:00,0.0,1.453700304031372,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Conceicao_dos_Ouros,2025-12-31T00:00:00,0.0,1.3648135662078855,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Congonhal,2025-12-31T00:00:00,0.0,1.327082872390747,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Cordislandia,2025-12-31T00:00:00,0.0,1.2543127536773682,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Cristina,2025-12-31T00:00:00,0.0,1.4999150037765503,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Datas,2025-12-31T00:00:00,0.0,0.9810713529586792,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Dom_Vicoso,2025-12-31T00:00:00,0.0,1.2214781045913696,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Fama,2025-12-31T00:00:00,0.0,1.473888874053955,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Fortaleza_de_Minas,2025-12-31T00:00:00,0.0,1.4240022897720337,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Heliodora,2025-12-31T00:00:00,0.0,1.2743788957595823,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Ibitiura_de_Minas,2025-12-31T00:00:00,0.0,1.291218400001526,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Inconfidentes,2025-12-31T00:00:00,0.0,1.5838284492492676,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Ingai,2025-12-31T00:00:00,0.0,1.4210546016693115,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Itajuba,2025-12-31T00:00:00,0.0,1.1519910097122192,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Itutinga,2025-12-31T00:00:00,0.0,1.4599679708480835,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Jesuania,2025-12-31T00:00:00,0.0,1.25864839553833,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Lambari,2025-12-31T00:00:00,0.0,1.3346842527389526,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Luminarias,2025-12-31T00:00:00,0.0,1.321017503738403,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Maria_da_Fe,2025-12-31T00:00:00,0.0,1.3761210441589355,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Monsenhor_Paulo,2025-12-31T00:00:00,0.0,1.3180606365203855,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Monte_Siao,2025-12-31T00:00:00,0.0,1.3677440881729126,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Natercia,2025-12-31T00:00:00,0.0,1.4603090286254885,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Olimpio_Noronha,2025-12-31T00:00:00,0.0,1.2557427883148191,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Ouro_Fino,2025-12-31T00:00:00,0.0,1.3144261837005615,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Paraisopolis,2025-12-31T00:00:00,0.0,1.261044144630432,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Pedralva,2025-12-31T00:00:00,0.0,1.4550622701644895,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Pirangucu,2025-12-31T00:00:00,0.0,1.277786135673523,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Piranguinho,2025-12-31T00:00:00,0.0,1.4199247360229492,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Poco_Fundo,2025-12-31T00:00:00,0.0,1.122517704963684,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Pocos_de_Caldas,2025-12-31T00:00:00,0.0,1.3791708946228027,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Pouso_Alegre,2025-12-31T00:00:00,0.0,1.447559118270874,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Pouso_Alto,2025-12-31T00:00:00,0.0,1.4733970165252686,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Santa_Rita_de_Caldas,2025-12-31T00:00:00,0.0,1.720728874206543,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Santa_Rita_do_Sapucai,2025-12-31T00:00:00,0.0,1.316214919090271,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Sao_Bento_Abade,2025-12-31T00:00:00,0.0,1.3213967084884644,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Sao_Goncalo_do_Sapucai,2025-12-31T00:00:00,0.0,1.262840747833252,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Sao_Joao_da_Mata,2025-12-31T00:00:00,0.0,1.0388314723968506,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Sao_Joao_del_Rei,2025-12-31T00:00:00,0.0,1.669783592224121,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Sao_Lourenco,2025-12-31T00:00:00,0.0,1.3243286609649658,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Sao_Sebastiao_da_Bela_Vista,2025-12-31T00:00:00,0.0,1.3600223064422607,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Sao_Tome_das_Letras,2025-12-31T00:00:00,0.0,1.2953178882598877,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Senador_Jose_Bento,2025-12-31T00:00:00,0.0,1.1106396913528442,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Serrania,2025-12-31T00:00:00,0.0,1.083896279335022,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Silvianopolis,2025-12-31T00:00:00,0.0,1.0612587928771973,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Soledade_de_Minas,2025-12-31T00:00:00,0.0,1.3725461959838867,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Tocos_do_Moji,2025-12-31T00:00:00,0.0,1.361365795135498,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Tres_Coracoes,2025-12-31T00:00:00,0.0,1.2685394287109375,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Turvolandia,2025-12-31T00:00:00,0.0,1.3249748945236206,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_3_2025,Virginia,2025-12-31T00:00:00,0.0,1.463043451309204,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_3_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:48:41
V42_cluster_4_2025,Abadia_dos_Dourados,2024-12-31T00:00:00,2.6116071428571423,2.116466760635376,-18.959221473106954,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Acucena,2024-12-31T00:00:00,1.6666666666666672,1.7099177837371826,2.595067024230925,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Agua_Boa,2024-12-31T00:00:00,1.7064935064935067,1.1641204357147217,-31.782896841679182,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Aguas_Vermelhas,2024-12-31T00:00:00,3.0,3.1232595443725586,4.108651479085287,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Angelandia,2024-12-31T00:00:00,1.870967741935484,2.173311233520508,16.15973834333748,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Aricanduva,2024-12-31T00:00:00,1.320245398773006,1.3224256038665771,0.16513635234762408,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Ataleia,2024-12-31T00:00:00,0.7529411764705883,0.9052028059959412,20.222247671335918,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Bandeira,2024-12-31T00:00:00,1.15,1.2665214538574219,10.132300335427997,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Berilo,2024-12-31T00:00:00,1.6818181818181819,1.9474937915802002,15.796928148011899,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Berizal,2024-12-31T00:00:00,3.5516129032258075,4.182986259460449,17.77708814102987,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Bonfinopolis_de_Minas,2024-12-31T00:00:00,2.1,3.5178329944610596,67.51585687909807,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Buritis,2024-12-31T00:00:00,2.69967707212056,3.1440484523773193,16.46016795289192,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Buritizeiro,2024-12-31T00:00:00,2.7000000000000006,3.306882619857788,22.47713406880694,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Capelinha,2024-12-31T00:00:00,1.62007678089009,1.6943202018737793,4.58271002087315,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Carai,2024-12-31T00:00:00,1.2,1.309582233428955,9.131852785746261,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Caranaiba,2024-12-31T00:00:00,1.5,2.398970127105713,59.931341807047524,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Catuji,2024-12-31T00:00:00,1.138461538461538,1.396578073501587,22.672398348112416,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Congonhas_do_Norte,2024-12-31T00:00:00,1.2,1.043282151222229,-13.059820731480915,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Coroaci,2024-12-31T00:00:00,2.1,2.594736099243164,23.55886186872209,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Cuparaque,2024-12-31T00:00:00,1.020408163265306,1.2782396078109741,25.267481565475485,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Diamantina,2024-12-31T00:00:00,2.967637540453075,3.64494252204895,22.82303591200931,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Formoso,2024-12-31T00:00:00,3.6000000000000005,3.528146505355835,-1.9959304067823769,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Franciscopolis,2024-12-31T00:00:00,0.9,1.636411428451538,81.8234920501709,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Frei_Gaspar,2024-12-31T00:00:00,1.2,1.3499915599822998,12.499296665191654,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Grao_Mogol,2024-12-31T00:00:00,2.0333333333333337,1.374560832977295,-32.39864755849371,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Indaiabira,2024-12-31T00:00:00,2.511111111111111,2.4132606983184814,-3.896697854573746,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Itabirinha,2024-12-31T00:00:00,0.8918918918918919,0.9909040927886963,11.101367979338676,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Itacambira,2024-12-31T00:00:00,0.5,0.7712065577507019,54.24131155014038,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Itaipe,2024-12-31T00:00:00,0.9,0.9473559856414795,5.261776182386608,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Itamarandiba,2024-12-31T00:00:00,2.1352380952380954,2.331859827041626,9.208421873046705,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Itambacuri,2024-12-31T00:00:00,1.084388185654009,1.0404614210128784,-4.050833937722931,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Jequitinhonha,2024-12-31T00:00:00,2.4,2.6876800060272217,11.986666917800907,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Joao_Pinheiro,2024-12-31T00:00:00,3.0,3.033604860305786,1.120162010192871,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Jose_Goncalves_de_Minas,2024-12-31T00:00:00,1.5,1.4895037412643433,-0.699750582377116,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Juiz_de_Fora,2024-12-31T00:00:00,1.166666666666667,1.2683902978897095,8.719168390546498,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Ladainha,2024-12-31T00:00:00,1.12,1.0332921743392944,-7.741770148277291,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Malacacheta,2024-12-31T00:00:00,1.5,1.784605622291565,18.973708152770996,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Mantena,2024-12-31T00:00:00,1.2,1.0100656747817993,-15.827860434850056,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Minas_Novas,2024-12-31T00:00:00,1.6204545454545451,1.525155782699585,-5.880989566926016,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Monte_Formoso,2024-12-31T00:00:00,0.6800000000000002,0.7435372471809387,9.343712820726257,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Ninheira,2024-12-31T00:00:00,2.819758276405675,2.758277654647827,-2.1803507865297145,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Nova_Belem,2024-12-31T00:00:00,0.9,1.0450297594070435,16.114417711893715,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Novo_Cruzeiro,2024-12-31T00:00:00,1.7997542997542997,1.213748574256897,-32.56031812661337,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Novorizonte,2024-12-31T00:00:00,1.333333333333333,1.663817286491394,24.78629648685458,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Ouro_Verde_de_Minas,2024-12-31T00:00:00,2.5,2.706949472427368,8.277978897094727,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Padre_Paraiso,2024-12-31T00:00:00,0.7212121212121212,0.6579675078392029,-8.769211097925648,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Paracatu,2024-12-31T00:00:00,2.7003236245954687,2.525299072265625,-6.481613934554377,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Pedra_Azul,2024-12-31T00:00:00,0.9,1.470591425895691,63.39904732174343,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Pirapora,2024-12-31T00:00:00,2.7000000000000006,3.760204315185547,39.26682648835356,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Pote,2024-12-31T00:00:00,1.125,1.2901477813720703,14.679802788628471,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Rio_Pardo_de_Minas,2024-12-31T00:00:00,3.064285714285715,2.786234140396118,-9.073944136257236,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Santa_Barbara_do_Monte_Verde,2024-12-31T00:00:00,1.3,1.571078896522522,20.852222809424763,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Santa_Maria_do_Suacui,2024-12-31T00:00:00,1.333333333333333,0.899665892124176,-32.525058090686784,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Santo_Antonio_do_Retiro,2024-12-31T00:00:00,1.9666666666666672,1.1143172979354858,-43.33979841006006,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Sao_Goncalo_do_Rio_Preto,2024-12-31T00:00:00,2.5238095238095246,4.260270118713379,68.80315564713383,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Sao_Joao_do_Manteninha,2024-12-31T00:00:00,0.9,1.2417502403259277,37.97224892510308,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Sao_Joao_do_Paraiso,2024-12-31T00:00:00,3.5396226415094336,3.309858560562134,-6.4912027133298995,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Sao_Sebastiao_do_Maranhao,2024-12-31T00:00:00,1.380952380952381,1.2154459953308105,-11.984945165699925,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Senador_Modestino_Goncalves,2024-12-31T00:00:00,2.706666666666667,3.838167190551758,41.804206547478714,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Setubinha,2024-12-31T00:00:00,1.020238095238095,1.148209810256958,12.543318624952743,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Taiobeiras,2024-12-31T00:00:00,3.1206349206349207,3.7129383087158203,18.98022047258224,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Teofilo_Otoni,2024-12-31T00:00:00,1.05,1.1859185695648193,12.944625672839932,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Turmalina,2024-12-31T00:00:00,1.8507692307692305,1.3696353435516357,-25.996427821399553,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Urucuia,2024-12-31T00:00:00,1.920433996383363,2.2293519973754883,16.085843177838544,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Vargem_Grande_do_Rio_Pardo,2024-12-31T00:00:00,1.8000000000000003,1.958672046661377,8.815113703409814,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Varzea_da_Palma,2024-12-31T00:00:00,3.0,3.111280679702759,3.7093559900919595,validacao,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Abadia_dos_Dourados,2025-12-31T00:00:00,0.0,2.301433563232422,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Acucena,2025-12-31T00:00:00,0.0,1.494739055633545,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Agua_Boa,2025-12-31T00:00:00,0.0,1.3107877969741821,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Aguas_Vermelhas,2025-12-31T00:00:00,0.0,3.034113645553589,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Angelandia,2025-12-31T00:00:00,0.0,2.1080517768859863,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Aricanduva,2025-12-31T00:00:00,0.0,1.222247838973999,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Ataleia,2025-12-31T00:00:00,0.0,0.8126569986343384,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Bandeira,2025-12-31T00:00:00,0.0,1.1439261436462402,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Berilo,2025-12-31T00:00:00,0.0,1.8470407724380493,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Berizal,2025-12-31T00:00:00,0.0,3.4585626125335693,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Bonfinopolis_de_Minas,2025-12-31T00:00:00,0.0,2.8174455165863037,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Buritis,2025-12-31T00:00:00,0.0,3.007613182067871,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Buritizeiro,2025-12-31T00:00:00,0.0,2.9467689990997314,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Capelinha,2025-12-31T00:00:00,0.0,1.7006075382232666,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Carai,2025-12-31T00:00:00,0.0,1.2658915519714355,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Caranaiba,2025-12-31T00:00:00,0.0,2.3300135135650635,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Catuji,2025-12-31T00:00:00,0.0,1.2941755056381226,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Congonhas_do_Norte,2025-12-31T00:00:00,0.0,1.0415189266204834,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Coroaci,2025-12-31T00:00:00,0.0,2.2420103549957275,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Cuparaque,2025-12-31T00:00:00,0.0,1.2099761962890625,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Diamantina,2025-12-31T00:00:00,0.0,3.191688060760498,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Formoso,2025-12-31T00:00:00,0.0,3.286749839782715,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Franciscopolis,2025-12-31T00:00:00,0.0,1.4489216804504395,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Frei_Gaspar,2025-12-31T00:00:00,0.0,1.1878197193145752,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Grao_Mogol,2025-12-31T00:00:00,0.0,1.4193296432495117,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Indaiabira,2025-12-31T00:00:00,0.0,2.305779457092285,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Itabirinha,2025-12-31T00:00:00,0.0,0.9610527753829956,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Itacambira,2025-12-31T00:00:00,0.0,0.6466820240020752,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Itaipe,2025-12-31T00:00:00,0.0,0.9267150163650513,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Itamarandiba,2025-12-31T00:00:00,0.0,1.9450584650039673,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Itambacuri,2025-12-31T00:00:00,0.0,0.9977349042892456,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Jequitinhonha,2025-12-31T00:00:00,0.0,2.546778917312622,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Joao_Pinheiro,2025-12-31T00:00:00,0.0,2.9695465564727783,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Jose_Goncalves_de_Minas,2025-12-31T00:00:00,0.0,1.532271146774292,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Juiz_de_Fora,2025-12-31T00:00:00,0.0,1.3195087909698486,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Ladainha,2025-12-31T00:00:00,0.0,1.0040295124053955,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Malacacheta,2025-12-31T00:00:00,0.0,1.646150827407837,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Mantena,2025-12-31T00:00:00,0.0,1.0714105367660522,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Minas_Novas,2025-12-31T00:00:00,0.0,1.5827393531799316,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Monte_Formoso,2025-12-31T00:00:00,0.0,0.7161662578582764,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Ninheira,2025-12-31T00:00:00,0.0,2.889209508895874,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Nova_Belem,2025-12-31T00:00:00,0.0,0.9814206957817078,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Novo_Cruzeiro,2025-12-31T00:00:00,0.0,1.4020699262619019,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Novorizonte,2025-12-31T00:00:00,0.0,1.4005379676818848,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Ouro_Verde_de_Minas,2025-12-31T00:00:00,0.0,2.385153293609619,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Padre_Paraiso,2025-12-31T00:00:00,0.0,0.6707004308700562,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Paracatu,2025-12-31T00:00:00,0.0,2.512007474899292,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Pedra_Azul,2025-12-31T00:00:00,0.0,1.286158800125122,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Pirapora,2025-12-31T00:00:00,0.0,3.273332118988037,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Pote,2025-12-31T00:00:00,0.0,1.060752511024475,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Rio_Pardo_de_Minas,2025-12-31T00:00:00,0.0,2.826190233230591,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Santa_Barbara_do_Monte_Verde,2025-12-31T00:00:00,0.0,1.6492606401443481,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Santa_Maria_do_Suacui,2025-12-31T00:00:00,0.0,1.0344414710998535,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Santo_Antonio_do_Retiro,2025-12-31T00:00:00,0.0,1.3601864576339722,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Sao_Goncalo_do_Rio_Preto,2025-12-31T00:00:00,0.0,3.1758928298950195,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Sao_Joao_do_Manteninha,2025-12-31T00:00:00,0.0,1.0443992614746094,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Sao_Joao_do_Paraiso,2025-12-31T00:00:00,0.0,3.569161891937256,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Sao_Sebastiao_do_Maranhao,2025-12-31T00:00:00,0.0,1.2783780097961426,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Senador_Modestino_Goncalves,2025-12-31T00:00:00,0.0,2.871762990951538,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Setubinha,2025-12-31T00:00:00,0.0,1.118834376335144,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Taiobeiras,2025-12-31T00:00:00,0.0,3.172316789627075,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Teofilo_Otoni,2025-12-31T00:00:00,0.0,0.9853256940841675,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Turmalina,2025-12-31T00:00:00,0.0,1.4662554264068604,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Urucuia,2025-12-31T00:00:00,0.0,2.0695691108703613,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Vargem_Grande_do_Rio_Pardo,2025-12-31T00:00:00,0.0,1.8771191835403442,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
V42_cluster_4_2025,Varzea_da_Palma,2025-12-31T00:00:00,0.0,2.9795172214508057,0.0,teste,V42,LSTM,"./Treinos/26_11_2025_todas_as_features/Modelos/V42_cluster_4_2025_(2025)
    Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2024 e testado com os dados de 2025.
    Para esse treinamento foi utilizado o dataset V42.
    O dataset foi dividido em 5 Clusters.
Nesta versão foram usadas todas as features disponíveis no dataset V42.

    O modelo foi treinado com os seguintes Hiperparâmetros:
    input_size: 6  (-1 significa que usou a janela de contexto maxima)
    h: 1 (horizonte de previsão)

    encoder_n_layers = 4
    learning_rate: 0.00013034723280377263
    encoder_hidden_size: 192
    decoder_layers: 1
    decoder_hidden_size: 64
    batch_size: 32
    dropout: 0.3
    weight_decay: 0.0001
    steps: 1000
    ",2025-11-26T23:49:15
