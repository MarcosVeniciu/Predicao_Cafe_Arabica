treino_id,unique_id,ds,y,y_pred,diferença_%,flag,dataset,modelo,comentario,data_treino
IBGE Cluster 0 - Metodo 1 (2023),Alpinopolis,2012-12-31T00:00:00,2.7,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alpinopolis,2013-12-31T00:00:00,2.71,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alpinopolis,2014-12-31T00:00:00,1.68,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alpinopolis,2015-12-31T00:00:00,1.82,3.0,64.84,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alpinopolis,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alpinopolis,2017-12-31T00:00:00,2.06,2.0,2.91,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alpinopolis,2018-12-31T00:00:00,2.23,2.0,10.31,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alpinopolis,2019-12-31T00:00:00,2.1,2.0,4.76,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alpinopolis,2020-12-31T00:00:00,2.77,2.0,27.8,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alpinopolis,2021-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Caparao,2012-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Caparao,2013-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Caparao,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Caparao,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Caparao,2016-12-31T00:00:00,2.1,1.0,52.38,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Caparao,2017-12-31T00:00:00,1.37,1.0,27.01,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Caparao,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Caparao,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Caparao,2020-12-31T00:00:00,1.92,1.0,47.92,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Caparao,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Jequitiba,2012-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Jequitiba,2013-12-31T00:00:00,1.62,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Jequitiba,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Jequitiba,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Jequitiba,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Jequitiba,2017-12-31T00:00:00,1.53,1.0,34.64,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Jequitiba,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Jequitiba,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Jequitiba,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Jequitiba,2021-12-31T00:00:00,0.9,2.0,122.22,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Areado,2012-12-31T00:00:00,1.95,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Areado,2013-12-31T00:00:00,1.86,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Areado,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Areado,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Areado,2016-12-31T00:00:00,2.04,2.0,1.96,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Areado,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Areado,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Areado,2019-12-31T00:00:00,1.92,2.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Areado,2020-12-31T00:00:00,2.08,2.0,3.85,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Areado,2021-12-31T00:00:00,1.52,2.0,31.58,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Bom_Jesus_da_Penha,2012-12-31T00:00:00,2.58,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Bom_Jesus_da_Penha,2013-12-31T00:00:00,2.44,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Bom_Jesus_da_Penha,2014-12-31T00:00:00,2.0,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Bom_Jesus_da_Penha,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Bom_Jesus_da_Penha,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Bom_Jesus_da_Penha,2017-12-31T00:00:00,2.22,2.0,9.91,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Bom_Jesus_da_Penha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Bom_Jesus_da_Penha,2019-12-31T00:00:00,2.22,2.0,9.91,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Bom_Jesus_da_Penha,2020-12-31T00:00:00,1.99,2.0,0.5,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Bom_Jesus_da_Penha,2021-12-31T00:00:00,1.52,2.0,31.58,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Botelhos,2012-12-31T00:00:00,1.38,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Botelhos,2013-12-31T00:00:00,1.56,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Botelhos,2014-12-31T00:00:00,1.74,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Botelhos,2015-12-31T00:00:00,1.44,2.0,38.89,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Botelhos,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Botelhos,2017-12-31T00:00:00,1.88,1.0,46.81,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Botelhos,2018-12-31T00:00:00,2.07,2.0,3.38,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Botelhos,2019-12-31T00:00:00,1.92,2.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Botelhos,2020-12-31T00:00:00,1.72,2.0,16.28,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Botelhos,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Cabo_Verde,2012-12-31T00:00:00,2.07,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Cabo_Verde,2013-12-31T00:00:00,2.04,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Cabo_Verde,2014-12-31T00:00:00,1.68,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Cabo_Verde,2015-12-31T00:00:00,1.98,2.0,1.01,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Cabo_Verde,2016-12-31T00:00:00,1.65,2.0,21.21,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Cabo_Verde,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Cabo_Verde,2018-12-31T00:00:00,1.61,2.0,24.22,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Cabo_Verde,2019-12-31T00:00:00,2.04,2.0,1.96,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Cabo_Verde,2020-12-31T00:00:00,2.25,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Cabo_Verde,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caiana,2012-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caiana,2013-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caiana,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caiana,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caiana,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caiana,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caiana,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caiana,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caiana,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caiana,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Campestre,2012-12-31T00:00:00,1.35,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Campestre,2013-12-31T00:00:00,1.31,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Campestre,2014-12-31T00:00:00,1.26,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Campestre,2015-12-31T00:00:00,1.91,1.0,47.64,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Campestre,2016-12-31T00:00:00,1.85,2.0,8.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Campestre,2017-12-31T00:00:00,2.11,2.0,5.21,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Campestre,2018-12-31T00:00:00,1.54,2.0,29.87,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Campestre,2019-12-31T00:00:00,1.55,2.0,29.03,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Campestre,2020-12-31T00:00:00,2.14,2.0,6.54,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Campestre,2021-12-31T00:00:00,1.29,2.0,55.04,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caparao,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caparao,2013-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caparao,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caparao,2015-12-31T00:00:00,1.02,1.0,1.96,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caparao,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caparao,2017-12-31T00:00:00,1.45,1.0,31.03,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caparao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caparao,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caparao,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caparao,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caputira,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caputira,2013-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caputira,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caputira,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caputira,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caputira,2017-12-31T00:00:00,0.77,1.0,29.87,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caputira,2018-12-31T00:00:00,1.2,2.0,66.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caputira,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caputira,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caputira,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Carangola,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Carangola,2013-12-31T00:00:00,0.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Carangola,2014-12-31T00:00:00,0.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Carangola,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Carangola,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Carangola,2017-12-31T00:00:00,1.1,1.0,9.09,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Carangola,2018-12-31T00:00:00,1.56,1.0,35.9,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Carangola,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Carangola,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Carangola,2021-12-31T00:00:00,1.02,1.0,1.96,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Chale,2012-12-31T00:00:00,1.14,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Chale,2013-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Chale,2014-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Chale,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Chale,2016-12-31T00:00:00,1.42,2.0,40.85,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Chale,2017-12-31T00:00:00,0.93,1.0,7.53,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Chale,2018-12-31T00:00:00,1.38,1.0,27.54,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Chale,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Chale,2020-12-31T00:00:00,1.71,1.0,41.52,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Chale,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Conceicao_da_Aparecida,2012-12-31T00:00:00,1.95,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Conceicao_da_Aparecida,2013-12-31T00:00:00,1.7,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Conceicao_da_Aparecida,2014-12-31T00:00:00,1.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Conceicao_da_Aparecida,2015-12-31T00:00:00,2.52,2.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Conceicao_da_Aparecida,2016-12-31T00:00:00,1.99,2.0,0.5,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Conceicao_da_Aparecida,2017-12-31T00:00:00,1.85,2.0,8.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Conceicao_da_Aparecida,2018-12-31T00:00:00,2.03,2.0,1.48,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Conceicao_da_Aparecida,2019-12-31T00:00:00,1.97,2.0,1.52,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Conceicao_da_Aparecida,2020-12-31T00:00:00,2.24,2.0,10.71,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Conceicao_da_Aparecida,2021-12-31T00:00:00,1.62,2.0,23.46,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Divino,2012-12-31T00:00:00,1.32,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Divino,2013-12-31T00:00:00,1.56,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Divino,2014-12-31T00:00:00,0.89,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Divino,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Divino,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Divino,2017-12-31T00:00:00,1.19,1.0,15.97,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Divino,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Divino,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Divino,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Divino,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Durande,2012-12-31T00:00:00,1.56,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Durande,2013-12-31T00:00:00,2.22,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Durande,2014-12-31T00:00:00,1.44,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Durande,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Durande,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Durande,2017-12-31T00:00:00,1.33,2.0,50.38,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Durande,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Durande,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Durande,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Durande,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ervalia,2012-12-31T00:00:00,1.38,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ervalia,2013-12-31T00:00:00,1.92,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ervalia,2014-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ervalia,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ervalia,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ervalia,2017-12-31T00:00:00,1.36,1.0,26.47,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ervalia,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ervalia,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ervalia,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ervalia,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Espera_Feliz,2012-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Espera_Feliz,2013-12-31T00:00:00,1.92,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Espera_Feliz,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Espera_Feliz,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Espera_Feliz,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Espera_Feliz,2017-12-31T00:00:00,1.21,1.0,17.36,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Espera_Feliz,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Espera_Feliz,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Espera_Feliz,2020-12-31T00:00:00,2.04,2.0,1.96,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Espera_Feliz,2021-12-31T00:00:00,0.78,1.0,28.21,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guape,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guape,2013-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guape,2014-12-31T00:00:00,1.44,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guape,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guape,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guape,2017-12-31T00:00:00,1.92,2.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guape,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guape,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guape,2020-12-31T00:00:00,1.85,2.0,8.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guape,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaranesia,2012-12-31T00:00:00,1.98,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaranesia,2013-12-31T00:00:00,1.52,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaranesia,2014-12-31T00:00:00,1.86,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaranesia,2015-12-31T00:00:00,1.65,2.0,21.21,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaranesia,2016-12-31T00:00:00,2.63,2.0,23.95,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaranesia,2017-12-31T00:00:00,1.36,2.0,47.06,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaranesia,2018-12-31T00:00:00,1.7,2.0,17.65,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaranesia,2019-12-31T00:00:00,1.26,2.0,58.73,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaranesia,2020-12-31T00:00:00,1.56,1.0,35.9,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaranesia,2021-12-31T00:00:00,1.03,1.0,2.91,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaxupe,2012-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaxupe,2013-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaxupe,2014-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaxupe,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaxupe,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaxupe,2017-12-31T00:00:00,1.51,2.0,32.45,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaxupe,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaxupe,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaxupe,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaxupe,2021-12-31T00:00:00,1.02,1.0,1.96,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guimarania,2012-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guimarania,2013-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guimarania,2014-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guimarania,2015-12-31T00:00:00,2.4,2.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guimarania,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guimarania,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guimarania,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guimarania,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guimarania,2020-12-31T00:00:00,2.15,2.0,6.98,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guimarania,2021-12-31T00:00:00,1.54,2.0,29.87,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ibitiura_de_Minas,2012-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ibitiura_de_Minas,2013-12-31T00:00:00,2.1,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ibitiura_de_Minas,2014-12-31T00:00:00,2.1,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ibitiura_de_Minas,2015-12-31T00:00:00,1.92,2.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ibitiura_de_Minas,2016-12-31T00:00:00,1.76,2.0,13.64,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ibitiura_de_Minas,2017-12-31T00:00:00,1.3,2.0,53.85,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ibitiura_de_Minas,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ibitiura_de_Minas,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ibitiura_de_Minas,2020-12-31T00:00:00,1.68,1.0,40.48,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ibitiura_de_Minas,2021-12-31T00:00:00,1.92,1.0,47.92,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Jacui,2012-12-31T00:00:00,1.65,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Jacui,2013-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Jacui,2014-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Jacui,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Jacui,2016-12-31T00:00:00,1.7,1.0,41.18,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Jacui,2017-12-31T00:00:00,1.73,2.0,15.61,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Jacui,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Jacui,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Jacui,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Jacui,2021-12-31T00:00:00,0.96,2.0,108.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Lajinha,2012-12-31T00:00:00,1.14,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Lajinha,2013-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Lajinha,2014-12-31T00:00:00,0.96,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Lajinha,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Lajinha,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Lajinha,2017-12-31T00:00:00,1.04,1.0,3.85,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Lajinha,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Lajinha,2019-12-31T00:00:00,1.06,1.0,5.66,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Lajinha,2020-12-31T00:00:00,1.92,1.0,47.92,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Lajinha,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhuacu,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhuacu,2013-12-31T00:00:00,1.44,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhuacu,2014-12-31T00:00:00,0.96,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhuacu,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhuacu,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhuacu,2017-12-31T00:00:00,1.43,1.0,30.07,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhuacu,2018-12-31T00:00:00,1.44,1.0,30.56,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhuacu,2019-12-31T00:00:00,1.02,1.0,1.96,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhuacu,2020-12-31T00:00:00,1.68,1.0,40.48,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhuacu,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhumirim,2012-12-31T00:00:00,1.56,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhumirim,2013-12-31T00:00:00,1.92,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhumirim,2014-12-31T00:00:00,1.26,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhumirim,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhumirim,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhumirim,2017-12-31T00:00:00,1.62,1.0,38.27,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhumirim,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhumirim,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhumirim,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhumirim,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Martins_Soares,2012-12-31T00:00:00,2.1,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Martins_Soares,2013-12-31T00:00:00,1.98,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Martins_Soares,2014-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Martins_Soares,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Martins_Soares,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Martins_Soares,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Martins_Soares,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Martins_Soares,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Martins_Soares,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Martins_Soares,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Matipo,2012-12-31T00:00:00,1.14,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Matipo,2013-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Matipo,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Matipo,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Matipo,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Matipo,2017-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Matipo,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Matipo,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Matipo,2020-12-31T00:00:00,1.56,1.0,35.9,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Matipo,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Miradouro,2012-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Miradouro,2013-12-31T00:00:00,2.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Miradouro,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Miradouro,2015-12-31T00:00:00,1.59,1.0,37.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Miradouro,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Miradouro,2017-12-31T00:00:00,1.16,1.0,13.79,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Miradouro,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Miradouro,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Miradouro,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Miradouro,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Mutum,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Mutum,2013-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Mutum,2014-12-31T00:00:00,1.14,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Mutum,2015-12-31T00:00:00,1.15,1.0,13.04,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Mutum,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Mutum,2017-12-31T00:00:00,1.45,1.0,31.03,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Mutum,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Mutum,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Mutum,2020-12-31T00:00:00,1.65,1.0,39.39,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Mutum,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Belem,2012-12-31T00:00:00,0.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Belem,2013-12-31T00:00:00,0.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Belem,2014-12-31T00:00:00,0.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Belem,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Belem,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Belem,2017-12-31T00:00:00,0.48,1.0,108.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Belem,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Belem,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Belem,2020-12-31T00:00:00,1.1,1.0,9.09,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Belem,2021-12-31T00:00:00,1.0,1.0,0.0,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Resende,2012-12-31T00:00:00,2.88,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Resende,2013-12-31T00:00:00,2.18,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Resende,2014-12-31T00:00:00,1.7,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Resende,2015-12-31T00:00:00,2.2,2.0,9.09,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Resende,2016-12-31T00:00:00,2.08,2.0,3.85,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Resende,2017-12-31T00:00:00,1.76,2.0,13.64,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Resende,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Resende,2019-12-31T00:00:00,2.12,2.0,5.66,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Resende,2020-12-31T00:00:00,2.35,2.0,14.89,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Resende,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Orizania,2012-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Orizania,2013-12-31T00:00:00,1.68,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Orizania,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Orizania,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Orizania,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Orizania,2017-12-31T00:00:00,1.47,1.0,31.97,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Orizania,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Orizania,2019-12-31T00:00:00,1.38,2.0,44.93,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Orizania,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Orizania,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Patos_de_Minas,2012-12-31T00:00:00,1.92,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Patos_de_Minas,2013-12-31T00:00:00,2.38,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Patos_de_Minas,2014-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Patos_de_Minas,2015-12-31T00:00:00,2.4,2.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Patos_de_Minas,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Patos_de_Minas,2017-12-31T00:00:00,1.7,2.0,17.65,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Patos_de_Minas,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Patos_de_Minas,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Patos_de_Minas,2020-12-31T00:00:00,1.94,2.0,3.09,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Patos_de_Minas,2021-12-31T00:00:00,1.72,2.0,16.28,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Reduto,2012-12-31T00:00:00,1.26,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Reduto,2013-12-31T00:00:00,1.62,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Reduto,2014-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Reduto,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Reduto,2016-12-31T00:00:00,1.44,1.0,30.56,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Reduto,2017-12-31T00:00:00,1.26,1.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Reduto,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Reduto,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Reduto,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Reduto,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Barbara_do_Leste,2012-12-31T00:00:00,1.02,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Barbara_do_Leste,2013-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Barbara_do_Leste,2014-12-31T00:00:00,0.96,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Barbara_do_Leste,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Barbara_do_Leste,2016-12-31T00:00:00,1.02,1.0,1.96,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Barbara_do_Leste,2017-12-31T00:00:00,0.91,1.0,9.89,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Barbara_do_Leste,2018-12-31T00:00:00,1.74,1.0,42.53,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Barbara_do_Leste,2019-12-31T00:00:00,1.14,1.0,12.28,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Barbara_do_Leste,2020-12-31T00:00:00,1.44,1.0,30.56,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Barbara_do_Leste,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Margarida,2012-12-31T00:00:00,1.38,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Margarida,2013-12-31T00:00:00,1.68,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Margarida,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Margarida,2015-12-31T00:00:00,1.56,1.0,35.9,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Margarida,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Margarida,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Margarida,2018-12-31T00:00:00,1.68,1.0,40.48,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Margarida,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Margarida,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Margarida,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Rita_de_Minas,2012-12-31T00:00:00,0.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Rita_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Rita_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Rita_de_Minas,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Rita_de_Minas,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Rita_de_Minas,2017-12-31T00:00:00,1.25,1.0,20.0,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Rita_de_Minas,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Rita_de_Minas,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Rita_de_Minas,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Rita_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santana_do_Manhuacu,2012-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santana_do_Manhuacu,2013-12-31T00:00:00,1.62,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santana_do_Manhuacu,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santana_do_Manhuacu,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santana_do_Manhuacu,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santana_do_Manhuacu,2017-12-31T00:00:00,1.28,1.0,21.88,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santana_do_Manhuacu,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santana_do_Manhuacu,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santana_do_Manhuacu,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santana_do_Manhuacu,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Domingos_das_Dores,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Domingos_das_Dores,2013-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Domingos_das_Dores,2014-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Domingos_das_Dores,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Domingos_das_Dores,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Domingos_das_Dores,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Domingos_das_Dores,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Domingos_das_Dores,2019-12-31T00:00:00,1.3,2.0,53.85,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Domingos_das_Dores,2020-12-31T00:00:00,1.56,2.0,28.21,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Domingos_das_Dores,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2012-12-31T00:00:00,1.26,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2013-12-31T00:00:00,1.74,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2014-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2017-12-31T00:00:00,1.7,1.0,41.18,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sericita,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sericita,2013-12-31T00:00:00,1.56,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sericita,2014-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sericita,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sericita,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sericita,2017-12-31T00:00:00,1.12,1.0,10.71,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sericita,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sericita,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sericita,2020-12-31T00:00:00,2.52,2.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sericita,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Varjao_de_Minas,2012-12-31T00:00:00,2.4,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Varjao_de_Minas,2013-12-31T00:00:00,2.4,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Varjao_de_Minas,2014-12-31T00:00:00,3.0,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Varjao_de_Minas,2015-12-31T00:00:00,2.4,3.0,25.0,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Varjao_de_Minas,2016-12-31T00:00:00,2.4,3.0,25.0,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Varjao_de_Minas,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Varjao_de_Minas,2018-12-31T00:00:00,2.4,2.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Varjao_de_Minas,2019-12-31T00:00:00,2.49,2.0,19.68,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Varjao_de_Minas,2020-12-31T00:00:00,2.52,2.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Varjao_de_Minas,2021-12-31T00:00:00,2.39,2.0,16.32,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alpinopolis,2022-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Caparao,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Jequitiba,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Areado,2022-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Bom_Jesus_da_Penha,2022-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Botelhos,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Cabo_Verde,2022-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caiana,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Campestre,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caparao,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caputira,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Carangola,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Chale,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Conceicao_da_Aparecida,2022-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Divino,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Durande,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ervalia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Espera_Feliz,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guape,2022-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaranesia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaxupe,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guimarania,2022-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ibitiura_de_Minas,2022-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Jacui,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Lajinha,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhuacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhumirim,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Martins_Soares,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Matipo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Miradouro,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Mutum,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Belem,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Resende,2022-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Orizania,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Patos_de_Minas,2022-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Reduto,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Barbara_do_Leste,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Margarida,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Rita_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santana_do_Manhuacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Domingos_das_Dores,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sericita,2022-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Varjao_de_Minas,2022-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alpinopolis,2023-12-31T00:00:00,1.8000000000000005,1.5216282606124878,-15.47,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Caparao,2023-12-31T00:00:00,1.32,1.3369250297546387,1.28,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Alto_Jequitiba,2023-12-31T00:00:00,1.08,0.9856905341148376,-8.73,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Areado,2023-12-31T00:00:00,1.590034364261168,1.3906276226043701,-12.54,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Bom_Jesus_da_Penha,2023-12-31T00:00:00,1.2,1.6550601720809937,37.92,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Botelhos,2023-12-31T00:00:00,1.080052666227781,1.2604362964630127,16.7,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Cabo_Verde,2023-12-31T00:00:00,1.319976635514019,1.3496286869049072,2.25,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caiana,2023-12-31T00:00:00,1.08,0.9426554441452026,-12.72,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Campestre,2023-12-31T00:00:00,1.319964428634949,1.4536951780319214,10.13,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caparao,2023-12-31T00:00:00,1.32,1.141123652458191,-13.55,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Caputira,2023-12-31T00:00:00,1.380065005417118,1.4591195583343506,5.73,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Carangola,2023-12-31T00:00:00,1.140118343195266,1.0816587209701538,-5.13,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Chale,2023-12-31T00:00:00,1.3799999999999997,1.403458595275879,1.7,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Conceicao_da_Aparecida,2023-12-31T00:00:00,1.679948420373952,1.7716188430786133,5.46,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Divino,2023-12-31T00:00:00,1.02,1.1642037630081177,14.14,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Durande,2023-12-31T00:00:00,1.3799999999999997,1.55866277217865,12.95,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ervalia,2023-12-31T00:00:00,1.380044843049327,1.2851616144180298,-6.88,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Espera_Feliz,2023-12-31T00:00:00,1.08,1.0274497270584106,-4.87,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guape,2023-12-31T00:00:00,1.560674157303371,1.3604047298431396,-12.83,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaranesia,2023-12-31T00:00:00,1.1961439588688951,1.1276166439056396,-5.73,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guaxupe,2023-12-31T00:00:00,1.2,1.062005877494812,-11.5,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Guimarania,2023-12-31T00:00:00,1.7704600484261497,1.6037176847457886,-9.42,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Ibitiura_de_Minas,2023-12-31T00:00:00,1.150344827586207,1.6330630779266357,41.96,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Jacui,2023-12-31T00:00:00,1.32,1.1721152067184448,-11.2,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Lajinha,2023-12-31T00:00:00,1.4399536768963517,1.4222946166992188,-1.23,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhuacu,2023-12-31T00:00:00,1.2,1.1850225925445557,-1.25,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Manhumirim,2023-12-31T00:00:00,1.44,1.316174864768982,-8.6,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Martins_Soares,2023-12-31T00:00:00,1.2,1.350380778312683,12.53,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Matipo,2023-12-31T00:00:00,1.25990675990676,1.2081685066223145,-4.11,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Miradouro,2023-12-31T00:00:00,1.140084388185654,1.1447994709014893,0.41,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Mutum,2023-12-31T00:00:00,1.019985196150999,1.388563632965088,36.14,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Belem,2023-12-31T00:00:00,0.96,0.9908131957054138,3.21,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Nova_Resende,2023-12-31T00:00:00,1.56,1.4281446933746338,-8.45,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Orizania,2023-12-31T00:00:00,1.2,1.2969669103622437,8.08,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Patos_de_Minas,2023-12-31T00:00:00,1.782452316076294,1.8223302364349365,2.24,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Reduto,2023-12-31T00:00:00,0.9,1.300137996673584,44.46,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Barbara_do_Leste,2023-12-31T00:00:00,1.08,1.1925703287124634,10.42,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Margarida,2023-12-31T00:00:00,1.32,1.1981139183044434,-9.23,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santa_Rita_de_Minas,2023-12-31T00:00:00,1.019930675909879,1.1319040060043335,10.98,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Santana_do_Manhuacu,2023-12-31T00:00:00,1.32,1.356628656387329,2.77,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Domingos_das_Dores,2023-12-31T00:00:00,1.296078431372549,1.241005301475525,-4.25,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2023-12-31T00:00:00,1.2,1.1218488216400146,-6.51,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Sericita,2023-12-31T00:00:00,1.26,1.2769584655761719,1.35,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 0 - Metodo 1 (2023),Varjao_de_Minas,2023-12-31T00:00:00,2.3892857142857142,2.417543888092041,1.18,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V22, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00023188514960436685
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.5
weight_decay: 1e-05
steps: 100
",2025-08-25T15:22:16
IBGE Cluster 1 - Metodo 1 (2023),Abre_Campo,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Abre_Campo,2013-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Abre_Campo,2014-12-31T00:00:00,0.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Abre_Campo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Abre_Campo,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Abre_Campo,2017-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Abre_Campo,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Abre_Campo,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Abre_Campo,2020-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Abre_Campo,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alfenas,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alfenas,2013-12-31T00:00:00,1.98,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alfenas,2014-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alfenas,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alfenas,2016-12-31T00:00:00,2.25,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alfenas,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alfenas,2018-12-31T00:00:00,2.34,2.0,14.53,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alfenas,2019-12-31T00:00:00,1.76,2.0,13.64,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alfenas,2020-12-31T00:00:00,2.36,2.0,15.25,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alfenas,2021-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alterosa,2012-12-31T00:00:00,1.59,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alterosa,2013-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alterosa,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alterosa,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alterosa,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alterosa,2017-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alterosa,2018-12-31T00:00:00,1.96,2.0,2.04,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alterosa,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alterosa,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alterosa,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Andradas,2012-12-31T00:00:00,1.98,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Andradas,2013-12-31T00:00:00,1.59,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Andradas,2014-12-31T00:00:00,1.21,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Andradas,2015-12-31T00:00:00,1.89,2.0,5.82,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Andradas,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Andradas,2017-12-31T00:00:00,1.34,1.0,25.37,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Andradas,2018-12-31T00:00:00,1.73,2.0,15.61,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Andradas,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Andradas,2020-12-31T00:00:00,2.57,3.0,16.73,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Andradas,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Angelandia,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Angelandia,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Angelandia,2014-12-31T00:00:00,1.15,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Angelandia,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Angelandia,2016-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Angelandia,2017-12-31T00:00:00,0.94,1.0,6.38,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Angelandia,2018-12-31T00:00:00,1.68,1.0,40.48,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Angelandia,2019-12-31T00:00:00,1.22,1.0,18.03,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Angelandia,2020-12-31T00:00:00,1.84,2.0,8.7,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Angelandia,2021-12-31T00:00:00,1.51,2.0,32.45,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Araponga,2012-12-31T00:00:00,1.14,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Araponga,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Araponga,2014-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Araponga,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Araponga,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Araponga,2017-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Araponga,2018-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Araponga,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Araponga,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Araponga,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Boa_Esperanca,2012-12-31T00:00:00,1.56,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Boa_Esperanca,2013-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Boa_Esperanca,2014-12-31T00:00:00,1.15,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Boa_Esperanca,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Boa_Esperanca,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Boa_Esperanca,2017-12-31T00:00:00,2.53,3.0,18.58,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Boa_Esperanca,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Boa_Esperanca,2019-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Boa_Esperanca,2020-12-31T00:00:00,2.06,2.0,2.91,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Boa_Esperanca,2021-12-31T00:00:00,1.57,2.0,27.39,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bom_Sucesso,2012-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bom_Sucesso,2013-12-31T00:00:00,1.26,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bom_Sucesso,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bom_Sucesso,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bom_Sucesso,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bom_Sucesso,2017-12-31T00:00:00,1.48,1.0,32.43,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bom_Sucesso,2018-12-31T00:00:00,1.7,2.0,17.65,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bom_Sucesso,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bom_Sucesso,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bom_Sucesso,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bueno_Brandao,2012-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bueno_Brandao,2013-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bueno_Brandao,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bueno_Brandao,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bueno_Brandao,2016-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bueno_Brandao,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bueno_Brandao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bueno_Brandao,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bueno_Brandao,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bueno_Brandao,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cambuquira,2012-12-31T00:00:00,1.15,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cambuquira,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cambuquira,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cambuquira,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cambuquira,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cambuquira,2017-12-31T00:00:00,1.82,2.0,9.89,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cambuquira,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cambuquira,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cambuquira,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cambuquira,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campanha,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campanha,2013-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campanha,2014-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campanha,2015-12-31T00:00:00,1.02,1.0,1.96,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campanha,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campanha,2017-12-31T00:00:00,1.88,2.0,6.38,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campanha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campanha,2019-12-31T00:00:00,1.51,2.0,32.45,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campanha,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campanha,2021-12-31T00:00:00,1.33,1.0,24.81,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_Belo,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_Belo,2013-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_Belo,2014-12-31T00:00:00,0.63,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_Belo,2015-12-31T00:00:00,0.84,1.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_Belo,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_Belo,2017-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_Belo,2018-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_Belo,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_Belo,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_Belo,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_do_Meio,2012-12-31T00:00:00,1.17,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_do_Meio,2013-12-31T00:00:00,1.17,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_do_Meio,2014-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_do_Meio,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_do_Meio,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_do_Meio,2017-12-31T00:00:00,2.08,2.0,3.85,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_do_Meio,2018-12-31T00:00:00,1.82,2.0,9.89,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_do_Meio,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_do_Meio,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_do_Meio,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campos_Gerais,2012-12-31T00:00:00,1.65,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campos_Gerais,2013-12-31T00:00:00,1.22,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campos_Gerais,2014-12-31T00:00:00,1.15,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campos_Gerais,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campos_Gerais,2016-12-31T00:00:00,2.5,3.0,20.0,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campos_Gerais,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campos_Gerais,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campos_Gerais,2019-12-31T00:00:00,1.59,2.0,25.79,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campos_Gerais,2020-12-31T00:00:00,2.69,3.0,11.52,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campos_Gerais,2021-12-31T00:00:00,1.59,2.0,25.79,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Candeias,2012-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Candeias,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Candeias,2014-12-31T00:00:00,0.99,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Candeias,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Candeias,2016-12-31T00:00:00,2.28,2.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Candeias,2017-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Candeias,2018-12-31T00:00:00,2.04,2.0,1.96,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Candeias,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Candeias,2020-12-31T00:00:00,1.86,2.0,7.53,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Candeias,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capelinha,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capelinha,2013-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capelinha,2014-12-31T00:00:00,1.26,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capelinha,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capelinha,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capelinha,2017-12-31T00:00:00,1.16,1.0,13.79,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capelinha,2018-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capelinha,2019-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capelinha,2020-12-31T00:00:00,1.43,1.0,30.07,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capelinha,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capetinga,2012-12-31T00:00:00,1.81,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capetinga,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capetinga,2014-12-31T00:00:00,1.59,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capetinga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capetinga,2016-12-31T00:00:00,1.93,2.0,3.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capetinga,2017-12-31T00:00:00,2.09,2.0,4.31,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capetinga,2018-12-31T00:00:00,2.7,3.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capetinga,2019-12-31T00:00:00,2.04,2.0,1.96,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capetinga,2020-12-31T00:00:00,2.21,2.0,9.5,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capetinga,2021-12-31T00:00:00,1.64,2.0,21.95,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capitolio,2012-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capitolio,2013-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capitolio,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capitolio,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capitolio,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capitolio,2017-12-31T00:00:00,1.47,1.0,31.97,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capitolio,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capitolio,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capitolio,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capitolio,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Caratinga,2012-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Caratinga,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Caratinga,2014-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Caratinga,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Caratinga,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Caratinga,2017-12-31T00:00:00,1.46,2.0,36.99,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Caratinga,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Caratinga,2019-12-31T00:00:00,1.0,1.0,0.0,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Caratinga,2020-12-31T00:00:00,2.04,2.0,1.96,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Caratinga,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_da_Cachoeira,2012-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_da_Cachoeira,2013-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_da_Cachoeira,2014-12-31T00:00:00,1.15,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_da_Cachoeira,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_da_Cachoeira,2016-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_da_Cachoeira,2017-12-31T00:00:00,1.73,2.0,15.61,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_da_Cachoeira,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_da_Cachoeira,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_da_Cachoeira,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_da_Cachoeira,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_de_Minas,2012-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_de_Minas,2013-12-31T00:00:00,1.74,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_de_Minas,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_de_Minas,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_de_Minas,2017-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_de_Minas,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_de_Minas,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_de_Minas,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_de_Minas,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_do_Rio_Claro,2012-12-31T00:00:00,1.66,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_do_Rio_Claro,2013-12-31T00:00:00,1.94,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_do_Rio_Claro,2014-12-31T00:00:00,1.65,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_do_Rio_Claro,2015-12-31T00:00:00,1.53,2.0,30.72,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_do_Rio_Claro,2016-12-31T00:00:00,2.37,2.0,15.61,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_do_Rio_Claro,2017-12-31T00:00:00,1.82,2.0,9.89,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_do_Rio_Claro,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_do_Rio_Claro,2019-12-31T00:00:00,1.94,2.0,3.09,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_do_Rio_Claro,2020-12-31T00:00:00,2.78,3.0,7.91,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_do_Rio_Claro,2021-12-31T00:00:00,1.64,2.0,21.95,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cassia,2012-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cassia,2013-12-31T00:00:00,1.56,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cassia,2014-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cassia,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cassia,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cassia,2017-12-31T00:00:00,1.6,2.0,25.0,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cassia,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cassia,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cassia,2020-12-31T00:00:00,2.17,2.0,7.83,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cassia,2021-12-31T00:00:00,1.31,1.0,23.66,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2014-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2015-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2017-12-31T00:00:00,2.04,2.0,1.96,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conselheiro_Pena,2012-12-31T00:00:00,1.48,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conselheiro_Pena,2013-12-31T00:00:00,1.3,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conselheiro_Pena,2014-12-31T00:00:00,1.3,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conselheiro_Pena,2015-12-31T00:00:00,1.06,1.0,5.66,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conselheiro_Pena,2016-12-31T00:00:00,1.18,1.0,15.25,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conselheiro_Pena,2017-12-31T00:00:00,1.29,1.0,22.48,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conselheiro_Pena,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conselheiro_Pena,2019-12-31T00:00:00,1.3,1.0,23.08,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conselheiro_Pena,2020-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conselheiro_Pena,2021-12-31T00:00:00,1.29,1.0,22.48,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Coqueiral,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Coqueiral,2013-12-31T00:00:00,1.14,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Coqueiral,2014-12-31T00:00:00,1.14,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Coqueiral,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Coqueiral,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Coqueiral,2017-12-31T00:00:00,1.75,2.0,14.29,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Coqueiral,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Coqueiral,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Coqueiral,2020-12-31T00:00:00,2.16,2.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Coqueiral,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cristais,2012-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cristais,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cristais,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cristais,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cristais,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cristais,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cristais,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cristais,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cristais,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cristais,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Divisa_Nova,2012-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Divisa_Nova,2013-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Divisa_Nova,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Divisa_Nova,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Divisa_Nova,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Divisa_Nova,2017-12-31T00:00:00,2.28,2.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Divisa_Nova,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Divisa_Nova,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Divisa_Nova,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Divisa_Nova,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Eloi_Mendes,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Eloi_Mendes,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Eloi_Mendes,2014-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Eloi_Mendes,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Eloi_Mendes,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Eloi_Mendes,2017-12-31T00:00:00,1.83,2.0,9.29,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Eloi_Mendes,2018-12-31T00:00:00,1.72,2.0,16.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Eloi_Mendes,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Eloi_Mendes,2020-12-31T00:00:00,2.11,2.0,5.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Eloi_Mendes,2021-12-31T00:00:00,1.04,1.0,3.85,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Fervedouro,2012-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Fervedouro,2013-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Fervedouro,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Fervedouro,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Fervedouro,2016-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Fervedouro,2017-12-31T00:00:00,1.06,1.0,5.66,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Fervedouro,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Fervedouro,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Fervedouro,2020-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Fervedouro,2021-12-31T00:00:00,1.15,1.0,13.04,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Heliodora,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Heliodora,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Heliodora,2014-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Heliodora,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Heliodora,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Heliodora,2017-12-31T00:00:00,1.57,2.0,27.39,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Heliodora,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Heliodora,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Heliodora,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Heliodora,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ilicinea,2012-12-31T00:00:00,1.69,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ilicinea,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ilicinea,2014-12-31T00:00:00,1.35,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ilicinea,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ilicinea,2016-12-31T00:00:00,2.46,2.0,18.7,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ilicinea,2017-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ilicinea,2018-12-31T00:00:00,2.46,2.0,18.7,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ilicinea,2019-12-31T00:00:00,1.86,2.0,7.53,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ilicinea,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ilicinea,2021-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Imbe_de_Minas,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Imbe_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Imbe_de_Minas,2014-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Imbe_de_Minas,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Imbe_de_Minas,2016-12-31T00:00:00,0.9,1.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Imbe_de_Minas,2017-12-31T00:00:00,1.29,1.0,22.48,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Imbe_de_Minas,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Imbe_de_Minas,2019-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Imbe_de_Minas,2020-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Imbe_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inconfidentes,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inconfidentes,2013-12-31T00:00:00,1.4,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inconfidentes,2014-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inconfidentes,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inconfidentes,2016-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inconfidentes,2017-12-31T00:00:00,1.3,1.0,23.08,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inconfidentes,2018-12-31T00:00:00,2.88,3.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inconfidentes,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inconfidentes,2020-12-31T00:00:00,2.46,2.0,18.7,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inconfidentes,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inhapim,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inhapim,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inhapim,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inhapim,2015-12-31T00:00:00,1.21,1.0,17.36,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inhapim,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inhapim,2017-12-31T00:00:00,1.12,1.0,10.71,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inhapim,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inhapim,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inhapim,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inhapim,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Itamogi,2012-12-31T00:00:00,1.95,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Itamogi,2013-12-31T00:00:00,1.17,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Itamogi,2014-12-31T00:00:00,1.6,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Itamogi,2015-12-31T00:00:00,1.58,2.0,26.58,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Itamogi,2016-12-31T00:00:00,1.95,2.0,2.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Itamogi,2017-12-31T00:00:00,1.42,1.0,29.58,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Itamogi,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Itamogi,2019-12-31T00:00:00,1.85,2.0,8.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Itamogi,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Itamogi,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jacutinga,2012-12-31T00:00:00,1.68,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jacutinga,2013-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jacutinga,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jacutinga,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jacutinga,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jacutinga,2017-12-31T00:00:00,1.37,1.0,27.01,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jacutinga,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jacutinga,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jacutinga,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jacutinga,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jequeri,2012-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jequeri,2013-12-31T00:00:00,1.86,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jequeri,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jequeri,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jequeri,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jequeri,2017-12-31T00:00:00,1.28,1.0,21.88,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jequeri,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jequeri,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jequeri,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jequeri,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Juruaia,2012-12-31T00:00:00,1.92,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Juruaia,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Juruaia,2014-12-31T00:00:00,1.72,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Juruaia,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Juruaia,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Juruaia,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Juruaia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Juruaia,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Juruaia,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Juruaia,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lambari,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lambari,2013-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lambari,2014-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lambari,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lambari,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lambari,2017-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lambari,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lambari,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lambari,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lambari,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lavras,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lavras,2013-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lavras,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lavras,2015-12-31T00:00:00,0.84,1.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lavras,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lavras,2017-12-31T00:00:00,2.25,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lavras,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lavras,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lavras,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lavras,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Luisburgo,2012-12-31T00:00:00,1.26,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Luisburgo,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Luisburgo,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Luisburgo,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Luisburgo,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Luisburgo,2017-12-31T00:00:00,1.53,2.0,30.72,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Luisburgo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Luisburgo,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Luisburgo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Luisburgo,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Machado,2012-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Machado,2013-12-31T00:00:00,1.62,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Machado,2014-12-31T00:00:00,1.15,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Machado,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Machado,2016-12-31T00:00:00,1.86,2.0,7.53,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Machado,2017-12-31T00:00:00,1.51,2.0,32.45,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Machado,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Machado,2019-12-31T00:00:00,1.53,2.0,30.72,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Machado,2020-12-31T00:00:00,2.21,2.0,9.5,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Machado,2021-12-31T00:00:00,1.44,0.0,100.0,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monsenhor_Paulo,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monsenhor_Paulo,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monsenhor_Paulo,2014-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monsenhor_Paulo,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monsenhor_Paulo,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monsenhor_Paulo,2017-12-31T00:00:00,1.72,2.0,16.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monsenhor_Paulo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monsenhor_Paulo,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monsenhor_Paulo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monsenhor_Paulo,2021-12-31T00:00:00,1.41,1.0,29.08,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Belo,2012-12-31T00:00:00,1.63,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Belo,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Belo,2014-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Belo,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Belo,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Belo,2017-12-31T00:00:00,2.43,2.0,17.7,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Belo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Belo,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Belo,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Belo,2021-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Santo_de_Minas,2012-12-31T00:00:00,1.89,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Santo_de_Minas,2013-12-31T00:00:00,1.62,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Santo_de_Minas,2014-12-31T00:00:00,1.6,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Santo_de_Minas,2015-12-31T00:00:00,1.35,1.0,25.93,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Santo_de_Minas,2016-12-31T00:00:00,2.04,2.0,1.96,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Santo_de_Minas,2017-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Santo_de_Minas,2018-12-31T00:00:00,1.88,2.0,6.38,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Santo_de_Minas,2019-12-31T00:00:00,1.59,2.0,25.79,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Santo_de_Minas,2020-12-31T00:00:00,2.12,2.0,5.66,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Santo_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Siao,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Siao,2013-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Siao,2014-12-31T00:00:00,0.66,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Siao,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Siao,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Siao,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Siao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Siao,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Siao,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Siao,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Muzambinho,2012-12-31T00:00:00,1.74,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Muzambinho,2013-12-31T00:00:00,1.35,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Muzambinho,2014-12-31T00:00:00,1.35,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Muzambinho,2015-12-31T00:00:00,1.48,1.0,32.43,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Muzambinho,2016-12-31T00:00:00,1.98,2.0,1.01,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Muzambinho,2017-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Muzambinho,2018-12-31T00:00:00,1.3,1.0,23.08,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Muzambinho,2019-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Muzambinho,2020-12-31T00:00:00,1.76,2.0,13.64,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Muzambinho,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Natercia,2012-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Natercia,2013-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Natercia,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Natercia,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Natercia,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Natercia,2017-12-31T00:00:00,1.6,2.0,25.0,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Natercia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Natercia,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Natercia,2020-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Natercia,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nazareno,2012-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nazareno,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nazareno,2014-12-31T00:00:00,1.3,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nazareno,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nazareno,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nazareno,2017-12-31T00:00:00,1.59,2.0,25.79,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nazareno,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nazareno,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nazareno,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nazareno,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nepomuceno,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nepomuceno,2013-12-31T00:00:00,1.68,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nepomuceno,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nepomuceno,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nepomuceno,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nepomuceno,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nepomuceno,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nepomuceno,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nepomuceno,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nepomuceno,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Oliveira,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Oliveira,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Oliveira,2014-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Oliveira,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Oliveira,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Oliveira,2017-12-31T00:00:00,1.4,1.0,28.57,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Oliveira,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Oliveira,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Oliveira,2020-12-31T00:00:00,1.95,2.0,2.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Oliveira,2021-12-31T00:00:00,1.54,2.0,29.87,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ouro_Fino,2012-12-31T00:00:00,1.26,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ouro_Fino,2013-12-31T00:00:00,1.56,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ouro_Fino,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ouro_Fino,2015-12-31T00:00:00,1.29,1.0,22.48,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ouro_Fino,2016-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ouro_Fino,2017-12-31T00:00:00,1.42,1.0,29.58,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ouro_Fino,2018-12-31T00:00:00,1.69,2.0,18.34,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ouro_Fino,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ouro_Fino,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ouro_Fino,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Paraguacu,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Paraguacu,2013-12-31T00:00:00,1.14,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Paraguacu,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Paraguacu,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Paraguacu,2016-12-31T00:00:00,1.98,2.0,1.01,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Paraguacu,2017-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Paraguacu,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Paraguacu,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Paraguacu,2020-12-31T00:00:00,2.34,2.0,14.53,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Paraguacu,2021-12-31T00:00:00,0.97,1.0,3.09,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedra_Bonita,2012-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedra_Bonita,2013-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedra_Bonita,2014-12-31T00:00:00,0.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedra_Bonita,2015-12-31T00:00:00,0.6,1.0,66.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedra_Bonita,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedra_Bonita,2017-12-31T00:00:00,0.86,1.0,16.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedra_Bonita,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedra_Bonita,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedra_Bonita,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedra_Bonita,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedralva,2012-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedralva,2013-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedralva,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedralva,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedralva,2016-12-31T00:00:00,2.28,2.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedralva,2017-12-31T00:00:00,1.43,2.0,39.86,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedralva,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedralva,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedralva,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedralva,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Perdoes,2012-12-31T00:00:00,1.68,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Perdoes,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Perdoes,2014-12-31T00:00:00,1.26,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Perdoes,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Perdoes,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Perdoes,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Perdoes,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Perdoes,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Perdoes,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Perdoes,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Piedade_de_Caratinga,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Piedade_de_Caratinga,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Piedade_de_Caratinga,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Piedade_de_Caratinga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Piedade_de_Caratinga,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Piedade_de_Caratinga,2017-12-31T00:00:00,1.59,2.0,25.79,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Piedade_de_Caratinga,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Piedade_de_Caratinga,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Piedade_de_Caratinga,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Piedade_de_Caratinga,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Poco_Fundo,2012-12-31T00:00:00,1.14,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Poco_Fundo,2013-12-31T00:00:00,1.14,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Poco_Fundo,2014-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Poco_Fundo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Poco_Fundo,2016-12-31T00:00:00,1.41,1.0,29.08,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Poco_Fundo,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Poco_Fundo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Poco_Fundo,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Poco_Fundo,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Poco_Fundo,2021-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pocos_de_Caldas,2012-12-31T00:00:00,1.56,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pocos_de_Caldas,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pocos_de_Caldas,2014-12-31T00:00:00,1.14,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pocos_de_Caldas,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pocos_de_Caldas,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pocos_de_Caldas,2017-12-31T00:00:00,1.77,2.0,12.99,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pocos_de_Caldas,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pocos_de_Caldas,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pocos_de_Caldas,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pocos_de_Caldas,2021-12-31T00:00:00,1.42,1.0,29.58,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pratinha,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pratinha,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pratinha,2014-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pratinha,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pratinha,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pratinha,2017-12-31T00:00:00,1.72,2.0,16.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pratinha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pratinha,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pratinha,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pratinha,2021-12-31T00:00:00,1.35,1.0,25.93,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Raul_Soares,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Raul_Soares,2013-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Raul_Soares,2014-12-31T00:00:00,0.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Raul_Soares,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Raul_Soares,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Raul_Soares,2017-12-31T00:00:00,0.86,1.0,16.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Raul_Soares,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Raul_Soares,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Raul_Soares,2020-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Raul_Soares,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Itueto,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Itueto,2013-12-31T00:00:00,1.17,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Itueto,2014-12-31T00:00:00,1.11,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Itueto,2015-12-31T00:00:00,0.92,1.0,8.7,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Itueto,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Itueto,2017-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Itueto,2018-12-31T00:00:00,1.35,1.0,25.93,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Itueto,2019-12-31T00:00:00,1.65,2.0,21.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Itueto,2020-12-31T00:00:00,1.77,2.0,12.99,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Itueto,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2017-12-31T00:00:00,1.79,2.0,11.73,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santana_da_Vargem,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santana_da_Vargem,2013-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santana_da_Vargem,2014-12-31T00:00:00,1.1,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santana_da_Vargem,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santana_da_Vargem,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santana_da_Vargem,2017-12-31T00:00:00,2.09,2.0,4.31,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santana_da_Vargem,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santana_da_Vargem,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santana_da_Vargem,2020-12-31T00:00:00,2.22,2.0,9.91,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santana_da_Vargem,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2012-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2013-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2014-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2017-12-31T00:00:00,1.34,1.0,25.37,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2018-12-31T00:00:00,2.16,2.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Francisco_de_Paula,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Francisco_de_Paula,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Francisco_de_Paula,2014-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Francisco_de_Paula,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Francisco_de_Paula,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Francisco_de_Paula,2017-12-31T00:00:00,1.56,1.0,35.9,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Francisco_de_Paula,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Francisco_de_Paula,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Francisco_de_Paula,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Francisco_de_Paula,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2012-12-31T00:00:00,1.26,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2013-12-31T00:00:00,1.26,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2014-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2017-12-31T00:00:00,2.04,2.0,1.96,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2021-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Jose_da_Barra,2012-12-31T00:00:00,1.56,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Jose_da_Barra,2013-12-31T00:00:00,1.56,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Jose_da_Barra,2014-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Jose_da_Barra,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Jose_da_Barra,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Jose_da_Barra,2017-12-31T00:00:00,1.91,2.0,4.71,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Jose_da_Barra,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Jose_da_Barra,2019-12-31T00:00:00,1.7,2.0,17.65,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Jose_da_Barra,2020-12-31T00:00:00,2.54,3.0,18.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Jose_da_Barra,2021-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2012-12-31T00:00:00,2.28,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2014-12-31T00:00:00,1.62,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2021-12-31T00:00:00,1.48,1.0,32.43,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Roque_de_Minas,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Roque_de_Minas,2013-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Roque_de_Minas,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Roque_de_Minas,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Roque_de_Minas,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Roque_de_Minas,2017-12-31T00:00:00,1.46,1.0,31.51,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Roque_de_Minas,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Roque_de_Minas,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Roque_de_Minas,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Roque_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2012-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2013-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2017-12-31T00:00:00,1.78,2.0,12.36,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2018-12-31T00:00:00,1.64,2.0,21.95,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2012-12-31T00:00:00,1.62,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2013-12-31T00:00:00,1.71,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2014-12-31T00:00:00,1.42,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2016-12-31T00:00:00,1.7,2.0,17.65,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2017-12-31T00:00:00,1.54,2.0,29.87,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2018-12-31T00:00:00,1.89,2.0,5.82,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2019-12-31T00:00:00,1.47,1.0,31.97,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2020-12-31T00:00:00,1.93,2.0,3.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2021-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2012-12-31T00:00:00,1.75,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2013-12-31T00:00:00,1.05,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2014-12-31T00:00:00,1.35,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2017-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Serrania,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Serrania,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Serrania,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Serrania,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Serrania,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Serrania,2017-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Serrania,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Serrania,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Serrania,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Serrania,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Simonesia,2012-12-31T00:00:00,1.56,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Simonesia,2013-12-31T00:00:00,1.62,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Simonesia,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Simonesia,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Simonesia,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Simonesia,2017-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Simonesia,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Simonesia,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Simonesia,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Simonesia,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Coracoes,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Coracoes,2013-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Coracoes,2014-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Coracoes,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Coracoes,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Coracoes,2017-12-31T00:00:00,2.67,2.0,25.09,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Coracoes,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Coracoes,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Coracoes,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Coracoes,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Pontas,2012-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Pontas,2013-12-31T00:00:00,1.53,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Pontas,2014-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Pontas,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Pontas,2016-12-31T00:00:00,1.86,2.0,7.53,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Pontas,2017-12-31T00:00:00,1.88,2.0,6.38,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Pontas,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Pontas,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Pontas,2020-12-31T00:00:00,2.26,2.0,11.5,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Pontas,2021-12-31T00:00:00,1.35,1.0,25.93,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Turvolandia,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Turvolandia,2013-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Turvolandia,2014-12-31T00:00:00,0.84,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Turvolandia,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Turvolandia,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Turvolandia,2017-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Turvolandia,2018-12-31T00:00:00,2.29,2.0,12.66,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Turvolandia,2019-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Turvolandia,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Turvolandia,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ubaporanga,2012-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ubaporanga,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ubaporanga,2014-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ubaporanga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ubaporanga,2016-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ubaporanga,2017-12-31T00:00:00,1.03,1.0,2.91,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ubaporanga,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ubaporanga,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ubaporanga,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ubaporanga,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Varginha,2012-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Varginha,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Varginha,2014-12-31T00:00:00,1.03,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Varginha,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Varginha,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Varginha,2017-12-31T00:00:00,2.12,2.0,5.66,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Varginha,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Varginha,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Varginha,2020-12-31T00:00:00,1.9,2.0,5.26,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Varginha,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Vermelho_Novo,2012-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Vermelho_Novo,2013-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Vermelho_Novo,2014-12-31T00:00:00,0.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Vermelho_Novo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Vermelho_Novo,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Vermelho_Novo,2017-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Vermelho_Novo,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Vermelho_Novo,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Vermelho_Novo,2020-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Vermelho_Novo,2021-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Abre_Campo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alfenas,2022-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alterosa,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Andradas,2022-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Angelandia,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Araponga,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Boa_Esperanca,2022-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bom_Sucesso,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bueno_Brandao,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cambuquira,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campanha,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_Belo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_do_Meio,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campos_Gerais,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Candeias,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capelinha,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capetinga,2022-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capitolio,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Caratinga,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_da_Cachoeira,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_do_Rio_Claro,2022-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cassia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conselheiro_Pena,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Coqueiral,2022-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cristais,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Divisa_Nova,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Eloi_Mendes,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Fervedouro,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Heliodora,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ilicinea,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Imbe_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inconfidentes,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inhapim,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Itamogi,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jacutinga,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jequeri,2022-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Juruaia,2022-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lambari,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lavras,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Luisburgo,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Machado,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monsenhor_Paulo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Belo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Santo_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Siao,2022-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Muzambinho,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Natercia,2022-12-31T00:00:00,1.0,0,-100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nazareno,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nepomuceno,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Oliveira,2022-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ouro_Fino,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Paraguacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedra_Bonita,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedralva,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Perdoes,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Piedade_de_Caratinga,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Poco_Fundo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pocos_de_Caldas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pratinha,2022-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Raul_Soares,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Itueto,2022-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santana_da_Vargem,2022-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2022-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Francisco_de_Paula,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2022-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Jose_da_Barra,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Roque_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2022-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Serrania,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Simonesia,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Coracoes,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Pontas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Turvolandia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ubaporanga,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Varginha,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Vermelho_Novo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Abre_Campo,2023-12-31T00:00:00,1.8000000000000005,1.4451031684875488,-19.72,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alfenas,2023-12-31T00:00:00,1.698430922311519,2.0376715660095215,19.97,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Alterosa,2023-12-31T00:00:00,1.2603960396039595,1.855768084526062,47.24,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Andradas,2023-12-31T00:00:00,1.22,1.9482613801956177,59.69,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Angelandia,2023-12-31T00:00:00,2.095663265306122,1.5746939182281494,-24.86,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Araponga,2023-12-31T00:00:00,1.5,1.347177505493164,-10.19,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Boa_Esperanca,2023-12-31T00:00:00,1.08,1.1207181215286255,3.77,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bom_Sucesso,2023-12-31T00:00:00,1.5,1.3356778621673584,-10.95,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Bueno_Brandao,2023-12-31T00:00:00,1.379901960784314,1.3610565662384033,-1.37,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cambuquira,2023-12-31T00:00:00,1.56,1.82257878780365,16.83,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campanha,2023-12-31T00:00:00,1.327075098814229,1.528287410736084,15.16,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_Belo,2023-12-31T00:00:00,1.5,1.3769938945770264,-8.2,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campo_do_Meio,2023-12-31T00:00:00,0.8771626297577856,2.0200164318084717,130.29,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Campos_Gerais,2023-12-31T00:00:00,1.560242401107029,2.4509403705596924,57.09,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Candeias,2023-12-31T00:00:00,1.080032206119163,1.3834840059280396,28.1,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capelinha,2023-12-31T00:00:00,1.541750580945004,1.3928626775741577,-9.66,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capetinga,2023-12-31T00:00:00,1.4484046164290565,1.7947466373443604,23.91,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Capitolio,2023-12-31T00:00:00,1.32,1.3058570623397827,-1.07,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Caratinga,2023-12-31T00:00:00,1.3799999999999997,1.4186509847640991,2.8,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_da_Cachoeira,2023-12-31T00:00:00,1.32,1.6608119010925293,25.82,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_de_Minas,2023-12-31T00:00:00,1.5,1.6077812910079956,7.19,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Carmo_do_Rio_Claro,2023-12-31T00:00:00,1.816856256463288,3.622413158416748,99.38,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cassia,2023-12-31T00:00:00,1.6377649325626198,1.817854642868042,11.0,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2023-12-31T00:00:00,1.44,1.8724608421325684,30.03,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Conselheiro_Pena,2023-12-31T00:00:00,1.320048602673147,1.3056033849716187,-1.09,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Coqueiral,2023-12-31T00:00:00,1.32,1.2839045524597168,-2.73,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Cristais,2023-12-31T00:00:00,1.5631111111111111,1.2072172164916992,-22.77,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Divisa_Nova,2023-12-31T00:00:00,1.680092592592593,1.6803138256072998,0.01,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Eloi_Mendes,2023-12-31T00:00:00,1.577412806790921,1.6557929515838623,4.97,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Fervedouro,2023-12-31T00:00:00,1.2,1.2313711643218994,2.61,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Heliodora,2023-12-31T00:00:00,1.439872408293461,1.3778438568115234,-4.31,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ilicinea,2023-12-31T00:00:00,1.560032362459547,1.3176288604736328,-15.54,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Imbe_de_Minas,2023-12-31T00:00:00,1.3799999999999997,1.3006689548492432,-5.75,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inconfidentes,2023-12-31T00:00:00,1.7401360544217692,1.9367440938949585,11.3,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Inhapim,2023-12-31T00:00:00,2.7000000000000006,1.2829619646072388,-52.48,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Itamogi,2023-12-31T00:00:00,1.740011254924029,3.0203750133514404,73.58,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jacutinga,2023-12-31T00:00:00,1.160053262316911,1.339275598526001,15.45,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Jequeri,2023-12-31T00:00:00,1.5,1.5462312698364258,3.08,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Juruaia,2023-12-31T00:00:00,1.5,2.168288230895996,44.55,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lambari,2023-12-31T00:00:00,1.56,1.4674493074417114,-5.93,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Lavras,2023-12-31T00:00:00,1.5,1.3373701572418213,-10.84,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Luisburgo,2023-12-31T00:00:00,1.44,1.7527389526367188,21.72,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Machado,2023-12-31T00:00:00,1.500560931145702,1.9270915985107422,28.42,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monsenhor_Paulo,2023-12-31T00:00:00,1.389931972789116,2.048800468444824,47.4,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Belo,2023-12-31T00:00:00,1.3799999999999997,1.3847476243972778,0.34,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Santo_de_Minas,2023-12-31T00:00:00,1.620050377833753,1.9722917079925537,21.74,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Monte_Siao,2023-12-31T00:00:00,1.5,1.276307463645935,-14.91,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Muzambinho,2023-12-31T00:00:00,1.68,1.8843039274215698,12.16,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Natercia,2023-12-31T00:00:00,1.5,5.94558572769165,296.37,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nazareno,2023-12-31T00:00:00,1.5599056603773582,1.9285106658935547,23.63,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Nepomuceno,2023-12-31T00:00:00,1.650045578851413,1.071772813796997,-35.05,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Oliveira,2023-12-31T00:00:00,1.8240227434257288,1.6416096687316895,-10.0,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ouro_Fino,2023-12-31T00:00:00,1.68,1.3869787454605103,-17.44,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Paraguacu,2023-12-31T00:00:00,1.298291457286432,1.9400897026062012,49.43,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedra_Bonita,2023-12-31T00:00:00,1.08,1.415815830230713,31.09,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pedralva,2023-12-31T00:00:00,1.67998417721519,1.549340009689331,-7.78,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Perdoes,2023-12-31T00:00:00,1.6799401197604789,1.2972830533981323,-22.78,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Piedade_de_Caratinga,2023-12-31T00:00:00,2.4,1.408278226852417,-41.32,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Poco_Fundo,2023-12-31T00:00:00,1.380160799652325,1.5070141553878784,9.19,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pocos_de_Caldas,2023-12-31T00:00:00,1.5,1.309265375137329,-12.72,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Pratinha,2023-12-31T00:00:00,2.13,1.7663919925689697,-17.07,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Raul_Soares,2023-12-31T00:00:00,1.5,1.0340700149536133,-31.06,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Itueto,2023-12-31T00:00:00,1.2,1.5893638134002686,32.45,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2023-12-31T00:00:00,1.3799999999999997,1.435309648513794,4.01,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santana_da_Vargem,2023-12-31T00:00:00,1.5593593593593589,1.871865153312683,20.04,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2023-12-31T00:00:00,1.8000000000000005,1.2614825963974,-29.92,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Francisco_de_Paula,2023-12-31T00:00:00,1.56,1.4771941900253296,-5.31,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2023-12-31T00:00:00,1.2,2.5109262466430664,109.24,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Jose_da_Barra,2023-12-31T00:00:00,2.2413698630136984,3.2083077430725098,43.14,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2023-12-31T00:00:00,1.5,3.0833606719970703,105.56,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Roque_de_Minas,2023-12-31T00:00:00,1.8000000000000005,1.2268154621124268,-31.84,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2023-12-31T00:00:00,1.7998212689901698,1.663820743560791,-7.56,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2023-12-31T00:00:00,1.440017746228926,1.6127806901931763,12.0,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2023-12-31T00:00:00,1.319968346082828,3.966120481491089,200.47,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Serrania,2023-12-31T00:00:00,0.9,1.5491840839385986,72.13,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Simonesia,2023-12-31T00:00:00,1.5,1.2454653978347778,-16.97,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Coracoes,2023-12-31T00:00:00,1.44,1.9256885051727295,33.73,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Tres_Pontas,2023-12-31T00:00:00,1.5900000000000003,2.096806526184082,31.87,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Turvolandia,2023-12-31T00:00:00,1.259748427672956,1.4719510078430176,16.84,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Ubaporanga,2023-12-31T00:00:00,1.5,1.0741260051727295,-28.39,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Varginha,2023-12-31T00:00:00,1.679990280646337,1.584655523300171,-5.67,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 1 - Metodo 1 (2023),Vermelho_Novo,2023-12-31T00:00:00,1.5,0.990482747554779,-33.97,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.002792044602691115
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 900
",2025-08-25T15:27:09
IBGE Cluster 2 - Metodo 1 (2023),Araguari,2012-12-31T00:00:00,2.4,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araguari,2013-12-31T00:00:00,1.92,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araguari,2014-12-31T00:00:00,2.4,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araguari,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araguari,2016-12-31T00:00:00,2.58,2.0,22.48,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araguari,2017-12-31T00:00:00,2.76,2.0,27.54,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araguari,2018-12-31T00:00:00,2.64,2.0,24.24,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araguari,2019-12-31T00:00:00,1.68,3.0,78.57,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araguari,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araguari,2021-12-31T00:00:00,1.86,2.0,7.53,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araxa,2012-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araxa,2013-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araxa,2014-12-31T00:00:00,1.8,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araxa,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araxa,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araxa,2017-12-31T00:00:00,1.76,2.0,13.64,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araxa,2018-12-31T00:00:00,1.94,2.0,3.09,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araxa,2019-12-31T00:00:00,1.39,2.0,43.88,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araxa,2020-12-31T00:00:00,1.79,2.0,11.73,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araxa,2021-12-31T00:00:00,1.25,1.0,20.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Bambui,2012-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Bambui,2013-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Bambui,2014-12-31T00:00:00,1.38,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Bambui,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Bambui,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Bambui,2017-12-31T00:00:00,1.5,1.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Bambui,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Bambui,2019-12-31T00:00:00,1.26,2.0,58.73,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Bambui,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Bambui,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Buritizeiro,2012-12-31T00:00:00,3.6,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Buritizeiro,2013-12-31T00:00:00,3.6,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Buritizeiro,2014-12-31T00:00:00,3.6,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Buritizeiro,2015-12-31T00:00:00,1.92,-0.0,100.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Buritizeiro,2016-12-31T00:00:00,2.1,3.0,42.86,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Buritizeiro,2017-12-31T00:00:00,1.97,2.0,1.52,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Buritizeiro,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Buritizeiro,2019-12-31T00:00:00,2.88,2.0,30.56,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Buritizeiro,2020-12-31T00:00:00,3.0,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Buritizeiro,2021-12-31T00:00:00,3.0,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Campos_Altos,2012-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Campos_Altos,2013-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Campos_Altos,2014-12-31T00:00:00,1.62,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Campos_Altos,2015-12-31T00:00:00,1.71,2.0,16.96,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Campos_Altos,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Campos_Altos,2017-12-31T00:00:00,1.99,2.0,0.5,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Campos_Altos,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Campos_Altos,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Campos_Altos,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Campos_Altos,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Carmo_do_Paranaiba,2012-12-31T00:00:00,2.11,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Carmo_do_Paranaiba,2013-12-31T00:00:00,1.92,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Carmo_do_Paranaiba,2014-12-31T00:00:00,1.93,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Carmo_do_Paranaiba,2015-12-31T00:00:00,1.89,2.0,5.82,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Carmo_do_Paranaiba,2016-12-31T00:00:00,2.37,2.0,15.61,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Carmo_do_Paranaiba,2017-12-31T00:00:00,1.75,2.0,14.29,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Carmo_do_Paranaiba,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Carmo_do_Paranaiba,2019-12-31T00:00:00,1.79,2.0,11.73,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Carmo_do_Paranaiba,2020-12-31T00:00:00,1.95,2.0,2.56,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Carmo_do_Paranaiba,2021-12-31T00:00:00,1.54,2.0,29.87,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Claraval,2012-12-31T00:00:00,2.55,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Claraval,2013-12-31T00:00:00,1.42,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Claraval,2014-12-31T00:00:00,2.5,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Claraval,2015-12-31T00:00:00,1.02,2.0,96.08,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Claraval,2016-12-31T00:00:00,2.34,2.0,14.53,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Claraval,2017-12-31T00:00:00,1.22,2.0,63.93,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Claraval,2018-12-31T00:00:00,2.55,1.0,60.78,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Claraval,2019-12-31T00:00:00,1.68,1.0,40.48,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Claraval,2020-12-31T00:00:00,2.8,2.0,28.57,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Claraval,2021-12-31T00:00:00,1.05,2.0,90.48,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Coromandel,2012-12-31T00:00:00,1.96,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Coromandel,2013-12-31T00:00:00,2.02,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Coromandel,2014-12-31T00:00:00,1.91,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Coromandel,2015-12-31T00:00:00,1.91,2.0,4.71,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Coromandel,2016-12-31T00:00:00,2.09,2.0,4.31,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Coromandel,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Coromandel,2018-12-31T00:00:00,2.65,2.0,24.53,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Coromandel,2019-12-31T00:00:00,1.65,2.0,21.21,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Coromandel,2020-12-31T00:00:00,1.82,2.0,9.89,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Coromandel,2021-12-31T00:00:00,1.25,2.0,60.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Estrela_do_Sul,2012-12-31T00:00:00,2.22,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Estrela_do_Sul,2013-12-31T00:00:00,1.92,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Estrela_do_Sul,2014-12-31T00:00:00,1.98,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Estrela_do_Sul,2015-12-31T00:00:00,1.26,2.0,58.73,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Estrela_do_Sul,2016-12-31T00:00:00,2.52,2.0,20.63,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Estrela_do_Sul,2017-12-31T00:00:00,2.38,2.0,15.97,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Estrela_do_Sul,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Estrela_do_Sul,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Estrela_do_Sul,2020-12-31T00:00:00,2.06,2.0,2.91,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Estrela_do_Sul,2021-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Formiga,2012-12-31T00:00:00,1.2,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Formiga,2013-12-31T00:00:00,1.2,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Formiga,2014-12-31T00:00:00,1.2,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Formiga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Formiga,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Formiga,2017-12-31T00:00:00,1.8,1.0,44.44,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Formiga,2018-12-31T00:00:00,2.4,1.0,58.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Formiga,2019-12-31T00:00:00,1.24,1.0,19.35,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Formiga,2020-12-31T00:00:00,1.44,2.0,38.89,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Formiga,2021-12-31T00:00:00,1.35,2.0,48.15,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibia,2012-12-31T00:00:00,1.74,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibia,2013-12-31T00:00:00,1.74,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibia,2014-12-31T00:00:00,1.5,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibia,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibia,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibia,2017-12-31T00:00:00,1.59,2.0,25.79,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibia,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibia,2020-12-31T00:00:00,1.75,1.0,42.86,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibia,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibiraci,2012-12-31T00:00:00,2.49,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibiraci,2013-12-31T00:00:00,1.5,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibiraci,2014-12-31T00:00:00,2.01,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibiraci,2015-12-31T00:00:00,1.08,2.0,85.19,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibiraci,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibiraci,2017-12-31T00:00:00,1.61,2.0,24.22,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibiraci,2018-12-31T00:00:00,3.0,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibiraci,2019-12-31T00:00:00,1.28,2.0,56.25,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibiraci,2020-12-31T00:00:00,2.63,2.0,23.95,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibiraci,2021-12-31T00:00:00,1.33,2.0,50.38,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Indianopolis,2012-12-31T00:00:00,2.4,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Indianopolis,2013-12-31T00:00:00,1.92,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Indianopolis,2014-12-31T00:00:00,2.45,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Indianopolis,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Indianopolis,2016-12-31T00:00:00,2.7,2.0,25.93,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Indianopolis,2017-12-31T00:00:00,1.67,2.0,19.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Indianopolis,2018-12-31T00:00:00,2.52,2.0,20.63,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Indianopolis,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Indianopolis,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Indianopolis,2021-12-31T00:00:00,1.92,2.0,4.17,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Joao_Pinheiro,2012-12-31T00:00:00,2.82,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Joao_Pinheiro,2013-12-31T00:00:00,3.0,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Joao_Pinheiro,2014-12-31T00:00:00,2.7,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Joao_Pinheiro,2015-12-31T00:00:00,2.4,3.0,25.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Joao_Pinheiro,2016-12-31T00:00:00,3.6,3.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Joao_Pinheiro,2017-12-31T00:00:00,1.91,3.0,57.07,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Joao_Pinheiro,2018-12-31T00:00:00,2.4,3.0,25.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Joao_Pinheiro,2019-12-31T00:00:00,2.4,2.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Joao_Pinheiro,2020-12-31T00:00:00,3.0,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Joao_Pinheiro,2021-12-31T00:00:00,2.4,2.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Medeiros,2012-12-31T00:00:00,2.1,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Medeiros,2013-12-31T00:00:00,1.8,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Medeiros,2014-12-31T00:00:00,1.56,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Medeiros,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Medeiros,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Medeiros,2017-12-31T00:00:00,1.79,2.0,11.73,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Medeiros,2018-12-31T00:00:00,2.04,2.0,1.96,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Medeiros,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Medeiros,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Medeiros,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Monte_Carmelo,2012-12-31T00:00:00,2.34,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Monte_Carmelo,2013-12-31T00:00:00,1.89,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Monte_Carmelo,2014-12-31T00:00:00,2.28,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Monte_Carmelo,2015-12-31T00:00:00,1.32,2.0,51.52,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Monte_Carmelo,2016-12-31T00:00:00,2.64,2.0,24.24,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Monte_Carmelo,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Monte_Carmelo,2018-12-31T00:00:00,2.31,2.0,13.42,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Monte_Carmelo,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Monte_Carmelo,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Monte_Carmelo,2021-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ninheira,2012-12-31T00:00:00,3.0,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ninheira,2013-12-31T00:00:00,3.0,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ninheira,2014-12-31T00:00:00,3.0,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ninheira,2015-12-31T00:00:00,2.76,-0.0,100.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ninheira,2016-12-31T00:00:00,3.0,3.0,0.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ninheira,2017-12-31T00:00:00,2.34,3.0,28.21,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ninheira,2018-12-31T00:00:00,3.0,3.0,0.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ninheira,2019-12-31T00:00:00,2.7,3.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ninheira,2020-12-31T00:00:00,3.0,3.0,0.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ninheira,2021-12-31T00:00:00,2.1,3.0,42.86,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Paracatu,2012-12-31T00:00:00,3.45,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Paracatu,2013-12-31T00:00:00,3.45,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Paracatu,2014-12-31T00:00:00,3.0,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Paracatu,2015-12-31T00:00:00,1.8,3.0,66.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Paracatu,2016-12-31T00:00:00,2.4,3.0,25.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Paracatu,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Paracatu,2018-12-31T00:00:00,3.0,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Paracatu,2019-12-31T00:00:00,2.4,3.0,25.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Paracatu,2020-12-31T00:00:00,2.7,3.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Paracatu,2021-12-31T00:00:00,2.4,3.0,25.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Passos,2012-12-31T00:00:00,2.16,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Passos,2013-12-31T00:00:00,1.32,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Passos,2014-12-31T00:00:00,1.08,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Passos,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Passos,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Passos,2017-12-31T00:00:00,1.92,1.0,47.92,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Passos,2018-12-31T00:00:00,2.1,1.0,52.38,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Passos,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Passos,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Passos,2021-12-31T00:00:00,0.99,2.0,102.02,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Patrocinio,2012-12-31T00:00:00,2.1,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Patrocinio,2013-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Patrocinio,2014-12-31T00:00:00,1.8,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Patrocinio,2015-12-31T00:00:00,1.25,2.0,60.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Patrocinio,2016-12-31T00:00:00,2.64,2.0,24.24,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Patrocinio,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Patrocinio,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Patrocinio,2019-12-31T00:00:00,1.38,2.0,44.93,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Patrocinio,2020-12-31T00:00:00,1.75,2.0,14.29,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Patrocinio,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Perdizes,2012-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Perdizes,2013-12-31T00:00:00,1.56,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Perdizes,2014-12-31T00:00:00,1.66,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Perdizes,2015-12-31T00:00:00,1.48,2.0,35.14,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Perdizes,2016-12-31T00:00:00,1.98,2.0,1.01,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Perdizes,2017-12-31T00:00:00,2.07,2.0,3.38,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Perdizes,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Perdizes,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Perdizes,2020-12-31T00:00:00,2.05,2.0,2.44,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Perdizes,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Pimenta,2012-12-31T00:00:00,1.8,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Pimenta,2013-12-31T00:00:00,1.74,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Pimenta,2014-12-31T00:00:00,1.62,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Pimenta,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Pimenta,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Pimenta,2017-12-31T00:00:00,1.73,2.0,15.61,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Pimenta,2018-12-31T00:00:00,2.7,2.0,25.93,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Pimenta,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Pimenta,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Pimenta,2021-12-31T00:00:00,1.35,2.0,48.15,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Piumhi,2012-12-31T00:00:00,1.44,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Piumhi,2013-12-31T00:00:00,1.5,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Piumhi,2014-12-31T00:00:00,1.2,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Piumhi,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Piumhi,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Piumhi,2017-12-31T00:00:00,1.36,1.0,26.47,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Piumhi,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Piumhi,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Piumhi,2020-12-31T00:00:00,1.74,1.0,42.53,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Piumhi,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Presidente_Olegario,2012-12-31T00:00:00,2.66,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Presidente_Olegario,2013-12-31T00:00:00,2.65,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Presidente_Olegario,2014-12-31T00:00:00,2.32,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Presidente_Olegario,2015-12-31T00:00:00,1.88,2.0,6.38,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Presidente_Olegario,2016-12-31T00:00:00,2.52,2.0,20.63,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Presidente_Olegario,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Presidente_Olegario,2018-12-31T00:00:00,2.28,2.0,12.28,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Presidente_Olegario,2019-12-31T00:00:00,1.87,2.0,6.95,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Presidente_Olegario,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Presidente_Olegario,2021-12-31T00:00:00,1.64,2.0,21.95,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Rio_Paranaiba,2012-12-31T00:00:00,2.16,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Rio_Paranaiba,2013-12-31T00:00:00,2.07,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Rio_Paranaiba,2014-12-31T00:00:00,1.89,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Rio_Paranaiba,2015-12-31T00:00:00,1.42,2.0,40.85,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Rio_Paranaiba,2016-12-31T00:00:00,2.08,2.0,3.85,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Rio_Paranaiba,2017-12-31T00:00:00,1.39,2.0,43.88,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Rio_Paranaiba,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Rio_Paranaiba,2019-12-31T00:00:00,1.14,2.0,75.44,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Rio_Paranaiba,2020-12-31T00:00:00,1.82,1.0,45.05,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Rio_Paranaiba,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Romaria,2012-12-31T00:00:00,2.1,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Romaria,2013-12-31T00:00:00,2.1,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Romaria,2014-12-31T00:00:00,2.28,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Romaria,2015-12-31T00:00:00,1.2,2.0,66.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Romaria,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Romaria,2017-12-31T00:00:00,1.53,2.0,30.72,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Romaria,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Romaria,2019-12-31T00:00:00,2.2,2.0,9.09,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Romaria,2020-12-31T00:00:00,2.33,2.0,14.16,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Romaria,2021-12-31T00:00:00,2.05,2.0,2.44,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sacramento,2012-12-31T00:00:00,2.22,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sacramento,2013-12-31T00:00:00,1.87,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sacramento,2014-12-31T00:00:00,1.53,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sacramento,2015-12-31T00:00:00,1.34,2.0,49.25,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sacramento,2016-12-31T00:00:00,1.87,2.0,6.95,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sacramento,2017-12-31T00:00:00,1.31,2.0,52.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sacramento,2018-12-31T00:00:00,1.87,1.0,46.52,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sacramento,2019-12-31T00:00:00,1.23,1.0,18.7,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sacramento,2020-12-31T00:00:00,1.83,1.0,45.36,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sacramento,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Santa_Rosa_da_Serra,2012-12-31T00:00:00,3.17,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Santa_Rosa_da_Serra,2013-12-31T00:00:00,3.0,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Santa_Rosa_da_Serra,2014-12-31T00:00:00,2.1,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Santa_Rosa_da_Serra,2015-12-31T00:00:00,1.62,2.0,23.46,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Santa_Rosa_da_Serra,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Santa_Rosa_da_Serra,2017-12-31T00:00:00,1.63,2.0,22.7,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Santa_Rosa_da_Serra,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Santa_Rosa_da_Serra,2019-12-31T00:00:00,1.44,2.0,38.89,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Santa_Rosa_da_Serra,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Santa_Rosa_da_Serra,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sao_Gotardo,2012-12-31T00:00:00,1.92,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sao_Gotardo,2013-12-31T00:00:00,1.92,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sao_Gotardo,2014-12-31T00:00:00,1.71,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sao_Gotardo,2015-12-31T00:00:00,1.62,2.0,23.46,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sao_Gotardo,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sao_Gotardo,2017-12-31T00:00:00,1.44,2.0,38.89,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sao_Gotardo,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sao_Gotardo,2019-12-31T00:00:00,1.38,2.0,44.93,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sao_Gotardo,2020-12-31T00:00:00,1.38,1.0,27.54,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sao_Gotardo,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Serra_do_Salitre,2012-12-31T00:00:00,3.1,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Serra_do_Salitre,2013-12-31T00:00:00,1.74,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Serra_do_Salitre,2014-12-31T00:00:00,2.22,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Serra_do_Salitre,2015-12-31T00:00:00,1.29,2.0,55.04,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Serra_do_Salitre,2016-12-31T00:00:00,3.0,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Serra_do_Salitre,2017-12-31T00:00:00,1.12,2.0,78.57,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Serra_do_Salitre,2018-12-31T00:00:00,2.34,2.0,14.53,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Serra_do_Salitre,2019-12-31T00:00:00,1.49,1.0,32.89,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Serra_do_Salitre,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Serra_do_Salitre,2021-12-31T00:00:00,1.47,2.0,36.05,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Tiros,2012-12-31T00:00:00,2.28,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Tiros,2013-12-31T00:00:00,2.28,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Tiros,2014-12-31T00:00:00,1.79,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Tiros,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Tiros,2016-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Tiros,2017-12-31T00:00:00,3.0,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Tiros,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Tiros,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Tiros,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Tiros,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Unai,2012-12-31T00:00:00,2.94,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Unai,2013-12-31T00:00:00,3.0,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Unai,2014-12-31T00:00:00,2.94,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Unai,2015-12-31T00:00:00,2.64,3.0,13.64,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Unai,2016-12-31T00:00:00,2.7,3.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Unai,2017-12-31T00:00:00,2.6,3.0,15.38,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Unai,2018-12-31T00:00:00,2.52,3.0,19.05,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Unai,2019-12-31T00:00:00,2.82,3.0,6.38,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Unai,2020-12-31T00:00:00,2.34,3.0,28.21,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Unai,2021-12-31T00:00:00,2.64,2.0,24.24,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araguari,2022-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araxa,2022-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Bambui,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Buritizeiro,2022-12-31T00:00:00,3.0,3,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Campos_Altos,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Carmo_do_Paranaiba,2022-12-31T00:00:00,1.0,2,100.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Claraval,2022-12-31T00:00:00,1.0,2,100.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Coromandel,2022-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Estrela_do_Sul,2022-12-31T00:00:00,1.0,2,100.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Formiga,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibiraci,2022-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Indianopolis,2022-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Joao_Pinheiro,2022-12-31T00:00:00,2.0,3,50.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Medeiros,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Monte_Carmelo,2022-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ninheira,2022-12-31T00:00:00,3.0,2,-33.33,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Paracatu,2022-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Passos,2022-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Patrocinio,2022-12-31T00:00:00,1.0,2,100.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Perdizes,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Pimenta,2022-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Piumhi,2022-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Presidente_Olegario,2022-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Rio_Paranaiba,2022-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Romaria,2022-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sacramento,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Santa_Rosa_da_Serra,2022-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sao_Gotardo,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Serra_do_Salitre,2022-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Tiros,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Unai,2022-12-31T00:00:00,3.0,3,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araguari,2023-12-31T00:00:00,3.0,1.9011770486831665,-36.63,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Araxa,2023-12-31T00:00:00,1.638401296246287,1.3092591762542725,-20.09,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Bambui,2023-12-31T00:00:00,1.5,1.532402753829956,2.16,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Buritizeiro,2023-12-31T00:00:00,3.0,2.468458652496338,-17.72,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Campos_Altos,2023-12-31T00:00:00,1.380040526849037,1.534804344177246,11.21,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Carmo_do_Paranaiba,2023-12-31T00:00:00,2.369801007771799,1.5971914529800415,-32.6,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Claraval,2023-12-31T00:00:00,1.8600214362272245,1.5029809474945068,-19.2,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Coromandel,2023-12-31T00:00:00,2.0926575541308825,1.3680083751678467,-34.63,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Estrela_do_Sul,2023-12-31T00:00:00,2.43015873015873,1.7362854480743408,-28.55,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Formiga,2023-12-31T00:00:00,2.021543985637344,1.4738796949386597,-27.09,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibia,2023-12-31T00:00:00,1.668,1.3166781663894653,-21.06,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ibiraci,2023-12-31T00:00:00,1.804169884169884,1.6519889831542969,-8.43,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Indianopolis,2023-12-31T00:00:00,2.7000000000000006,1.8273625373840332,-32.32,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Joao_Pinheiro,2023-12-31T00:00:00,3.3,2.486980438232422,-24.64,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Medeiros,2023-12-31T00:00:00,1.86,1.57417893409729,-15.37,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Monte_Carmelo,2023-12-31T00:00:00,2.4900284900284904,1.8264819383621216,-26.65,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Ninheira,2023-12-31T00:00:00,3.0,2.5576188564300537,-14.75,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Paracatu,2023-12-31T00:00:00,2.4,2.4339089393615723,1.41,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Passos,2023-12-31T00:00:00,1.829801324503311,1.289335012435913,-29.54,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Patrocinio,2023-12-31T00:00:00,2.4370275910039414,1.2182769775390625,-50.01,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Perdizes,2023-12-31T00:00:00,2.106451612903226,1.5239735841751099,-27.65,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Pimenta,2023-12-31T00:00:00,1.504186046511628,1.443577527999878,-4.03,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Piumhi,2023-12-31T00:00:00,1.7399633363886338,1.4027409553527832,-19.38,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Presidente_Olegario,2023-12-31T00:00:00,2.160074626865672,1.8166606426239014,-15.9,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Rio_Paranaiba,2023-12-31T00:00:00,1.8000000000000005,1.2981736660003662,-27.88,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Romaria,2023-12-31T00:00:00,2.131578947368421,2.0810790061950684,-2.37,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sacramento,2023-12-31T00:00:00,1.75,1.511659026145935,-13.62,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Santa_Rosa_da_Serra,2023-12-31T00:00:00,1.68,1.8760393857955933,11.67,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Sao_Gotardo,2023-12-31T00:00:00,1.5,1.3636245727539062,-9.09,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Serra_do_Salitre,2023-12-31T00:00:00,1.489636363636364,1.5386626720428467,3.29,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Tiros,2023-12-31T00:00:00,2.382038834951457,1.4855413436889648,-37.64,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 2 - Metodo 1 (2023),Unai,2023-12-31T00:00:00,2.520095187731359,2.480503559112549,-1.57,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2023, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00012666975776441463
encoder_hidden_size: 64
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 1e-05
steps: 100
",2025-08-25T15:30:18
IBGE Cluster 0 - Metodo 1 (2020-2023),Alpinopolis,2012-12-31T00:00:00,2.7,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alpinopolis,2013-12-31T00:00:00,2.71,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alpinopolis,2014-12-31T00:00:00,1.68,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alpinopolis,2015-12-31T00:00:00,1.82,2.0,9.89,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alpinopolis,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alpinopolis,2017-12-31T00:00:00,2.06,2.0,2.91,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alpinopolis,2018-12-31T00:00:00,2.23,2.0,10.31,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Caparao,2012-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Caparao,2013-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Caparao,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Caparao,2015-12-31T00:00:00,1.2,2.0,66.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Caparao,2016-12-31T00:00:00,2.1,1.0,52.38,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Caparao,2017-12-31T00:00:00,1.37,1.0,27.01,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Caparao,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Jequitiba,2012-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Jequitiba,2013-12-31T00:00:00,1.62,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Jequitiba,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Jequitiba,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Jequitiba,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Jequitiba,2017-12-31T00:00:00,1.53,1.0,34.64,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Jequitiba,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Areado,2012-12-31T00:00:00,1.95,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Areado,2013-12-31T00:00:00,1.86,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Areado,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Areado,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Areado,2016-12-31T00:00:00,2.04,2.0,1.96,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Areado,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Areado,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Bom_Jesus_da_Penha,2012-12-31T00:00:00,2.58,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Bom_Jesus_da_Penha,2013-12-31T00:00:00,2.44,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Bom_Jesus_da_Penha,2014-12-31T00:00:00,2.0,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Bom_Jesus_da_Penha,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Bom_Jesus_da_Penha,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Bom_Jesus_da_Penha,2017-12-31T00:00:00,2.22,2.0,9.91,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Bom_Jesus_da_Penha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Botelhos,2012-12-31T00:00:00,1.38,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Botelhos,2013-12-31T00:00:00,1.56,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Botelhos,2014-12-31T00:00:00,1.74,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Botelhos,2015-12-31T00:00:00,1.44,2.0,38.89,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Botelhos,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Botelhos,2017-12-31T00:00:00,1.88,1.0,46.81,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Botelhos,2018-12-31T00:00:00,2.07,2.0,3.38,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Cabo_Verde,2012-12-31T00:00:00,2.07,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Cabo_Verde,2013-12-31T00:00:00,2.04,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Cabo_Verde,2014-12-31T00:00:00,1.68,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Cabo_Verde,2015-12-31T00:00:00,1.98,2.0,1.01,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Cabo_Verde,2016-12-31T00:00:00,1.65,2.0,21.21,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Cabo_Verde,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Cabo_Verde,2018-12-31T00:00:00,1.61,2.0,24.22,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caiana,2012-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caiana,2013-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caiana,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caiana,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caiana,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caiana,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caiana,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Campestre,2012-12-31T00:00:00,1.35,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Campestre,2013-12-31T00:00:00,1.31,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Campestre,2014-12-31T00:00:00,1.26,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Campestre,2015-12-31T00:00:00,1.91,1.0,47.64,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Campestre,2016-12-31T00:00:00,1.85,2.0,8.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Campestre,2017-12-31T00:00:00,2.11,2.0,5.21,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Campestre,2018-12-31T00:00:00,1.54,2.0,29.87,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caparao,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caparao,2013-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caparao,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caparao,2015-12-31T00:00:00,1.02,1.0,1.96,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caparao,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caparao,2017-12-31T00:00:00,1.45,1.0,31.03,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caparao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caputira,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caputira,2013-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caputira,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caputira,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caputira,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caputira,2017-12-31T00:00:00,0.77,1.0,29.87,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caputira,2018-12-31T00:00:00,1.2,2.0,66.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Carangola,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Carangola,2013-12-31T00:00:00,0.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Carangola,2014-12-31T00:00:00,0.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Carangola,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Carangola,2016-12-31T00:00:00,1.2,3.0,150.0,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Carangola,2017-12-31T00:00:00,1.1,1.0,9.09,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Carangola,2018-12-31T00:00:00,1.56,1.0,35.9,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Chale,2012-12-31T00:00:00,1.14,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Chale,2013-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Chale,2014-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Chale,2015-12-31T00:00:00,1.38,2.0,44.93,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Chale,2016-12-31T00:00:00,1.42,2.0,40.85,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Chale,2017-12-31T00:00:00,0.93,1.0,7.53,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Chale,2018-12-31T00:00:00,1.38,1.0,27.54,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Conceicao_da_Aparecida,2012-12-31T00:00:00,1.95,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Conceicao_da_Aparecida,2013-12-31T00:00:00,1.7,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Conceicao_da_Aparecida,2014-12-31T00:00:00,1.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Conceicao_da_Aparecida,2015-12-31T00:00:00,2.52,2.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Conceicao_da_Aparecida,2016-12-31T00:00:00,1.99,2.0,0.5,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Conceicao_da_Aparecida,2017-12-31T00:00:00,1.85,2.0,8.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Conceicao_da_Aparecida,2018-12-31T00:00:00,2.03,2.0,1.48,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Divino,2012-12-31T00:00:00,1.32,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Divino,2013-12-31T00:00:00,1.56,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Divino,2014-12-31T00:00:00,0.89,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Divino,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Divino,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Divino,2017-12-31T00:00:00,1.19,1.0,15.97,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Divino,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Durande,2012-12-31T00:00:00,1.56,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Durande,2013-12-31T00:00:00,2.22,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Durande,2014-12-31T00:00:00,1.44,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Durande,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Durande,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Durande,2017-12-31T00:00:00,1.33,2.0,50.38,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Durande,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ervalia,2012-12-31T00:00:00,1.38,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ervalia,2013-12-31T00:00:00,1.92,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ervalia,2014-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ervalia,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ervalia,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ervalia,2017-12-31T00:00:00,1.36,1.0,26.47,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ervalia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Espera_Feliz,2012-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Espera_Feliz,2013-12-31T00:00:00,1.92,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Espera_Feliz,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Espera_Feliz,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Espera_Feliz,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Espera_Feliz,2017-12-31T00:00:00,1.21,1.0,17.36,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Espera_Feliz,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guape,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guape,2013-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guape,2014-12-31T00:00:00,1.44,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guape,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guape,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guape,2017-12-31T00:00:00,1.92,2.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guape,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaranesia,2012-12-31T00:00:00,1.98,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaranesia,2013-12-31T00:00:00,1.52,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaranesia,2014-12-31T00:00:00,1.86,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaranesia,2015-12-31T00:00:00,1.65,2.0,21.21,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaranesia,2016-12-31T00:00:00,2.63,2.0,23.95,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaranesia,2017-12-31T00:00:00,1.36,2.0,47.06,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaranesia,2018-12-31T00:00:00,1.7,2.0,17.65,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaxupe,2012-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaxupe,2013-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaxupe,2014-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaxupe,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaxupe,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaxupe,2017-12-31T00:00:00,1.51,2.0,32.45,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaxupe,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guimarania,2012-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guimarania,2013-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guimarania,2014-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guimarania,2015-12-31T00:00:00,2.4,2.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guimarania,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guimarania,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guimarania,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ibitiura_de_Minas,2012-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ibitiura_de_Minas,2013-12-31T00:00:00,2.1,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ibitiura_de_Minas,2014-12-31T00:00:00,2.1,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ibitiura_de_Minas,2015-12-31T00:00:00,1.92,2.0,4.17,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ibitiura_de_Minas,2016-12-31T00:00:00,1.76,2.0,13.64,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ibitiura_de_Minas,2017-12-31T00:00:00,1.3,2.0,53.85,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ibitiura_de_Minas,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Jacui,2012-12-31T00:00:00,1.65,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Jacui,2013-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Jacui,2014-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Jacui,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Jacui,2016-12-31T00:00:00,1.7,1.0,41.18,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Jacui,2017-12-31T00:00:00,1.73,2.0,15.61,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Jacui,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Lajinha,2012-12-31T00:00:00,1.14,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Lajinha,2013-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Lajinha,2014-12-31T00:00:00,0.96,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Lajinha,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Lajinha,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Lajinha,2017-12-31T00:00:00,1.04,1.0,3.85,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Lajinha,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhuacu,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhuacu,2013-12-31T00:00:00,1.44,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhuacu,2014-12-31T00:00:00,0.96,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhuacu,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhuacu,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhuacu,2017-12-31T00:00:00,1.43,1.0,30.07,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhuacu,2018-12-31T00:00:00,1.44,1.0,30.56,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhumirim,2012-12-31T00:00:00,1.56,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhumirim,2013-12-31T00:00:00,1.92,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhumirim,2014-12-31T00:00:00,1.26,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhumirim,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhumirim,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhumirim,2017-12-31T00:00:00,1.62,1.0,38.27,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhumirim,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Martins_Soares,2012-12-31T00:00:00,2.1,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Martins_Soares,2013-12-31T00:00:00,1.98,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Martins_Soares,2014-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Martins_Soares,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Martins_Soares,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Martins_Soares,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Martins_Soares,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Matipo,2012-12-31T00:00:00,1.14,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Matipo,2013-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Matipo,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Matipo,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Matipo,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Matipo,2017-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Matipo,2018-12-31T00:00:00,1.32,2.0,51.52,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Miradouro,2012-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Miradouro,2013-12-31T00:00:00,2.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Miradouro,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Miradouro,2015-12-31T00:00:00,1.59,1.0,37.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Miradouro,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Miradouro,2017-12-31T00:00:00,1.16,1.0,13.79,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Miradouro,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Mutum,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Mutum,2013-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Mutum,2014-12-31T00:00:00,1.14,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Mutum,2015-12-31T00:00:00,1.15,1.0,13.04,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Mutum,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Mutum,2017-12-31T00:00:00,1.45,1.0,31.03,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Mutum,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Belem,2012-12-31T00:00:00,0.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Belem,2013-12-31T00:00:00,0.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Belem,2014-12-31T00:00:00,0.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Belem,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Belem,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Belem,2017-12-31T00:00:00,0.48,1.0,108.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Belem,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Resende,2012-12-31T00:00:00,2.88,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Resende,2013-12-31T00:00:00,2.18,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Resende,2014-12-31T00:00:00,1.7,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Resende,2015-12-31T00:00:00,2.2,2.0,9.09,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Resende,2016-12-31T00:00:00,2.08,2.0,3.85,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Resende,2017-12-31T00:00:00,1.76,2.0,13.64,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Resende,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Orizania,2012-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Orizania,2013-12-31T00:00:00,1.68,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Orizania,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Orizania,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Orizania,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Orizania,2017-12-31T00:00:00,1.47,1.0,31.97,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Orizania,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Patos_de_Minas,2012-12-31T00:00:00,1.92,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Patos_de_Minas,2013-12-31T00:00:00,2.38,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Patos_de_Minas,2014-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Patos_de_Minas,2015-12-31T00:00:00,2.4,2.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Patos_de_Minas,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Patos_de_Minas,2017-12-31T00:00:00,1.7,2.0,17.65,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Patos_de_Minas,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Reduto,2012-12-31T00:00:00,1.26,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Reduto,2013-12-31T00:00:00,1.62,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Reduto,2014-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Reduto,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Reduto,2016-12-31T00:00:00,1.44,1.0,30.56,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Reduto,2017-12-31T00:00:00,1.26,1.0,20.63,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Reduto,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Barbara_do_Leste,2012-12-31T00:00:00,1.02,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Barbara_do_Leste,2013-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Barbara_do_Leste,2014-12-31T00:00:00,0.96,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Barbara_do_Leste,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Barbara_do_Leste,2016-12-31T00:00:00,1.02,1.0,1.96,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Barbara_do_Leste,2017-12-31T00:00:00,0.91,1.0,9.89,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Barbara_do_Leste,2018-12-31T00:00:00,1.74,1.0,42.53,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Margarida,2012-12-31T00:00:00,1.38,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Margarida,2013-12-31T00:00:00,1.68,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Margarida,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Margarida,2015-12-31T00:00:00,1.56,1.0,35.9,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Margarida,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Margarida,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Margarida,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Rita_de_Minas,2012-12-31T00:00:00,0.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Rita_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Rita_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Rita_de_Minas,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Rita_de_Minas,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Rita_de_Minas,2017-12-31T00:00:00,1.25,1.0,20.0,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Rita_de_Minas,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santana_do_Manhuacu,2012-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santana_do_Manhuacu,2013-12-31T00:00:00,1.62,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santana_do_Manhuacu,2014-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santana_do_Manhuacu,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santana_do_Manhuacu,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santana_do_Manhuacu,2017-12-31T00:00:00,1.28,1.0,21.88,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santana_do_Manhuacu,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Domingos_das_Dores,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Domingos_das_Dores,2013-12-31T00:00:00,1.8,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Domingos_das_Dores,2014-12-31T00:00:00,1.5,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Domingos_das_Dores,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Domingos_das_Dores,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Domingos_das_Dores,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Domingos_das_Dores,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Joao_do_Manhuacu,2012-12-31T00:00:00,1.26,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Joao_do_Manhuacu,2013-12-31T00:00:00,1.74,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Joao_do_Manhuacu,2014-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Joao_do_Manhuacu,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Joao_do_Manhuacu,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Joao_do_Manhuacu,2017-12-31T00:00:00,1.7,1.0,41.18,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Joao_do_Manhuacu,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sericita,2012-12-31T00:00:00,1.2,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sericita,2013-12-31T00:00:00,1.56,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sericita,2014-12-31T00:00:00,1.08,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sericita,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sericita,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sericita,2017-12-31T00:00:00,1.12,1.0,10.71,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sericita,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Varjao_de_Minas,2012-12-31T00:00:00,2.4,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Varjao_de_Minas,2013-12-31T00:00:00,2.4,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Varjao_de_Minas,2014-12-31T00:00:00,3.0,-,-,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Varjao_de_Minas,2015-12-31T00:00:00,2.4,3.0,25.0,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Varjao_de_Minas,2016-12-31T00:00:00,2.4,3.0,25.0,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Varjao_de_Minas,2017-12-31T00:00:00,2.4,3.0,25.0,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Varjao_de_Minas,2018-12-31T00:00:00,2.4,4.0,66.67,treino,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alpinopolis,2019-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Caparao,2019-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Jequitiba,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Areado,2019-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Bom_Jesus_da_Penha,2019-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Botelhos,2019-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Cabo_Verde,2019-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caiana,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Campestre,2019-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caparao,2019-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caputira,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Carangola,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Chale,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Conceicao_da_Aparecida,2019-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Divino,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Durande,2019-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ervalia,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Espera_Feliz,2019-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guape,2019-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaranesia,2019-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaxupe,2019-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guimarania,2019-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ibitiura_de_Minas,2019-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Jacui,2019-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Lajinha,2019-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhuacu,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhumirim,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Martins_Soares,2019-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Matipo,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Miradouro,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Mutum,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Belem,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Resende,2019-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Orizania,2019-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Patos_de_Minas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Reduto,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Barbara_do_Leste,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Margarida,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Rita_de_Minas,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santana_do_Manhuacu,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Domingos_das_Dores,2019-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Joao_do_Manhuacu,2019-12-31T00:00:00,1.0,2,100.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sericita,2019-12-31T00:00:00,1.0,1,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Varjao_de_Minas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alpinopolis,2020-12-31T00:00:00,2.765998457979954,2.127368450164795,-23.09,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Caparao,2020-12-31T00:00:00,1.92,1.4119887351989746,-26.46,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Jequitiba,2020-12-31T00:00:00,1.8000000000000005,1.569544792175293,-12.8,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Areado,2020-12-31T00:00:00,2.078746484531941,1.713303565979004,-17.58,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Bom_Jesus_da_Penha,2020-12-31T00:00:00,1.986087924318308,2.0582752227783203,3.63,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Botelhos,2020-12-31T00:00:00,1.716062736614386,1.9782742261886597,15.28,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Cabo_Verde,2020-12-31T00:00:00,2.249052581714827,1.7856889963150024,-20.6,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caiana,2020-12-31T00:00:00,1.5,1.3120689392089844,-12.53,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Campestre,2020-12-31T00:00:00,2.138000770416025,1.796068549156189,-15.99,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caparao,2020-12-31T00:00:00,1.9199600798403191,1.5751491785049438,-17.96,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caputira,2020-12-31T00:00:00,1.8000000000000005,1.1401147842407227,-36.66,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Carangola,2020-12-31T00:00:00,1.5,1.3492268323898315,-10.05,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Chale,2020-12-31T00:00:00,1.71015625,1.183904767036438,-30.77,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Conceicao_da_Aparecida,2020-12-31T00:00:00,2.243948871362524,1.9611458778381348,-12.6,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Divino,2020-12-31T00:00:00,1.8000483851457605,1.2116563320159912,-32.69,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Durande,2020-12-31T00:00:00,2.4,1.6486220359802246,-31.31,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ervalia,2020-12-31T00:00:00,1.7999614197530858,1.4938206672668457,-17.01,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Espera_Feliz,2020-12-31T00:00:00,2.04,1.3771718740463257,-32.49,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guape,2020-12-31T00:00:00,1.847986942328618,1.8776464462280273,1.6,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaranesia,2020-12-31T00:00:00,1.564686285397002,1.4199795722961426,-9.25,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaxupe,2020-12-31T00:00:00,1.7400000000000002,1.49672269821167,-13.98,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guimarania,2020-12-31T00:00:00,2.149930843706777,1.6086137294769287,-25.18,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ibitiura_de_Minas,2020-12-31T00:00:00,1.68,1.4050260782241821,-16.37,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Jacui,2020-12-31T00:00:00,1.68,1.7396266460418701,3.55,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Lajinha,2020-12-31T00:00:00,1.9200446677833607,1.2811583280563354,-33.27,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhuacu,2020-12-31T00:00:00,1.6800182481751822,1.3705135583877563,-18.42,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhumirim,2020-12-31T00:00:00,1.68,1.5248425006866455,-9.24,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Martins_Soares,2020-12-31T00:00:00,1.8000000000000005,1.603036880493164,-10.94,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Matipo,2020-12-31T00:00:00,1.56,1.258709192276001,-19.31,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Miradouro,2020-12-31T00:00:00,1.5,1.349820852279663,-10.01,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Mutum,2020-12-31T00:00:00,1.65,1.4734079837799072,-10.7,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Belem,2020-12-31T00:00:00,1.1,0.8623760342597961,-21.6,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Resende,2020-12-31T00:00:00,2.353040067245727,1.8055408000946045,-23.27,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Orizania,2020-12-31T00:00:00,1.679901960784314,1.4798121452331543,-11.91,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Patos_de_Minas,2020-12-31T00:00:00,1.941759465478842,1.6894807815551758,-12.99,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Reduto,2020-12-31T00:00:00,1.8000000000000005,1.5311188697814941,-14.94,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Barbara_do_Leste,2020-12-31T00:00:00,1.4400953029271608,1.3619835376739502,-5.42,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Margarida,2020-12-31T00:00:00,1.8000000000000005,1.4143140316009521,-21.43,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Rita_de_Minas,2020-12-31T00:00:00,1.5,1.3959858417510986,-6.93,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santana_do_Manhuacu,2020-12-31T00:00:00,1.8000000000000005,1.4655507802963257,-18.58,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Domingos_das_Dores,2020-12-31T00:00:00,1.56,1.6190568208694458,3.79,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Joao_do_Manhuacu,2020-12-31T00:00:00,1.8000000000000005,1.616971492767334,-10.17,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sericita,2020-12-31T00:00:00,2.5200000000000005,1.232366681098938,-51.1,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Varjao_de_Minas,2020-12-31T00:00:00,2.519786096256685,2.447603464126587,-2.86,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alpinopolis,2021-12-31T00:00:00,1.67993145468393,2.2688426971435547,35.06,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Caparao,2021-12-31T00:00:00,1.08,1.4971710443496704,38.63,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Jequitiba,2021-12-31T00:00:00,0.9,1.6025662422180176,78.06,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Areado,2021-12-31T00:00:00,1.5228426395939092,1.812321424484253,19.01,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Bom_Jesus_da_Penha,2021-12-31T00:00:00,1.519900497512438,1.957015872001648,28.76,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Botelhos,2021-12-31T00:00:00,1.26,1.8794053792953491,49.16,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Cabo_Verde,2021-12-31T00:00:00,1.32,1.9016833305358887,44.07,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caiana,2021-12-31T00:00:00,0.9,1.3013932704925537,44.6,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Campestre,2021-12-31T00:00:00,1.2899602385685882,1.6929044723510742,31.24,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caparao,2021-12-31T00:00:00,0.9,1.5685945749282837,74.29,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caputira,2021-12-31T00:00:00,1.38008658008658,1.200256109237671,-13.03,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Carangola,2021-12-31T00:00:00,1.02,1.3824286460876465,35.53,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Chale,2021-12-31T00:00:00,1.26016713091922,1.2740107774734497,1.1,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Conceicao_da_Aparecida,2021-12-31T00:00:00,1.62,2.065406322479248,27.49,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Divino,2021-12-31T00:00:00,1.08,1.2067623138427734,11.74,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Durande,2021-12-31T00:00:00,1.32,1.6926541328430176,28.23,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ervalia,2021-12-31T00:00:00,1.08,1.3804857730865479,27.82,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Espera_Feliz,2021-12-31T00:00:00,0.78,1.31442391872406,68.52,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guape,2021-12-31T00:00:00,1.376842105263158,1.8165122270584106,31.93,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaranesia,2021-12-31T00:00:00,1.031014249790444,1.4268178939819336,38.39,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaxupe,2021-12-31T00:00:00,1.02,1.57476806640625,54.39,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guimarania,2021-12-31T00:00:00,1.541380188439012,1.6859490871429443,9.38,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ibitiura_de_Minas,2021-12-31T00:00:00,1.9198795180722887,1.5063674449920654,-21.54,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Jacui,2021-12-31T00:00:00,0.96,1.7091023921966553,78.03,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Lajinha,2021-12-31T00:00:00,1.260014255167498,1.354729413986206,7.52,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhuacu,2021-12-31T00:00:00,0.9,1.2142525911331177,34.92,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhumirim,2021-12-31T00:00:00,1.2,1.4043688774108887,17.03,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Martins_Soares,2021-12-31T00:00:00,1.2,1.649871587753296,37.49,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Matipo,2021-12-31T00:00:00,1.07995337995338,1.3220932483673096,22.42,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Miradouro,2021-12-31T00:00:00,1.2,1.394024133682251,16.17,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Mutum,2021-12-31T00:00:00,1.319934249850568,1.5035505294799805,13.91,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Belem,2021-12-31T00:00:00,1.0,0.9882774949073792,-1.17,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Resende,2021-12-31T00:00:00,1.3800031392246113,1.945970058441162,41.01,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Orizania,2021-12-31T00:00:00,1.08,1.4856741428375244,37.56,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Patos_de_Minas,2021-12-31T00:00:00,1.7224109589041097,1.6906179189682007,-1.85,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Reduto,2021-12-31T00:00:00,1.2,1.4621331691741943,21.84,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Barbara_do_Leste,2021-12-31T00:00:00,1.319917440660475,1.2756476402282715,-3.35,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Margarida,2021-12-31T00:00:00,0.9,1.3649015426635742,51.66,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Rita_de_Minas,2021-12-31T00:00:00,1.2,1.39327871799469,16.11,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santana_do_Manhuacu,2021-12-31T00:00:00,1.32,1.5185058116912842,15.04,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Domingos_das_Dores,2021-12-31T00:00:00,1.2,1.461555004119873,21.8,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Joao_do_Manhuacu,2021-12-31T00:00:00,0.9,1.4666489362716675,62.96,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sericita,2021-12-31T00:00:00,0.9,1.0054612159729004,11.72,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Varjao_de_Minas,2021-12-31T00:00:00,2.390254805543138,2.4180800914764404,1.16,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alpinopolis,2022-12-31T00:00:00,1.4420494699646638,1.9421782493591309,34.68,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Caparao,2022-12-31T00:00:00,1.5,1.108910083770752,-26.07,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Jequitiba,2022-12-31T00:00:00,0.9,1.045854926109314,16.21,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Areado,2022-12-31T00:00:00,0.9076923076923076,1.6807200908660889,85.16,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Bom_Jesus_da_Penha,2022-12-31T00:00:00,1.6298200514138823,1.7743375301361084,8.87,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Botelhos,2022-12-31T00:00:00,0.9,1.4621638059616089,62.46,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Cabo_Verde,2022-12-31T00:00:00,1.35,1.6473500728607178,22.03,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caiana,2022-12-31T00:00:00,0.9,0.9908841848373413,10.1,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Campestre,2022-12-31T00:00:00,1.020018115942029,1.467440128326416,43.86,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caparao,2022-12-31T00:00:00,1.32,1.1250159740447998,-14.77,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caputira,2022-12-31T00:00:00,1.440044247787611,1.2989330291748047,-9.8,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Carangola,2022-12-31T00:00:00,1.02,1.1628541946411133,14.01,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Chale,2022-12-31T00:00:00,1.44,1.289280652999878,-10.47,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Conceicao_da_Aparecida,2022-12-31T00:00:00,1.53,1.8215386867523193,19.05,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Divino,2022-12-31T00:00:00,1.08,1.1382222175598145,5.39,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Durande,2022-12-31T00:00:00,1.5,1.2408851385116577,-17.27,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ervalia,2022-12-31T00:00:00,1.2,0.9311993718147278,-22.4,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Espera_Feliz,2022-12-31T00:00:00,1.2,0.8743792772293091,-27.14,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guape,2022-12-31T00:00:00,1.262758620689655,1.5214935541152954,20.49,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaranesia,2022-12-31T00:00:00,1.086968085106383,1.1795129776000977,8.51,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaxupe,2022-12-31T00:00:00,1.14,1.2199758291244507,7.02,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guimarania,2022-12-31T00:00:00,1.2220588235294123,1.5786001682281494,29.18,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ibitiura_de_Minas,2022-12-31T00:00:00,1.3799999999999997,1.515697956085205,9.83,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Jacui,2022-12-31T00:00:00,1.08,1.267242670059204,17.34,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Lajinha,2022-12-31T00:00:00,1.419975786924939,1.0764179229736328,-24.19,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhuacu,2022-12-31T00:00:00,1.44,0.6397457122802734,-55.57,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhumirim,2022-12-31T00:00:00,1.3799999999999997,1.1982496976852417,-13.17,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Martins_Soares,2022-12-31T00:00:00,1.3799999999999997,1.2864561080932617,-6.78,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Matipo,2022-12-31T00:00:00,1.26,1.185704231262207,-5.9,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Miradouro,2022-12-31T00:00:00,0.96,1.2225459814071655,27.35,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Mutum,2022-12-31T00:00:00,1.379940564635958,1.2082102298736572,-12.44,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Belem,2022-12-31T00:00:00,0.96,0.9054240584373474,-5.68,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Resende,2022-12-31T00:00:00,1.5,1.7545082569122314,16.97,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Orizania,2022-12-31T00:00:00,1.5,1.2484867572784424,-16.77,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Patos_de_Minas,2022-12-31T00:00:00,1.91614730878187,1.696763277053833,-11.45,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Reduto,2022-12-31T00:00:00,1.260059171597633,1.1055208444595337,-12.26,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Barbara_do_Leste,2022-12-31T00:00:00,1.05015873015873,1.1947122812271118,13.76,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Margarida,2022-12-31T00:00:00,1.3799999999999997,1.0646616220474243,-22.85,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Rita_de_Minas,2022-12-31T00:00:00,1.019909502262443,1.2353253364562988,21.12,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santana_do_Manhuacu,2022-12-31T00:00:00,1.26,1.3515757322311401,7.27,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Domingos_das_Dores,2022-12-31T00:00:00,1.2,1.2148449420928955,1.24,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Joao_do_Manhuacu,2022-12-31T00:00:00,1.2,1.1779296398162842,-1.84,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sericita,2022-12-31T00:00:00,1.3799999999999997,0.9640920162200928,-30.14,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Varjao_de_Minas,2022-12-31T00:00:00,2.383928571428572,2.4402966499328613,2.36,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alpinopolis,2023-12-31T00:00:00,1.8000000000000005,1.682985782623291,-6.5,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Caparao,2023-12-31T00:00:00,1.32,1.374993085861206,4.17,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Alto_Jequitiba,2023-12-31T00:00:00,1.08,1.0221631526947021,-5.36,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Areado,2023-12-31T00:00:00,1.590034364261168,1.5084192752838135,-5.13,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Bom_Jesus_da_Penha,2023-12-31T00:00:00,1.2,1.6943073272705078,41.19,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Botelhos,2023-12-31T00:00:00,1.080052666227781,1.5035197734832764,39.21,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Cabo_Verde,2023-12-31T00:00:00,1.319976635514019,1.4602012634277344,10.62,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caiana,2023-12-31T00:00:00,1.08,0.9986497163772583,-7.53,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Campestre,2023-12-31T00:00:00,1.319964428634949,1.718252420425415,30.17,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caparao,2023-12-31T00:00:00,1.32,1.2141902446746826,-8.02,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Caputira,2023-12-31T00:00:00,1.380065005417118,1.4707863330841064,6.57,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Carangola,2023-12-31T00:00:00,1.140118343195266,1.1345105171203613,-0.49,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Chale,2023-12-31T00:00:00,1.3799999999999997,1.4176700115203857,2.73,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Conceicao_da_Aparecida,2023-12-31T00:00:00,1.679948420373952,1.7607417106628418,4.81,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Divino,2023-12-31T00:00:00,1.02,1.2584565877914429,23.38,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Durande,2023-12-31T00:00:00,1.3799999999999997,1.540170669555664,11.61,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ervalia,2023-12-31T00:00:00,1.380044843049327,1.3727071285247803,-0.53,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Espera_Feliz,2023-12-31T00:00:00,1.08,1.1201062202453613,3.71,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guape,2023-12-31T00:00:00,1.560674157303371,1.416260838508606,-9.25,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaranesia,2023-12-31T00:00:00,1.1961439588688951,1.185233235359192,-0.91,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guaxupe,2023-12-31T00:00:00,1.2,1.1278812885284424,-6.01,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Guimarania,2023-12-31T00:00:00,1.7704600484261497,1.731338381767273,-2.21,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Ibitiura_de_Minas,2023-12-31T00:00:00,1.150344827586207,1.7231371402740479,49.79,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Jacui,2023-12-31T00:00:00,1.32,1.2804049253463745,-3.0,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Lajinha,2023-12-31T00:00:00,1.4399536768963517,1.4341862201690674,-0.4,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhuacu,2023-12-31T00:00:00,1.2,1.1865676641464233,-1.12,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Manhumirim,2023-12-31T00:00:00,1.44,1.2936389446258545,-10.16,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Martins_Soares,2023-12-31T00:00:00,1.2,1.386570930480957,15.55,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Matipo,2023-12-31T00:00:00,1.25990675990676,1.2248849868774414,-2.78,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Miradouro,2023-12-31T00:00:00,1.140084388185654,1.245440125465393,9.24,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Mutum,2023-12-31T00:00:00,1.019985196150999,1.193974256515503,17.06,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Belem,2023-12-31T00:00:00,0.96,1.0177000761032104,6.01,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Nova_Resende,2023-12-31T00:00:00,1.56,1.512194275856018,-3.06,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Orizania,2023-12-31T00:00:00,1.2,1.3546497821807861,12.89,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Patos_de_Minas,2023-12-31T00:00:00,1.782452316076294,1.852636456489563,3.94,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Reduto,2023-12-31T00:00:00,0.9,1.3485687971115112,49.84,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Barbara_do_Leste,2023-12-31T00:00:00,1.08,1.2086474895477295,11.91,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Margarida,2023-12-31T00:00:00,1.32,1.2214770317077637,-7.46,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santa_Rita_de_Minas,2023-12-31T00:00:00,1.019930675909879,1.1802258491516113,15.72,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Santana_do_Manhuacu,2023-12-31T00:00:00,1.32,1.3799091577529907,4.54,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Domingos_das_Dores,2023-12-31T00:00:00,1.296078431372549,1.277106523513794,-1.46,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sao_Joao_do_Manhuacu,2023-12-31T00:00:00,1.2,1.1654126644134521,-2.88,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Sericita,2023-12-31T00:00:00,1.26,1.3647798299789429,8.32,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 0 - Metodo 1 (2020-2023),Varjao_de_Minas,2023-12-31T00:00:00,2.3892857142857142,2.430088520050049,1.71,teste,V22 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00010524987278525288
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.2
weight_decay: 0.0001
steps: 100
",2025-08-27T13:42:09
IBGE Cluster 1 - Metodo 1 (2020-2023),Abre_Campo,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Abre_Campo,2013-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Abre_Campo,2014-12-31T00:00:00,0.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Abre_Campo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Abre_Campo,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Abre_Campo,2017-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Abre_Campo,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alfenas,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alfenas,2013-12-31T00:00:00,1.98,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alfenas,2014-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alfenas,2015-12-31T00:00:00,1.38,2.0,44.93,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alfenas,2016-12-31T00:00:00,2.25,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alfenas,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alfenas,2018-12-31T00:00:00,2.34,2.0,14.53,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alterosa,2012-12-31T00:00:00,1.59,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alterosa,2013-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alterosa,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alterosa,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alterosa,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alterosa,2017-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alterosa,2018-12-31T00:00:00,1.96,2.0,2.04,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Andradas,2012-12-31T00:00:00,1.98,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Andradas,2013-12-31T00:00:00,1.59,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Andradas,2014-12-31T00:00:00,1.21,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Andradas,2015-12-31T00:00:00,1.89,2.0,5.82,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Andradas,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Andradas,2017-12-31T00:00:00,1.34,2.0,49.25,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Andradas,2018-12-31T00:00:00,1.73,2.0,15.61,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Angelandia,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Angelandia,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Angelandia,2014-12-31T00:00:00,1.15,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Angelandia,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Angelandia,2016-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Angelandia,2017-12-31T00:00:00,0.94,1.0,6.38,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Angelandia,2018-12-31T00:00:00,1.68,1.0,40.48,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Araponga,2012-12-31T00:00:00,1.14,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Araponga,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Araponga,2014-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Araponga,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Araponga,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Araponga,2017-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Araponga,2018-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Boa_Esperanca,2012-12-31T00:00:00,1.56,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Boa_Esperanca,2013-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Boa_Esperanca,2014-12-31T00:00:00,1.15,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Boa_Esperanca,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Boa_Esperanca,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Boa_Esperanca,2017-12-31T00:00:00,2.53,2.0,20.95,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Boa_Esperanca,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bom_Sucesso,2012-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bom_Sucesso,2013-12-31T00:00:00,1.26,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bom_Sucesso,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bom_Sucesso,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bom_Sucesso,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bom_Sucesso,2017-12-31T00:00:00,1.48,2.0,35.14,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bom_Sucesso,2018-12-31T00:00:00,1.7,2.0,17.65,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bueno_Brandao,2012-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bueno_Brandao,2013-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bueno_Brandao,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bueno_Brandao,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bueno_Brandao,2016-12-31T00:00:00,2.1,1.0,52.38,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bueno_Brandao,2017-12-31T00:00:00,1.2,2.0,66.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bueno_Brandao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cambuquira,2012-12-31T00:00:00,1.15,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cambuquira,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cambuquira,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cambuquira,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cambuquira,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cambuquira,2017-12-31T00:00:00,1.82,1.0,45.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cambuquira,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campanha,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campanha,2013-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campanha,2014-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campanha,2015-12-31T00:00:00,1.02,1.0,1.96,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campanha,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campanha,2017-12-31T00:00:00,1.88,1.0,46.81,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campanha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_Belo,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_Belo,2013-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_Belo,2014-12-31T00:00:00,0.63,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_Belo,2015-12-31T00:00:00,0.84,1.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_Belo,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_Belo,2017-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_Belo,2018-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_do_Meio,2012-12-31T00:00:00,1.17,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_do_Meio,2013-12-31T00:00:00,1.17,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_do_Meio,2014-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_do_Meio,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_do_Meio,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_do_Meio,2017-12-31T00:00:00,2.08,1.0,51.92,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_do_Meio,2018-12-31T00:00:00,1.82,2.0,9.89,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campos_Gerais,2012-12-31T00:00:00,1.65,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campos_Gerais,2013-12-31T00:00:00,1.22,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campos_Gerais,2014-12-31T00:00:00,1.15,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campos_Gerais,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campos_Gerais,2016-12-31T00:00:00,2.5,1.0,60.0,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campos_Gerais,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campos_Gerais,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Candeias,2012-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Candeias,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Candeias,2014-12-31T00:00:00,0.99,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Candeias,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Candeias,2016-12-31T00:00:00,2.28,2.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Candeias,2017-12-31T00:00:00,1.38,2.0,44.93,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Candeias,2018-12-31T00:00:00,2.04,2.0,1.96,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capelinha,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capelinha,2013-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capelinha,2014-12-31T00:00:00,1.26,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capelinha,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capelinha,2016-12-31T00:00:00,1.08,2.0,85.19,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capelinha,2017-12-31T00:00:00,1.16,1.0,13.79,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capelinha,2018-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capetinga,2012-12-31T00:00:00,1.81,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capetinga,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capetinga,2014-12-31T00:00:00,1.59,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capetinga,2015-12-31T00:00:00,1.2,2.0,66.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capetinga,2016-12-31T00:00:00,1.93,2.0,3.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capetinga,2017-12-31T00:00:00,2.09,2.0,4.31,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capetinga,2018-12-31T00:00:00,2.7,2.0,25.93,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capitolio,2012-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capitolio,2013-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capitolio,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capitolio,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capitolio,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capitolio,2017-12-31T00:00:00,1.47,1.0,31.97,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capitolio,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Caratinga,2012-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Caratinga,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Caratinga,2014-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Caratinga,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Caratinga,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Caratinga,2017-12-31T00:00:00,1.46,1.0,31.51,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Caratinga,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_da_Cachoeira,2012-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_da_Cachoeira,2013-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_da_Cachoeira,2014-12-31T00:00:00,1.15,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_da_Cachoeira,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_da_Cachoeira,2016-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_da_Cachoeira,2017-12-31T00:00:00,1.73,1.0,42.2,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_da_Cachoeira,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_de_Minas,2012-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_de_Minas,2013-12-31T00:00:00,1.74,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_de_Minas,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_de_Minas,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_de_Minas,2017-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_de_Minas,2018-12-31T00:00:00,1.56,1.0,35.9,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_do_Rio_Claro,2012-12-31T00:00:00,1.66,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_do_Rio_Claro,2013-12-31T00:00:00,1.94,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_do_Rio_Claro,2014-12-31T00:00:00,1.65,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_do_Rio_Claro,2015-12-31T00:00:00,1.53,2.0,30.72,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_do_Rio_Claro,2016-12-31T00:00:00,2.37,2.0,15.61,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_do_Rio_Claro,2017-12-31T00:00:00,1.82,2.0,9.89,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_do_Rio_Claro,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cassia,2012-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cassia,2013-12-31T00:00:00,1.56,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cassia,2014-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cassia,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cassia,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cassia,2017-12-31T00:00:00,1.6,1.0,37.5,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cassia,2018-12-31T00:00:00,2.1,1.0,52.38,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conceicao_do_Rio_Verde,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conceicao_do_Rio_Verde,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conceicao_do_Rio_Verde,2014-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conceicao_do_Rio_Verde,2015-12-31T00:00:00,1.62,1.0,38.27,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conceicao_do_Rio_Verde,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conceicao_do_Rio_Verde,2017-12-31T00:00:00,2.04,2.0,1.96,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conceicao_do_Rio_Verde,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conselheiro_Pena,2012-12-31T00:00:00,1.48,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conselheiro_Pena,2013-12-31T00:00:00,1.3,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conselheiro_Pena,2014-12-31T00:00:00,1.3,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conselheiro_Pena,2015-12-31T00:00:00,1.06,1.0,5.66,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conselheiro_Pena,2016-12-31T00:00:00,1.18,1.0,15.25,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conselheiro_Pena,2017-12-31T00:00:00,1.29,1.0,22.48,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conselheiro_Pena,2018-12-31T00:00:00,1.62,1.0,38.27,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Coqueiral,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Coqueiral,2013-12-31T00:00:00,1.14,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Coqueiral,2014-12-31T00:00:00,1.14,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Coqueiral,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Coqueiral,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Coqueiral,2017-12-31T00:00:00,1.75,1.0,42.86,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Coqueiral,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cristais,2012-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cristais,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cristais,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cristais,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cristais,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cristais,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cristais,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Divisa_Nova,2012-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Divisa_Nova,2013-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Divisa_Nova,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Divisa_Nova,2015-12-31T00:00:00,1.08,2.0,85.19,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Divisa_Nova,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Divisa_Nova,2017-12-31T00:00:00,2.28,1.0,56.14,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Divisa_Nova,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Eloi_Mendes,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Eloi_Mendes,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Eloi_Mendes,2014-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Eloi_Mendes,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Eloi_Mendes,2016-12-31T00:00:00,1.74,1.0,42.53,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Eloi_Mendes,2017-12-31T00:00:00,1.83,1.0,45.36,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Eloi_Mendes,2018-12-31T00:00:00,1.72,2.0,16.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Fervedouro,2012-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Fervedouro,2013-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Fervedouro,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Fervedouro,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Fervedouro,2016-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Fervedouro,2017-12-31T00:00:00,1.06,1.0,5.66,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Fervedouro,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Heliodora,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Heliodora,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Heliodora,2014-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Heliodora,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Heliodora,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Heliodora,2017-12-31T00:00:00,1.57,1.0,36.31,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Heliodora,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ilicinea,2012-12-31T00:00:00,1.69,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ilicinea,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ilicinea,2014-12-31T00:00:00,1.35,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ilicinea,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ilicinea,2016-12-31T00:00:00,2.46,2.0,18.7,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ilicinea,2017-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ilicinea,2018-12-31T00:00:00,2.46,2.0,18.7,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Imbe_de_Minas,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Imbe_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Imbe_de_Minas,2014-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Imbe_de_Minas,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Imbe_de_Minas,2016-12-31T00:00:00,0.9,1.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Imbe_de_Minas,2017-12-31T00:00:00,1.29,1.0,22.48,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Imbe_de_Minas,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inconfidentes,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inconfidentes,2013-12-31T00:00:00,1.4,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inconfidentes,2014-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inconfidentes,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inconfidentes,2016-12-31T00:00:00,2.1,1.0,52.38,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inconfidentes,2017-12-31T00:00:00,1.3,2.0,53.85,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inconfidentes,2018-12-31T00:00:00,2.88,2.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inhapim,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inhapim,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inhapim,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inhapim,2015-12-31T00:00:00,1.21,1.0,17.36,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inhapim,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inhapim,2017-12-31T00:00:00,1.12,1.0,10.71,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inhapim,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Itamogi,2012-12-31T00:00:00,1.95,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Itamogi,2013-12-31T00:00:00,1.17,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Itamogi,2014-12-31T00:00:00,1.6,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Itamogi,2015-12-31T00:00:00,1.58,2.0,26.58,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Itamogi,2016-12-31T00:00:00,1.95,2.0,2.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Itamogi,2017-12-31T00:00:00,1.42,2.0,40.85,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Itamogi,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jacutinga,2012-12-31T00:00:00,1.68,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jacutinga,2013-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jacutinga,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jacutinga,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jacutinga,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jacutinga,2017-12-31T00:00:00,1.37,1.0,27.01,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jacutinga,2018-12-31T00:00:00,1.74,1.0,42.53,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jequeri,2012-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jequeri,2013-12-31T00:00:00,1.86,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jequeri,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jequeri,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jequeri,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jequeri,2017-12-31T00:00:00,1.28,1.0,21.88,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jequeri,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Juruaia,2012-12-31T00:00:00,1.92,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Juruaia,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Juruaia,2014-12-31T00:00:00,1.72,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Juruaia,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Juruaia,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Juruaia,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Juruaia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lambari,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lambari,2013-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lambari,2014-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lambari,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lambari,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lambari,2017-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lambari,2018-12-31T00:00:00,1.56,1.0,35.9,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lavras,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lavras,2013-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lavras,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lavras,2015-12-31T00:00:00,0.84,1.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lavras,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lavras,2017-12-31T00:00:00,2.25,1.0,55.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lavras,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Luisburgo,2012-12-31T00:00:00,1.26,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Luisburgo,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Luisburgo,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Luisburgo,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Luisburgo,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Luisburgo,2017-12-31T00:00:00,1.53,1.0,34.64,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Luisburgo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Machado,2012-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Machado,2013-12-31T00:00:00,1.62,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Machado,2014-12-31T00:00:00,1.15,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Machado,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Machado,2016-12-31T00:00:00,1.86,2.0,7.53,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Machado,2017-12-31T00:00:00,1.51,2.0,32.45,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Machado,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monsenhor_Paulo,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monsenhor_Paulo,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monsenhor_Paulo,2014-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monsenhor_Paulo,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monsenhor_Paulo,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monsenhor_Paulo,2017-12-31T00:00:00,1.72,1.0,41.86,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monsenhor_Paulo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Belo,2012-12-31T00:00:00,1.63,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Belo,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Belo,2014-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Belo,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Belo,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Belo,2017-12-31T00:00:00,2.43,2.0,17.7,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Belo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Santo_de_Minas,2012-12-31T00:00:00,1.89,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Santo_de_Minas,2013-12-31T00:00:00,1.62,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Santo_de_Minas,2014-12-31T00:00:00,1.6,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Santo_de_Minas,2015-12-31T00:00:00,1.35,2.0,48.15,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Santo_de_Minas,2016-12-31T00:00:00,2.04,2.0,1.96,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Santo_de_Minas,2017-12-31T00:00:00,1.08,2.0,85.19,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Santo_de_Minas,2018-12-31T00:00:00,1.88,2.0,6.38,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Siao,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Siao,2013-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Siao,2014-12-31T00:00:00,0.66,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Siao,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Siao,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Siao,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Siao,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Muzambinho,2012-12-31T00:00:00,1.74,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Muzambinho,2013-12-31T00:00:00,1.35,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Muzambinho,2014-12-31T00:00:00,1.35,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Muzambinho,2015-12-31T00:00:00,1.48,1.0,32.43,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Muzambinho,2016-12-31T00:00:00,1.98,1.0,49.49,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Muzambinho,2017-12-31T00:00:00,1.62,2.0,23.46,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Muzambinho,2018-12-31T00:00:00,1.3,2.0,53.85,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Natercia,2012-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Natercia,2013-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Natercia,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Natercia,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Natercia,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Natercia,2017-12-31T00:00:00,1.6,1.0,37.5,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Natercia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nazareno,2012-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nazareno,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nazareno,2014-12-31T00:00:00,1.3,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nazareno,2015-12-31T00:00:00,1.8,1.0,44.44,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nazareno,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nazareno,2017-12-31T00:00:00,1.59,2.0,25.79,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nazareno,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nepomuceno,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nepomuceno,2013-12-31T00:00:00,1.68,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nepomuceno,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nepomuceno,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nepomuceno,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nepomuceno,2017-12-31T00:00:00,1.69,1.0,40.83,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nepomuceno,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Oliveira,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Oliveira,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Oliveira,2014-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Oliveira,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Oliveira,2016-12-31T00:00:00,1.74,1.0,42.53,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Oliveira,2017-12-31T00:00:00,1.4,2.0,42.86,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Oliveira,2018-12-31T00:00:00,2.1,1.0,52.38,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ouro_Fino,2012-12-31T00:00:00,1.26,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ouro_Fino,2013-12-31T00:00:00,1.56,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ouro_Fino,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ouro_Fino,2015-12-31T00:00:00,1.29,1.0,22.48,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ouro_Fino,2016-12-31T00:00:00,1.44,2.0,38.89,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ouro_Fino,2017-12-31T00:00:00,1.42,1.0,29.58,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ouro_Fino,2018-12-31T00:00:00,1.69,1.0,40.83,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Paraguacu,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Paraguacu,2013-12-31T00:00:00,1.14,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Paraguacu,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Paraguacu,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Paraguacu,2016-12-31T00:00:00,1.98,1.0,49.49,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Paraguacu,2017-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Paraguacu,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedra_Bonita,2012-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedra_Bonita,2013-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedra_Bonita,2014-12-31T00:00:00,0.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedra_Bonita,2015-12-31T00:00:00,0.6,1.0,66.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedra_Bonita,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedra_Bonita,2017-12-31T00:00:00,0.86,1.0,16.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedra_Bonita,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedralva,2012-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedralva,2013-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedralva,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedralva,2015-12-31T00:00:00,0.96,2.0,108.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedralva,2016-12-31T00:00:00,2.28,2.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedralva,2017-12-31T00:00:00,1.43,2.0,39.86,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedralva,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Perdoes,2012-12-31T00:00:00,1.68,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Perdoes,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Perdoes,2014-12-31T00:00:00,1.26,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Perdoes,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Perdoes,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Perdoes,2017-12-31T00:00:00,1.84,1.0,45.65,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Perdoes,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Piedade_de_Caratinga,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Piedade_de_Caratinga,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Piedade_de_Caratinga,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Piedade_de_Caratinga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Piedade_de_Caratinga,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Piedade_de_Caratinga,2017-12-31T00:00:00,1.59,1.0,37.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Piedade_de_Caratinga,2018-12-31T00:00:00,1.68,1.0,40.48,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Poco_Fundo,2012-12-31T00:00:00,1.14,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Poco_Fundo,2013-12-31T00:00:00,1.14,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Poco_Fundo,2014-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Poco_Fundo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Poco_Fundo,2016-12-31T00:00:00,1.41,1.0,29.08,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Poco_Fundo,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Poco_Fundo,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pocos_de_Caldas,2012-12-31T00:00:00,1.56,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pocos_de_Caldas,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pocos_de_Caldas,2014-12-31T00:00:00,1.14,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pocos_de_Caldas,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pocos_de_Caldas,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pocos_de_Caldas,2017-12-31T00:00:00,1.77,1.0,43.5,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pocos_de_Caldas,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pratinha,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pratinha,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pratinha,2014-12-31T00:00:00,1.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pratinha,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pratinha,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pratinha,2017-12-31T00:00:00,1.72,2.0,16.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pratinha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Raul_Soares,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Raul_Soares,2013-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Raul_Soares,2014-12-31T00:00:00,0.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Raul_Soares,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Raul_Soares,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Raul_Soares,2017-12-31T00:00:00,0.86,1.0,16.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Raul_Soares,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Itueto,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Itueto,2013-12-31T00:00:00,1.17,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Itueto,2014-12-31T00:00:00,1.11,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Itueto,2015-12-31T00:00:00,0.92,1.0,8.7,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Itueto,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Itueto,2017-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Itueto,2018-12-31T00:00:00,1.35,1.0,25.93,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Sapucai,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Sapucai,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Sapucai,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Sapucai,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Sapucai,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Sapucai,2017-12-31T00:00:00,1.79,1.0,44.13,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Sapucai,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santana_da_Vargem,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santana_da_Vargem,2013-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santana_da_Vargem,2014-12-31T00:00:00,1.1,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santana_da_Vargem,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santana_da_Vargem,2016-12-31T00:00:00,1.74,1.0,42.53,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santana_da_Vargem,2017-12-31T00:00:00,2.09,1.0,52.15,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santana_da_Vargem,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santo_Antonio_do_Amparo,2012-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santo_Antonio_do_Amparo,2013-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santo_Antonio_do_Amparo,2014-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santo_Antonio_do_Amparo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santo_Antonio_do_Amparo,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santo_Antonio_do_Amparo,2017-12-31T00:00:00,1.34,2.0,49.25,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santo_Antonio_do_Amparo,2018-12-31T00:00:00,2.16,1.0,53.7,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Francisco_de_Paula,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Francisco_de_Paula,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Francisco_de_Paula,2014-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Francisco_de_Paula,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Francisco_de_Paula,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Francisco_de_Paula,2017-12-31T00:00:00,1.56,1.0,35.9,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Francisco_de_Paula,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Goncalo_do_Sapucai,2012-12-31T00:00:00,1.26,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Goncalo_do_Sapucai,2013-12-31T00:00:00,1.26,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Goncalo_do_Sapucai,2014-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Goncalo_do_Sapucai,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Goncalo_do_Sapucai,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Goncalo_do_Sapucai,2017-12-31T00:00:00,2.04,1.0,50.98,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Goncalo_do_Sapucai,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Jose_da_Barra,2012-12-31T00:00:00,1.56,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Jose_da_Barra,2013-12-31T00:00:00,1.56,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Jose_da_Barra,2014-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Jose_da_Barra,2015-12-31T00:00:00,1.2,2.0,66.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Jose_da_Barra,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Jose_da_Barra,2017-12-31T00:00:00,1.91,1.0,47.64,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Jose_da_Barra,2018-12-31T00:00:00,2.1,1.0,52.38,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Pedro_da_Uniao,2012-12-31T00:00:00,2.28,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Pedro_da_Uniao,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Pedro_da_Uniao,2014-12-31T00:00:00,1.62,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Pedro_da_Uniao,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Pedro_da_Uniao,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Pedro_da_Uniao,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Pedro_da_Uniao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Roque_de_Minas,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Roque_de_Minas,2013-12-31T00:00:00,1.44,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Roque_de_Minas,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Roque_de_Minas,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Roque_de_Minas,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Roque_de_Minas,2017-12-31T00:00:00,1.46,1.0,31.51,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Roque_de_Minas,2018-12-31T00:00:00,1.62,1.0,38.27,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Anta,2012-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Anta,2013-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Anta,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Anta,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Anta,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Anta,2017-12-31T00:00:00,1.78,2.0,12.36,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Anta,2018-12-31T00:00:00,1.64,2.0,21.95,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Paraiso,2012-12-31T00:00:00,1.62,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Paraiso,2013-12-31T00:00:00,1.71,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Paraiso,2014-12-31T00:00:00,1.42,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Paraiso,2015-12-31T00:00:00,1.38,2.0,44.93,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Paraiso,2016-12-31T00:00:00,1.7,2.0,17.65,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Paraiso,2017-12-31T00:00:00,1.54,2.0,29.87,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Paraiso,2018-12-31T00:00:00,1.89,2.0,5.82,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Tomas_de_Aquino,2012-12-31T00:00:00,1.75,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Tomas_de_Aquino,2013-12-31T00:00:00,1.05,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Tomas_de_Aquino,2014-12-31T00:00:00,1.35,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Tomas_de_Aquino,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Tomas_de_Aquino,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Tomas_de_Aquino,2017-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Tomas_de_Aquino,2018-12-31T00:00:00,2.1,1.0,52.38,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Serrania,2012-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Serrania,2013-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Serrania,2014-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Serrania,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Serrania,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Serrania,2017-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Serrania,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Simonesia,2012-12-31T00:00:00,1.56,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Simonesia,2013-12-31T00:00:00,1.62,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Simonesia,2014-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Simonesia,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Simonesia,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Simonesia,2017-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Simonesia,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Coracoes,2012-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Coracoes,2013-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Coracoes,2014-12-31T00:00:00,0.9,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Coracoes,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Coracoes,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Coracoes,2017-12-31T00:00:00,2.67,1.0,62.55,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Coracoes,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Pontas,2012-12-31T00:00:00,1.38,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Pontas,2013-12-31T00:00:00,1.53,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Pontas,2014-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Pontas,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Pontas,2016-12-31T00:00:00,1.86,2.0,7.53,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Pontas,2017-12-31T00:00:00,1.88,1.0,46.81,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Pontas,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Turvolandia,2012-12-31T00:00:00,1.2,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Turvolandia,2013-12-31T00:00:00,1.08,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Turvolandia,2014-12-31T00:00:00,0.84,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Turvolandia,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Turvolandia,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Turvolandia,2017-12-31T00:00:00,1.8,1.0,44.44,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Turvolandia,2018-12-31T00:00:00,2.29,2.0,12.66,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ubaporanga,2012-12-31T00:00:00,1.02,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ubaporanga,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ubaporanga,2014-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ubaporanga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ubaporanga,2016-12-31T00:00:00,0.96,1.0,4.17,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ubaporanga,2017-12-31T00:00:00,1.03,1.0,2.91,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ubaporanga,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Varginha,2012-12-31T00:00:00,1.32,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Varginha,2013-12-31T00:00:00,1.5,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Varginha,2014-12-31T00:00:00,1.03,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Varginha,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Varginha,2016-12-31T00:00:00,1.74,1.0,42.53,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Varginha,2017-12-31T00:00:00,2.12,1.0,52.83,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Varginha,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Vermelho_Novo,2012-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Vermelho_Novo,2013-12-31T00:00:00,0.96,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Vermelho_Novo,2014-12-31T00:00:00,0.8,-,-,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Vermelho_Novo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Vermelho_Novo,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Vermelho_Novo,2017-12-31T00:00:00,1.08,1.0,7.41,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Vermelho_Novo,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Abre_Campo,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alfenas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alterosa,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Andradas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Angelandia,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Araponga,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Boa_Esperanca,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bom_Sucesso,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bueno_Brandao,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cambuquira,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campanha,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_Belo,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_do_Meio,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campos_Gerais,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Candeias,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capelinha,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capetinga,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capitolio,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Caratinga,2019-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_da_Cachoeira,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_de_Minas,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_do_Rio_Claro,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cassia,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conceicao_do_Rio_Verde,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conselheiro_Pena,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Coqueiral,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cristais,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Divisa_Nova,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Eloi_Mendes,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Fervedouro,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Heliodora,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ilicinea,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Imbe_de_Minas,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inconfidentes,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inhapim,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Itamogi,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jacutinga,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jequeri,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Juruaia,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lambari,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lavras,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Luisburgo,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Machado,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monsenhor_Paulo,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Belo,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Santo_de_Minas,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Siao,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Muzambinho,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Natercia,2019-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nazareno,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nepomuceno,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Oliveira,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ouro_Fino,2019-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Paraguacu,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedra_Bonita,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedralva,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Perdoes,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Piedade_de_Caratinga,2019-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Poco_Fundo,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pocos_de_Caldas,2019-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pratinha,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Raul_Soares,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Itueto,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Sapucai,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santana_da_Vargem,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santo_Antonio_do_Amparo,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Francisco_de_Paula,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Goncalo_do_Sapucai,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Jose_da_Barra,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Pedro_da_Uniao,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Roque_de_Minas,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Anta,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Paraiso,2019-12-31T00:00:00,1.0,2,100.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Tomas_de_Aquino,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Serrania,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Simonesia,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Coracoes,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Pontas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Turvolandia,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ubaporanga,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Varginha,2019-12-31T00:00:00,2.0,2,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Vermelho_Novo,2019-12-31T00:00:00,1.0,1,0.0,validacao,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Abre_Campo,2020-12-31T00:00:00,1.5,1.074647068977356,-28.36,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alfenas,2020-12-31T00:00:00,2.364130434782609,2.043916702270508,-13.54,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alterosa,2020-12-31T00:00:00,1.980023501762632,1.826667308807373,-7.75,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Andradas,2020-12-31T00:00:00,2.568,1.5134079456329346,-41.07,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Angelandia,2020-12-31T00:00:00,1.8400349650349648,1.4967992305755615,-18.65,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Araponga,2020-12-31T00:00:00,1.8000000000000005,1.4597954750061035,-18.9,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Boa_Esperanca,2020-12-31T00:00:00,2.063248407643312,2.009059190750122,-2.63,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bom_Sucesso,2020-12-31T00:00:00,2.1,1.6034595966339111,-23.64,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bueno_Brandao,2020-12-31T00:00:00,1.8000000000000005,1.4485766887664795,-19.52,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cambuquira,2020-12-31T00:00:00,1.800145348837209,1.728628158569336,-3.97,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campanha,2020-12-31T00:00:00,1.6833930704898452,1.7539215087890625,4.19,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_Belo,2020-12-31T00:00:00,1.8000000000000005,1.288915991783142,-28.39,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_do_Meio,2020-12-31T00:00:00,2.4,1.70326566696167,-29.03,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campos_Gerais,2020-12-31T00:00:00,2.690649114843395,1.6702094078063965,-37.93,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Candeias,2020-12-31T00:00:00,1.859937013094646,1.7639094591140747,-5.16,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capelinha,2020-12-31T00:00:00,1.434123222748815,1.2226946353912354,-14.74,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capetinga,2020-12-31T00:00:00,2.213808463251671,2.5708022117614746,16.13,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capitolio,2020-12-31T00:00:00,2.1,1.2970441579818726,-38.24,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Caratinga,2020-12-31T00:00:00,2.040022111663903,1.7068531513214111,-16.33,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_da_Cachoeira,2020-12-31T00:00:00,1.8000000000000005,1.7272167205810547,-4.04,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_de_Minas,2020-12-31T00:00:00,1.7400000000000002,1.4737586975097656,-15.3,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_do_Rio_Claro,2020-12-31T00:00:00,2.7847803881511743,1.7713502645492554,-36.39,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cassia,2020-12-31T00:00:00,2.1679245283018864,1.8689424991607666,-13.79,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conceicao_do_Rio_Verde,2020-12-31T00:00:00,1.8000000000000005,1.9171252250671387,6.51,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conselheiro_Pena,2020-12-31T00:00:00,1.55981308411215,1.5452872514724731,-0.93,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Coqueiral,2020-12-31T00:00:00,2.160028248587571,1.578192949295044,-26.94,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cristais,2020-12-31T00:00:00,1.92,1.8103632926940918,-5.71,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Divisa_Nova,2020-12-31T00:00:00,1.8000000000000005,1.8507741689682007,2.82,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Eloi_Mendes,2020-12-31T00:00:00,2.109128416709644,1.6805758476257324,-20.32,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Fervedouro,2020-12-31T00:00:00,1.5,1.2205369472503662,-18.63,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Heliodora,2020-12-31T00:00:00,1.8000000000000005,1.6536986827850342,-8.13,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ilicinea,2020-12-31T00:00:00,2.09995817649519,1.815926432609558,-13.53,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Imbe_de_Minas,2020-12-31T00:00:00,1.439877300613497,1.3563770055770874,-5.8,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inconfidentes,2020-12-31T00:00:00,2.46,1.4827359914779663,-39.73,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inhapim,2020-12-31T00:00:00,1.8000000000000005,1.4624496698379517,-18.75,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Itamogi,2020-12-31T00:00:00,2.400049176297025,1.6555683612823486,-31.02,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jacutinga,2020-12-31T00:00:00,1.8000000000000005,1.508811593055725,-16.18,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jequeri,2020-12-31T00:00:00,2.1,1.4416868686676025,-31.35,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Juruaia,2020-12-31T00:00:00,1.92,1.7120535373687744,-10.83,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lambari,2020-12-31T00:00:00,1.8000970402717127,1.526033639907837,-15.22,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lavras,2020-12-31T00:00:00,1.8000000000000005,1.8421692848205566,2.34,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Luisburgo,2020-12-31T00:00:00,2.1,1.7197437286376953,-18.11,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Machado,2020-12-31T00:00:00,2.211839166046165,1.7555032968521118,-20.63,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monsenhor_Paulo,2020-12-31T00:00:00,2.1,1.6869919300079346,-19.67,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Belo,2020-12-31T00:00:00,1.8000000000000005,1.8390400409698486,2.17,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Santo_de_Minas,2020-12-31T00:00:00,2.123987903619159,1.446659803390503,-31.89,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Siao,2020-12-31T00:00:00,1.8000000000000005,1.4542863368988037,-19.21,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Muzambinho,2020-12-31T00:00:00,1.764006791171477,1.4590528011322021,-17.29,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Natercia,2020-12-31T00:00:00,1.4399193548387097,1.5745292901992798,9.35,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nazareno,2020-12-31T00:00:00,2.1,1.7321795225143433,-17.52,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nepomuceno,2020-12-31T00:00:00,1.920040743570155,1.6142607927322388,-15.93,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Oliveira,2020-12-31T00:00:00,1.947151898734177,1.8871017694473267,-3.08,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ouro_Fino,2020-12-31T00:00:00,2.099920063948841,1.4563605785369873,-30.65,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Paraguacu,2020-12-31T00:00:00,2.3413259668508286,1.7404437065124512,-25.66,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedra_Bonita,2020-12-31T00:00:00,1.7400000000000002,1.1270025968551636,-35.23,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedralva,2020-12-31T00:00:00,1.68,1.5643709897994995,-6.88,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Perdoes,2020-12-31T00:00:00,1.92,1.8107861280441284,-5.69,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Piedade_de_Caratinga,2020-12-31T00:00:00,1.8000000000000005,1.6132256984710693,-10.38,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Poco_Fundo,2020-12-31T00:00:00,1.740033329060377,1.3723931312561035,-21.13,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pocos_de_Caldas,2020-12-31T00:00:00,1.8000000000000005,1.3466811180114746,-25.18,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pratinha,2020-12-31T00:00:00,2.1,1.7887365818023682,-14.82,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Raul_Soares,2020-12-31T00:00:00,1.320083682008368,1.137695550918579,-13.82,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Itueto,2020-12-31T00:00:00,1.77,1.5606296062469482,-11.83,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Sapucai,2020-12-31T00:00:00,1.68,1.6863387823104858,0.38,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santana_da_Vargem,2020-12-31T00:00:00,2.22,1.9523897171020508,-12.05,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santo_Antonio_do_Amparo,2020-12-31T00:00:00,2.1,1.872200608253479,-10.85,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Francisco_de_Paula,2020-12-31T00:00:00,1.62,1.733171820640564,6.99,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Goncalo_do_Sapucai,2020-12-31T00:00:00,2.4,1.8083076477050781,-24.65,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Jose_da_Barra,2020-12-31T00:00:00,2.537024221453287,1.920257329940796,-24.31,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Pedro_da_Uniao,2020-12-31T00:00:00,2.1,1.6948237419128418,-19.29,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Roque_de_Minas,2020-12-31T00:00:00,1.8000000000000005,1.5583573579788208,-13.42,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Anta,2020-12-31T00:00:00,1.8000000000000005,1.7170488834381104,-4.61,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Paraiso,2020-12-31T00:00:00,1.929071661237785,1.6152658462524414,-16.27,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Tomas_de_Aquino,2020-12-31T00:00:00,2.099966151415999,1.490379810333252,-29.03,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Serrania,2020-12-31T00:00:00,1.8000000000000005,1.6352612972259521,-9.15,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Simonesia,2020-12-31T00:00:00,1.8000000000000005,1.218968391418457,-32.28,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Coracoes,2020-12-31T00:00:00,2.1,2.0646440982818604,-1.68,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Pontas,2020-12-31T00:00:00,2.26,1.7593159675598145,-22.15,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Turvolandia,2020-12-31T00:00:00,1.6202531645569618,2.0488085746765137,26.45,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ubaporanga,2020-12-31T00:00:00,1.6200873362445412,1.4427025318145752,-10.95,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Varginha,2020-12-31T00:00:00,1.899977968715576,1.9293633699417114,1.55,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Vermelho_Novo,2020-12-31T00:00:00,1.320080321285141,1.2242134809494019,-7.26,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Abre_Campo,2021-12-31T00:00:00,1.2,0.9556726813316345,-20.36,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alfenas,2021-12-31T00:00:00,1.6209774981853378,2.1278393268585205,31.27,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alterosa,2021-12-31T00:00:00,1.08,1.9121367931365967,77.05,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Andradas,2021-12-31T00:00:00,1.26,1.9621325731277466,55.72,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Angelandia,2021-12-31T00:00:00,1.5133433283358322,1.532315731048584,1.25,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Araponga,2021-12-31T00:00:00,1.2,1.3072338104248047,8.94,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Boa_Esperanca,2021-12-31T00:00:00,1.570339108544351,1.9268728494644165,22.7,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bom_Sucesso,2021-12-31T00:00:00,1.37989417989418,1.7602617740631104,27.56,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bueno_Brandao,2021-12-31T00:00:00,1.2,1.7409579753875732,45.08,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cambuquira,2021-12-31T00:00:00,1.2,1.7292826175689697,44.11,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campanha,2021-12-31T00:00:00,1.325757575757576,1.6433354616165161,23.95,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_Belo,2021-12-31T00:00:00,1.5,1.4132424592971802,-5.78,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_do_Meio,2021-12-31T00:00:00,1.255813953488372,1.941339373588562,54.59,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campos_Gerais,2021-12-31T00:00:00,1.589070422535211,2.1147050857543945,33.08,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Candeias,2021-12-31T00:00:00,1.199963309484498,1.720666527748108,43.39,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capelinha,2021-12-31T00:00:00,1.4350318471337582,1.268259048461914,-11.62,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capetinga,2021-12-31T00:00:00,1.6399999999999997,2.378493070602417,45.03,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capitolio,2021-12-31T00:00:00,1.2,1.8033727407455444,50.28,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Caratinga,2021-12-31T00:00:00,1.26,1.1468948125839233,-8.98,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_da_Cachoeira,2021-12-31T00:00:00,1.439980158730159,1.8022651672363281,25.16,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_de_Minas,2021-12-31T00:00:00,1.3799999999999997,1.6021356582641602,16.1,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_do_Rio_Claro,2021-12-31T00:00:00,1.643702081051479,2.327072858810425,41.58,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cassia,2021-12-31T00:00:00,1.314977578475336,1.9688758850097656,49.73,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conceicao_do_Rio_Verde,2021-12-31T00:00:00,1.500115340253749,1.8633415699005127,24.21,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conselheiro_Pena,2021-12-31T00:00:00,1.2898734177215188,1.4625619649887085,13.39,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Coqueiral,2021-12-31T00:00:00,1.5,1.8006473779678345,20.04,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cristais,2021-12-31T00:00:00,1.318776371308017,1.8350367546081543,39.15,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Divisa_Nova,2021-12-31T00:00:00,1.2601319509896318,1.7998161315917969,42.83,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Eloi_Mendes,2021-12-31T00:00:00,1.037885462555066,1.8173158168792725,75.1,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Fervedouro,2021-12-31T00:00:00,1.15006090133983,0.8786131143569946,-23.6,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Heliodora,2021-12-31T00:00:00,1.2,1.6956787109375,41.31,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ilicinea,2021-12-31T00:00:00,1.560035211267606,2.144329071044922,37.45,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Imbe_de_Minas,2021-12-31T00:00:00,1.2,1.1682095527648926,-2.65,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inconfidentes,2021-12-31T00:00:00,1.5,2.3247621059417725,54.98,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inhapim,2021-12-31T00:00:00,1.2,1.4338842630386353,19.49,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Itamogi,2021-12-31T00:00:00,1.260064724919094,2.1795012950897217,72.97,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jacutinga,2021-12-31T00:00:00,1.32014652014652,1.750441074371338,32.59,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jequeri,2021-12-31T00:00:00,1.5,1.4912782907485962,-0.58,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Juruaia,2021-12-31T00:00:00,1.5,1.8346396684646606,22.31,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lambari,2021-12-31T00:00:00,1.199954863461973,1.6322100162506104,36.02,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lavras,2021-12-31T00:00:00,1.259900454447089,1.7647504806518555,40.07,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Luisburgo,2021-12-31T00:00:00,1.3799999999999997,1.7696824073791504,28.24,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Machado,2021-12-31T00:00:00,1.444286871961102,1.8457388877868652,27.8,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monsenhor_Paulo,2021-12-31T00:00:00,1.41,1.8423383235931396,30.66,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Belo,2021-12-31T00:00:00,1.56,1.7580153942108154,12.69,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Santo_de_Minas,2021-12-31T00:00:00,1.200018932222643,1.928772211074829,60.73,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Siao,2021-12-31T00:00:00,1.5,1.730116367340088,15.34,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Muzambinho,2021-12-31T00:00:00,1.199937762564182,1.7928558588027954,49.41,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Natercia,2021-12-31T00:00:00,1.4399193548387097,1.5031588077545166,4.39,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nazareno,2021-12-31T00:00:00,1.5,1.9068267345428467,27.12,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nepomuceno,2021-12-31T00:00:00,1.379962721342032,1.6851478815078735,22.12,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Oliveira,2021-12-31T00:00:00,1.537805840568272,1.8180736303329468,18.23,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ouro_Fino,2021-12-31T00:00:00,1.2,1.7266160249710083,43.88,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Paraguacu,2021-12-31T00:00:00,0.9712280701754386,1.8767471313476562,93.23,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedra_Bonita,2021-12-31T00:00:00,1.08,1.3081103563308716,21.12,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedralva,2021-12-31T00:00:00,1.5,1.645120620727539,9.67,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Perdoes,2021-12-31T00:00:00,1.44017094017094,1.8251721858978271,26.73,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Piedade_de_Caratinga,2021-12-31T00:00:00,1.2,1.5257723331451416,27.15,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Poco_Fundo,2021-12-31T00:00:00,0.9601760412194076,1.5861926078796387,65.2,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pocos_de_Caldas,2021-12-31T00:00:00,1.4199999999999997,1.5535469055175781,9.4,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pratinha,2021-12-31T00:00:00,1.35,1.9167169332504272,41.98,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Raul_Soares,2021-12-31T00:00:00,1.320079522862823,1.137162685394287,-13.86,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Itueto,2021-12-31T00:00:00,1.3799999999999997,1.5849688053131104,14.85,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Sapucai,2021-12-31T00:00:00,1.5,1.635589599609375,9.04,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santana_da_Vargem,2021-12-31T00:00:00,1.44,1.9463900327682495,35.17,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santo_Antonio_do_Amparo,2021-12-31T00:00:00,1.3799999999999997,1.9802066087722778,43.49,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Francisco_de_Paula,2021-12-31T00:00:00,1.3200867052023115,1.678174614906311,27.13,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Goncalo_do_Sapucai,2021-12-31T00:00:00,1.62,2.054964303970337,26.85,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Jose_da_Barra,2021-12-31T00:00:00,1.682068965517241,2.1637094020843506,28.63,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Pedro_da_Uniao,2021-12-31T00:00:00,1.479945054945055,1.8467384576797485,24.78,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Roque_de_Minas,2021-12-31T00:00:00,1.2,1.6558024883270264,37.98,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Anta,2021-12-31T00:00:00,1.5,1.6235896348953247,8.24,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Paraiso,2021-12-31T00:00:00,1.14450771643146,1.7626926898956299,54.01,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Tomas_de_Aquino,2021-12-31T00:00:00,1.2,1.9423543214797974,61.86,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Serrania,2021-12-31T00:00:00,1.08,1.7528636455535889,62.3,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Simonesia,2021-12-31T00:00:00,1.08,0.5531590580940247,-48.78,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Coracoes,2021-12-31T00:00:00,1.5,1.9306906461715698,28.71,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Pontas,2021-12-31T00:00:00,1.3500296384113808,2.0783443450927734,53.95,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Turvolandia,2021-12-31T00:00:00,1.3797385620915028,1.844864845275879,33.71,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ubaporanga,2021-12-31T00:00:00,1.319867549668874,1.3532295227050781,2.53,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Varginha,2021-12-31T00:00:00,1.2,1.8008266687393188,50.07,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Vermelho_Novo,2021-12-31T00:00:00,0.96,1.014701008796692,5.7,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Abre_Campo,2022-12-31T00:00:00,1.2,1.3920981884002686,16.01,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alfenas,2022-12-31T00:00:00,0.9243951612903224,1.92238187789917,107.96,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alterosa,2022-12-31T00:00:00,1.080104712041885,1.6347532272338867,51.35,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Andradas,2022-12-31T00:00:00,1.26,2.0871851444244385,65.65,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Angelandia,2022-12-31T00:00:00,1.9799126637554585,1.5306406021118164,-22.69,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Araponga,2022-12-31T00:00:00,1.260041407867495,1.3734710216522217,9.0,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Boa_Esperanca,2022-12-31T00:00:00,0.9726256983240223,1.8833489418029785,93.64,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bom_Sucesso,2022-12-31T00:00:00,1.440136054421769,1.6750646829605103,16.31,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bueno_Brandao,2022-12-31T00:00:00,1.440196078431373,1.6124725341796875,11.96,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cambuquira,2022-12-31T00:00:00,1.32,1.63187837600708,23.63,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campanha,2022-12-31T00:00:00,1.266469038208169,1.5349351167678833,21.2,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_Belo,2022-12-31T00:00:00,1.2,1.5530251264572144,29.42,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_do_Meio,2022-12-31T00:00:00,1.580968858131488,1.8387902975082397,16.31,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campos_Gerais,2022-12-31T00:00:00,1.535771358328211,2.017899513244629,31.39,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Candeias,2022-12-31T00:00:00,1.020017406440383,1.5680912733078003,53.73,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capelinha,2022-12-31T00:00:00,1.576751592356688,1.320522665977478,-16.25,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capetinga,2022-12-31T00:00:00,1.407056229327453,1.964942216873169,39.65,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capitolio,2022-12-31T00:00:00,1.08,1.5050888061523438,39.36,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Caratinga,2022-12-31T00:00:00,1.620022123893805,1.3706685304641724,-15.39,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_da_Cachoeira,2022-12-31T00:00:00,1.2,1.6440653800964355,37.01,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_de_Minas,2022-12-31T00:00:00,1.14,1.5834681987762451,38.9,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_do_Rio_Claro,2022-12-31T00:00:00,1.619555143651529,2.070768356323242,27.86,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cassia,2022-12-31T00:00:00,1.413626373626374,1.666213035583496,17.87,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conceicao_do_Rio_Verde,2022-12-31T00:00:00,1.5,1.7899253368377686,19.33,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conselheiro_Pena,2022-12-31T00:00:00,1.8000000000000005,1.3957611322402954,-22.46,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Coqueiral,2022-12-31T00:00:00,1.2,1.729732632637024,44.14,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cristais,2022-12-31T00:00:00,1.059063136456212,1.6363670825958252,54.51,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Divisa_Nova,2022-12-31T00:00:00,1.260204081632653,1.6481266021728516,30.78,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Eloi_Mendes,2022-12-31T00:00:00,1.090201870999508,1.5622739791870117,43.3,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Fervedouro,2022-12-31T00:00:00,1.4399108138238568,1.2219836711883545,-15.13,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Heliodora,2022-12-31T00:00:00,1.080140597539543,1.538680076599121,42.45,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ilicinea,2022-12-31T00:00:00,1.080069324090121,1.8624763488769531,72.44,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Imbe_de_Minas,2022-12-31T00:00:00,1.3799999999999997,1.139768123626709,-17.41,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inconfidentes,2022-12-31T00:00:00,1.739917695473251,1.9383184909820557,11.4,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inhapim,2022-12-31T00:00:00,1.8000000000000005,1.404727578163147,-21.96,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Itamogi,2022-12-31T00:00:00,1.32,1.7617738246917725,33.47,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jacutinga,2022-12-31T00:00:00,1.289920424403183,1.6367576122283936,26.89,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jequeri,2022-12-31T00:00:00,1.8000000000000005,1.694973349571228,-5.83,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Juruaia,2022-12-31T00:00:00,1.5,1.7070324420928955,13.8,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lambari,2022-12-31T00:00:00,1.2,1.6067514419555664,33.9,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lavras,2022-12-31T00:00:00,1.260078277886497,1.510382890701294,19.86,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Luisburgo,2022-12-31T00:00:00,1.5,1.6779266595840454,11.86,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Machado,2022-12-31T00:00:00,0.964047619047619,1.7416493892669678,80.66,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monsenhor_Paulo,2022-12-31T00:00:00,1.23,1.6964560747146606,37.92,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Belo,2022-12-31T00:00:00,0.96,1.6764674186706543,74.63,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Santo_de_Minas,2022-12-31T00:00:00,1.4699677072120565,1.6306273937225342,10.93,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Siao,2022-12-31T00:00:00,1.2,1.7283822298049927,44.03,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Muzambinho,2022-12-31T00:00:00,1.43993993993994,1.7172331809997559,19.26,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Natercia,2022-12-31T00:00:00,1.4399193548387097,1.8174484968185425,26.22,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nazareno,2022-12-31T00:00:00,1.319811320754717,1.7649234533309937,33.73,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nepomuceno,2022-12-31T00:00:00,0.8699884125144843,1.637281060218811,88.2,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Oliveira,2022-12-31T00:00:00,1.535560504825538,1.7040443420410156,10.97,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ouro_Fino,2022-12-31T00:00:00,1.2,1.8095552921295166,50.8,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Paraguacu,2022-12-31T00:00:00,1.043954802259887,1.635075330734253,56.62,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedra_Bonita,2022-12-31T00:00:00,1.8000000000000005,1.3273098468780518,-26.26,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedralva,2022-12-31T00:00:00,1.320091324200913,1.5635621547698975,18.44,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Perdoes,2022-12-31T00:00:00,1.259859154929577,1.7168928384780884,36.28,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Piedade_de_Caratinga,2022-12-31T00:00:00,1.56,1.4261882305145264,-8.58,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Poco_Fundo,2022-12-31T00:00:00,1.079978925184405,1.309070110321045,21.21,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pocos_de_Caldas,2022-12-31T00:00:00,1.169867549668874,1.4336433410644531,22.55,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pratinha,2022-12-31T00:00:00,1.720111731843575,1.6530852317810059,-3.9,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Raul_Soares,2022-12-31T00:00:00,0.8399548532731377,1.284598469734192,52.94,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Itueto,2022-12-31T00:00:00,1.6399999999999997,1.6334388256072998,-0.4,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Sapucai,2022-12-31T00:00:00,1.14,1.5680941343307495,37.55,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santana_da_Vargem,2022-12-31T00:00:00,0.9900921658986176,1.8724286556243896,89.12,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santo_Antonio_do_Amparo,2022-12-31T00:00:00,1.26,1.7457528114318848,38.55,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Francisco_de_Paula,2022-12-31T00:00:00,1.62,1.5654170513153076,-3.37,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Goncalo_do_Sapucai,2022-12-31T00:00:00,1.08,1.8618847131729126,72.4,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Jose_da_Barra,2022-12-31T00:00:00,2.3076923076923066,1.9209097623825073,-16.76,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Pedro_da_Uniao,2022-12-31T00:00:00,1.4700000000000002,1.7287569046020508,17.6,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Roque_de_Minas,2022-12-31T00:00:00,1.32,1.4477121829986572,9.68,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Anta,2022-12-31T00:00:00,1.8000000000000005,1.5765162706375122,-12.42,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Paraiso,2022-12-31T00:00:00,1.3172147001934242,1.49100923538208,13.19,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Tomas_de_Aquino,2022-12-31T00:00:00,1.2,1.57393217086792,31.16,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Serrania,2022-12-31T00:00:00,1.2,1.517634630203247,26.47,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Simonesia,2022-12-31T00:00:00,1.8000000000000005,1.3511972427368164,-24.93,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Coracoes,2022-12-31T00:00:00,1.08,1.7061991691589355,57.98,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Pontas,2022-12-31T00:00:00,0.975023651844844,1.839866280555725,88.7,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Turvolandia,2022-12-31T00:00:00,1.4398692810457523,1.7149379253387451,19.1,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ubaporanga,2022-12-31T00:00:00,1.2,1.406795620918274,17.23,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Varginha,2022-12-31T00:00:00,1.020039292730845,1.5925133228302002,56.12,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Vermelho_Novo,2022-12-31T00:00:00,0.9598214285714286,1.1351346969604492,18.27,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Abre_Campo,2023-12-31T00:00:00,1.8000000000000005,1.52830970287323,-15.09,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alfenas,2023-12-31T00:00:00,1.698430922311519,1.848724365234375,8.85,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Alterosa,2023-12-31T00:00:00,1.2603960396039595,1.5549798011779785,23.37,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Andradas,2023-12-31T00:00:00,1.22,1.7867116928100586,46.45,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Angelandia,2023-12-31T00:00:00,2.095663265306122,1.8798396587371826,-10.3,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Araponga,2023-12-31T00:00:00,1.5,1.6461737155914307,9.74,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Boa_Esperanca,2023-12-31T00:00:00,1.08,1.7547285556793213,62.47,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bom_Sucesso,2023-12-31T00:00:00,1.5,1.879660725593567,25.31,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Bueno_Brandao,2023-12-31T00:00:00,1.379901960784314,1.5187979936599731,10.07,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cambuquira,2023-12-31T00:00:00,1.56,1.6222063302993774,3.99,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campanha,2023-12-31T00:00:00,1.327075098814229,1.5440802574157715,16.35,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_Belo,2023-12-31T00:00:00,1.5,1.6458944082260132,9.73,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campo_do_Meio,2023-12-31T00:00:00,0.8771626297577856,1.9259636402130127,119.57,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Campos_Gerais,2023-12-31T00:00:00,1.560242401107029,2.1606340408325195,38.48,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Candeias,2023-12-31T00:00:00,1.080032206119163,1.5330876111984253,41.95,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capelinha,2023-12-31T00:00:00,1.541750580945004,1.5182639360427856,-1.52,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capetinga,2023-12-31T00:00:00,1.4484046164290565,1.9380725622177124,33.81,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Capitolio,2023-12-31T00:00:00,1.32,1.5488733053207397,17.34,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Caratinga,2023-12-31T00:00:00,1.3799999999999997,2.0935282707214355,51.7,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_da_Cachoeira,2023-12-31T00:00:00,1.32,1.6096704006195068,21.94,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_de_Minas,2023-12-31T00:00:00,1.5,1.6031105518341064,6.87,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Carmo_do_Rio_Claro,2023-12-31T00:00:00,1.816856256463288,2.225369930267334,22.48,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cassia,2023-12-31T00:00:00,1.6377649325626198,1.8028056621551514,10.08,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conceicao_do_Rio_Verde,2023-12-31T00:00:00,1.44,1.7038757801055908,18.32,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Conselheiro_Pena,2023-12-31T00:00:00,1.320048602673147,1.67861008644104,27.16,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Coqueiral,2023-12-31T00:00:00,1.32,1.81044340133667,37.15,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Cristais,2023-12-31T00:00:00,1.5631111111111111,1.6111798286437988,3.08,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Divisa_Nova,2023-12-31T00:00:00,1.680092592592593,1.5379397869110107,-8.46,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Eloi_Mendes,2023-12-31T00:00:00,1.577412806790921,1.6097633838653564,2.05,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Fervedouro,2023-12-31T00:00:00,1.2,1.5258578062057495,27.15,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Heliodora,2023-12-31T00:00:00,1.439872408293461,1.5691335201263428,8.98,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ilicinea,2023-12-31T00:00:00,1.560032362459547,1.7937521934509277,14.98,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Imbe_de_Minas,2023-12-31T00:00:00,1.3799999999999997,1.4211513996124268,2.98,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inconfidentes,2023-12-31T00:00:00,1.7401360544217692,1.9683808088302612,13.12,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Inhapim,2023-12-31T00:00:00,2.7000000000000006,1.8058738708496094,-33.12,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Itamogi,2023-12-31T00:00:00,1.740011254924029,1.8773736953735352,7.89,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jacutinga,2023-12-31T00:00:00,1.160053262316911,1.5430173873901367,33.01,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Jequeri,2023-12-31T00:00:00,1.5,2.0116734504699707,34.11,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Juruaia,2023-12-31T00:00:00,1.5,1.73712158203125,15.81,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lambari,2023-12-31T00:00:00,1.56,1.5857551097869873,1.65,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Lavras,2023-12-31T00:00:00,1.5,1.6436220407485962,9.57,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Luisburgo,2023-12-31T00:00:00,1.44,2.0501179695129395,42.37,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Machado,2023-12-31T00:00:00,1.500560931145702,1.7422211170196533,16.1,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monsenhor_Paulo,2023-12-31T00:00:00,1.389931972789116,1.7519288063049316,26.04,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Belo,2023-12-31T00:00:00,1.3799999999999997,1.5413347482681274,11.69,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Santo_de_Minas,2023-12-31T00:00:00,1.620050377833753,1.7420601844787598,7.53,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Monte_Siao,2023-12-31T00:00:00,1.5,1.553093433380127,3.54,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Muzambinho,2023-12-31T00:00:00,1.68,1.5680286884307861,-6.66,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Natercia,2023-12-31T00:00:00,1.5,2.471916913986206,64.79,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nazareno,2023-12-31T00:00:00,1.5599056603773582,1.8241010904312134,16.94,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Nepomuceno,2023-12-31T00:00:00,1.650045578851413,1.6957077980041504,2.77,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Oliveira,2023-12-31T00:00:00,1.8240227434257288,1.8314356803894043,0.41,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ouro_Fino,2023-12-31T00:00:00,1.68,1.5243083238601685,-9.27,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Paraguacu,2023-12-31T00:00:00,1.298291457286432,1.663081169128418,28.1,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedra_Bonita,2023-12-31T00:00:00,1.08,2.0714755058288574,91.8,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pedralva,2023-12-31T00:00:00,1.67998417721519,1.5725221633911133,-6.4,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Perdoes,2023-12-31T00:00:00,1.6799401197604789,1.6958229541778564,0.95,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Piedade_de_Caratinga,2023-12-31T00:00:00,2.4,1.7354674339294434,-27.69,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Poco_Fundo,2023-12-31T00:00:00,1.380160799652325,1.3864281177520752,0.45,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pocos_de_Caldas,2023-12-31T00:00:00,1.5,1.518718957901001,1.25,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Pratinha,2023-12-31T00:00:00,2.13,1.870138168334961,-12.2,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Raul_Soares,2023-12-31T00:00:00,1.5,1.422269582748413,-5.18,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Itueto,2023-12-31T00:00:00,1.2,1.7291580438613892,44.1,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santa_Rita_do_Sapucai,2023-12-31T00:00:00,1.3799999999999997,1.561627745628357,13.16,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santana_da_Vargem,2023-12-31T00:00:00,1.5593593593593589,1.7488585710525513,12.15,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Santo_Antonio_do_Amparo,2023-12-31T00:00:00,1.8000000000000005,1.7809545993804932,-1.06,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Francisco_de_Paula,2023-12-31T00:00:00,1.56,1.6061275005340576,2.96,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Goncalo_do_Sapucai,2023-12-31T00:00:00,1.2,2.0390970706939697,69.92,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Jose_da_Barra,2023-12-31T00:00:00,2.2413698630136984,2.3434596061706543,4.55,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Pedro_da_Uniao,2023-12-31T00:00:00,1.5,1.7897992134094238,19.32,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Roque_de_Minas,2023-12-31T00:00:00,1.8000000000000005,1.544851541519165,-14.17,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Anta,2023-12-31T00:00:00,1.7998212689901698,1.8041326999664307,0.24,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Sebastiao_do_Paraiso,2023-12-31T00:00:00,1.440017746228926,1.599003791809082,11.04,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Sao_Tomas_de_Aquino,2023-12-31T00:00:00,1.319968346082828,1.652794599533081,25.21,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Serrania,2023-12-31T00:00:00,0.9,1.474008560180664,63.78,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Simonesia,2023-12-31T00:00:00,1.5,2.0025110244750977,33.5,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Coracoes,2023-12-31T00:00:00,1.44,1.8793792724609375,30.51,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Tres_Pontas,2023-12-31T00:00:00,1.5900000000000003,1.7383620738983154,9.33,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Turvolandia,2023-12-31T00:00:00,1.259748427672956,1.520315408706665,20.68,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Ubaporanga,2023-12-31T00:00:00,1.5,1.5239282846450806,1.6,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Varginha,2023-12-31T00:00:00,1.679990280646337,1.562880277633667,-6.97,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 1 - Metodo 1 (2020-2023),Vermelho_Novo,2023-12-31T00:00:00,1.5,1.3180391788482666,-12.13,teste,V23 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V23, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.00013996533767455206
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 64
batch_size: 32
dropout: 0.2
weight_decay: 0.01
steps: 100
",2025-08-27T13:51:07
IBGE Cluster 2 - Metodo 1 (2020-2023),Araguari,2012-12-31T00:00:00,2.4,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araguari,2013-12-31T00:00:00,1.92,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araguari,2014-12-31T00:00:00,2.4,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araguari,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araguari,2016-12-31T00:00:00,2.58,2.0,22.48,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araguari,2017-12-31T00:00:00,2.76,2.0,27.54,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araguari,2018-12-31T00:00:00,2.64,3.0,13.64,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araxa,2012-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araxa,2013-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araxa,2014-12-31T00:00:00,1.8,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araxa,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araxa,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araxa,2017-12-31T00:00:00,1.76,2.0,13.64,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araxa,2018-12-31T00:00:00,1.94,2.0,3.09,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Bambui,2012-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Bambui,2013-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Bambui,2014-12-31T00:00:00,1.38,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Bambui,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Bambui,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Bambui,2017-12-31T00:00:00,1.5,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Bambui,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Buritizeiro,2012-12-31T00:00:00,3.6,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Buritizeiro,2013-12-31T00:00:00,3.6,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Buritizeiro,2014-12-31T00:00:00,3.6,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Buritizeiro,2015-12-31T00:00:00,1.92,0.0,100.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Buritizeiro,2016-12-31T00:00:00,2.1,4.0,90.48,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Buritizeiro,2017-12-31T00:00:00,1.97,2.0,1.52,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Buritizeiro,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Campos_Altos,2012-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Campos_Altos,2013-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Campos_Altos,2014-12-31T00:00:00,1.62,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Campos_Altos,2015-12-31T00:00:00,1.71,2.0,16.96,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Campos_Altos,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Campos_Altos,2017-12-31T00:00:00,1.99,2.0,0.5,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Campos_Altos,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Carmo_do_Paranaiba,2012-12-31T00:00:00,2.11,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Carmo_do_Paranaiba,2013-12-31T00:00:00,1.92,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Carmo_do_Paranaiba,2014-12-31T00:00:00,1.93,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Carmo_do_Paranaiba,2015-12-31T00:00:00,1.89,2.0,5.82,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Carmo_do_Paranaiba,2016-12-31T00:00:00,2.37,2.0,15.61,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Carmo_do_Paranaiba,2017-12-31T00:00:00,1.75,2.0,14.29,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Carmo_do_Paranaiba,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Claraval,2012-12-31T00:00:00,2.55,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Claraval,2013-12-31T00:00:00,1.42,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Claraval,2014-12-31T00:00:00,2.5,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Claraval,2015-12-31T00:00:00,1.02,1.0,1.96,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Claraval,2016-12-31T00:00:00,2.34,2.0,14.53,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Claraval,2017-12-31T00:00:00,1.22,2.0,63.93,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Claraval,2018-12-31T00:00:00,2.55,2.0,21.57,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Coromandel,2012-12-31T00:00:00,1.96,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Coromandel,2013-12-31T00:00:00,2.02,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Coromandel,2014-12-31T00:00:00,1.91,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Coromandel,2015-12-31T00:00:00,1.91,2.0,4.71,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Coromandel,2016-12-31T00:00:00,2.09,2.0,4.31,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Coromandel,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Coromandel,2018-12-31T00:00:00,2.65,2.0,24.53,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Estrela_do_Sul,2012-12-31T00:00:00,2.22,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Estrela_do_Sul,2013-12-31T00:00:00,1.92,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Estrela_do_Sul,2014-12-31T00:00:00,1.98,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Estrela_do_Sul,2015-12-31T00:00:00,1.26,2.0,58.73,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Estrela_do_Sul,2016-12-31T00:00:00,2.52,2.0,20.63,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Estrela_do_Sul,2017-12-31T00:00:00,2.38,2.0,15.97,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Estrela_do_Sul,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Formiga,2012-12-31T00:00:00,1.2,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Formiga,2013-12-31T00:00:00,1.2,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Formiga,2014-12-31T00:00:00,1.2,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Formiga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Formiga,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Formiga,2017-12-31T00:00:00,1.8,1.0,44.44,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Formiga,2018-12-31T00:00:00,2.4,2.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibia,2012-12-31T00:00:00,1.74,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibia,2013-12-31T00:00:00,1.74,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibia,2014-12-31T00:00:00,1.5,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibia,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibia,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibia,2017-12-31T00:00:00,1.59,2.0,25.79,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibiraci,2012-12-31T00:00:00,2.49,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibiraci,2013-12-31T00:00:00,1.5,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibiraci,2014-12-31T00:00:00,2.01,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibiraci,2015-12-31T00:00:00,1.08,2.0,85.19,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibiraci,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibiraci,2017-12-31T00:00:00,1.61,2.0,24.22,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibiraci,2018-12-31T00:00:00,3.0,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Indianopolis,2012-12-31T00:00:00,2.4,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Indianopolis,2013-12-31T00:00:00,1.92,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Indianopolis,2014-12-31T00:00:00,2.45,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Indianopolis,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Indianopolis,2016-12-31T00:00:00,2.7,2.0,25.93,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Indianopolis,2017-12-31T00:00:00,1.67,2.0,19.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Indianopolis,2018-12-31T00:00:00,2.52,2.0,20.63,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Joao_Pinheiro,2012-12-31T00:00:00,2.82,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Joao_Pinheiro,2013-12-31T00:00:00,3.0,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Joao_Pinheiro,2014-12-31T00:00:00,2.7,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Joao_Pinheiro,2015-12-31T00:00:00,2.4,3.0,25.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Joao_Pinheiro,2016-12-31T00:00:00,3.6,3.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Joao_Pinheiro,2017-12-31T00:00:00,1.91,3.0,57.07,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Joao_Pinheiro,2018-12-31T00:00:00,2.4,3.0,25.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Medeiros,2012-12-31T00:00:00,2.1,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Medeiros,2013-12-31T00:00:00,1.8,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Medeiros,2014-12-31T00:00:00,1.56,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Medeiros,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Medeiros,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Medeiros,2017-12-31T00:00:00,1.79,2.0,11.73,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Medeiros,2018-12-31T00:00:00,2.04,2.0,1.96,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Monte_Carmelo,2012-12-31T00:00:00,2.34,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Monte_Carmelo,2013-12-31T00:00:00,1.89,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Monte_Carmelo,2014-12-31T00:00:00,2.28,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Monte_Carmelo,2015-12-31T00:00:00,1.32,2.0,51.52,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Monte_Carmelo,2016-12-31T00:00:00,2.64,2.0,24.24,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Monte_Carmelo,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Monte_Carmelo,2018-12-31T00:00:00,2.31,2.0,13.42,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ninheira,2012-12-31T00:00:00,3.0,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ninheira,2013-12-31T00:00:00,3.0,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ninheira,2014-12-31T00:00:00,3.0,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ninheira,2015-12-31T00:00:00,2.76,-0.0,100.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ninheira,2016-12-31T00:00:00,3.0,3.0,0.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ninheira,2017-12-31T00:00:00,2.34,3.0,28.21,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ninheira,2018-12-31T00:00:00,3.0,3.0,0.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Paracatu,2012-12-31T00:00:00,3.45,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Paracatu,2013-12-31T00:00:00,3.45,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Paracatu,2014-12-31T00:00:00,3.0,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Paracatu,2015-12-31T00:00:00,1.8,3.0,66.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Paracatu,2016-12-31T00:00:00,2.4,3.0,25.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Paracatu,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Paracatu,2018-12-31T00:00:00,3.0,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Passos,2012-12-31T00:00:00,2.16,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Passos,2013-12-31T00:00:00,1.32,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Passos,2014-12-31T00:00:00,1.08,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Passos,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Passos,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Passos,2017-12-31T00:00:00,1.92,1.0,47.92,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Passos,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Patrocinio,2012-12-31T00:00:00,2.1,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Patrocinio,2013-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Patrocinio,2014-12-31T00:00:00,1.8,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Patrocinio,2015-12-31T00:00:00,1.25,1.0,20.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Patrocinio,2016-12-31T00:00:00,2.64,2.0,24.24,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Patrocinio,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Patrocinio,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Perdizes,2012-12-31T00:00:00,1.68,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Perdizes,2013-12-31T00:00:00,1.56,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Perdizes,2014-12-31T00:00:00,1.66,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Perdizes,2015-12-31T00:00:00,1.48,2.0,35.14,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Perdizes,2016-12-31T00:00:00,1.98,2.0,1.01,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Perdizes,2017-12-31T00:00:00,2.07,2.0,3.38,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Perdizes,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Pimenta,2012-12-31T00:00:00,1.8,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Pimenta,2013-12-31T00:00:00,1.74,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Pimenta,2014-12-31T00:00:00,1.62,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Pimenta,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Pimenta,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Pimenta,2017-12-31T00:00:00,1.73,2.0,15.61,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Pimenta,2018-12-31T00:00:00,2.7,2.0,25.93,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Piumhi,2012-12-31T00:00:00,1.44,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Piumhi,2013-12-31T00:00:00,1.5,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Piumhi,2014-12-31T00:00:00,1.2,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Piumhi,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Piumhi,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Piumhi,2017-12-31T00:00:00,1.36,1.0,26.47,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Piumhi,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Presidente_Olegario,2012-12-31T00:00:00,2.66,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Presidente_Olegario,2013-12-31T00:00:00,2.65,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Presidente_Olegario,2014-12-31T00:00:00,2.32,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Presidente_Olegario,2015-12-31T00:00:00,1.88,2.0,6.38,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Presidente_Olegario,2016-12-31T00:00:00,2.52,3.0,19.05,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Presidente_Olegario,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Presidente_Olegario,2018-12-31T00:00:00,2.28,2.0,12.28,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Rio_Paranaiba,2012-12-31T00:00:00,2.16,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Rio_Paranaiba,2013-12-31T00:00:00,2.07,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Rio_Paranaiba,2014-12-31T00:00:00,1.89,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Rio_Paranaiba,2015-12-31T00:00:00,1.42,2.0,40.85,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Rio_Paranaiba,2016-12-31T00:00:00,2.08,2.0,3.85,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Rio_Paranaiba,2017-12-31T00:00:00,1.39,2.0,43.88,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Rio_Paranaiba,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Romaria,2012-12-31T00:00:00,2.1,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Romaria,2013-12-31T00:00:00,2.1,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Romaria,2014-12-31T00:00:00,2.28,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Romaria,2015-12-31T00:00:00,1.2,2.0,66.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Romaria,2016-12-31T00:00:00,2.4,3.0,25.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Romaria,2017-12-31T00:00:00,1.53,2.0,30.72,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Romaria,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sacramento,2012-12-31T00:00:00,2.22,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sacramento,2013-12-31T00:00:00,1.87,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sacramento,2014-12-31T00:00:00,1.53,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sacramento,2015-12-31T00:00:00,1.34,1.0,25.37,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sacramento,2016-12-31T00:00:00,1.87,2.0,6.95,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sacramento,2017-12-31T00:00:00,1.31,2.0,52.67,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sacramento,2018-12-31T00:00:00,1.87,2.0,6.95,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Santa_Rosa_da_Serra,2012-12-31T00:00:00,3.17,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Santa_Rosa_da_Serra,2013-12-31T00:00:00,3.0,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Santa_Rosa_da_Serra,2014-12-31T00:00:00,2.1,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Santa_Rosa_da_Serra,2015-12-31T00:00:00,1.62,2.0,23.46,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Santa_Rosa_da_Serra,2016-12-31T00:00:00,1.5,3.0,100.0,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Santa_Rosa_da_Serra,2017-12-31T00:00:00,1.63,2.0,22.7,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Santa_Rosa_da_Serra,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sao_Gotardo,2012-12-31T00:00:00,1.92,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sao_Gotardo,2013-12-31T00:00:00,1.92,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sao_Gotardo,2014-12-31T00:00:00,1.71,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sao_Gotardo,2015-12-31T00:00:00,1.62,2.0,23.46,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sao_Gotardo,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sao_Gotardo,2017-12-31T00:00:00,1.44,2.0,38.89,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sao_Gotardo,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Serra_do_Salitre,2012-12-31T00:00:00,3.1,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Serra_do_Salitre,2013-12-31T00:00:00,1.74,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Serra_do_Salitre,2014-12-31T00:00:00,2.22,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Serra_do_Salitre,2015-12-31T00:00:00,1.29,2.0,55.04,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Serra_do_Salitre,2016-12-31T00:00:00,3.0,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Serra_do_Salitre,2017-12-31T00:00:00,1.12,2.0,78.57,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Serra_do_Salitre,2018-12-31T00:00:00,2.34,2.0,14.53,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Tiros,2012-12-31T00:00:00,2.28,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Tiros,2013-12-31T00:00:00,2.28,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Tiros,2014-12-31T00:00:00,1.79,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Tiros,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Tiros,2016-12-31T00:00:00,2.1,2.0,4.76,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Tiros,2017-12-31T00:00:00,3.0,2.0,33.33,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Tiros,2018-12-31T00:00:00,1.92,3.0,56.25,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Unai,2012-12-31T00:00:00,2.94,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Unai,2013-12-31T00:00:00,3.0,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Unai,2014-12-31T00:00:00,2.94,-,-,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Unai,2015-12-31T00:00:00,2.64,3.0,13.64,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Unai,2016-12-31T00:00:00,2.7,3.0,11.11,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Unai,2017-12-31T00:00:00,2.6,3.0,15.38,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Unai,2018-12-31T00:00:00,2.52,3.0,19.05,treino,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araguari,2019-12-31T00:00:00,2.0,3,50.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araxa,2019-12-31T00:00:00,1.0,2,100.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Bambui,2019-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Buritizeiro,2019-12-31T00:00:00,3.0,2,-33.33,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Campos_Altos,2019-12-31T00:00:00,1.0,2,100.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Carmo_do_Paranaiba,2019-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Claraval,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Coromandel,2019-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Estrela_do_Sul,2019-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Formiga,2019-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibia,2019-12-31T00:00:00,1.0,2,100.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibiraci,2019-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Indianopolis,2019-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Joao_Pinheiro,2019-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Medeiros,2019-12-31T00:00:00,1.0,2,100.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Monte_Carmelo,2019-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ninheira,2019-12-31T00:00:00,3.0,3,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Paracatu,2019-12-31T00:00:00,2.0,3,50.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Passos,2019-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Patrocinio,2019-12-31T00:00:00,1.0,2,100.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Perdizes,2019-12-31T00:00:00,1.0,2,100.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Pimenta,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Piumhi,2019-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Presidente_Olegario,2019-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Rio_Paranaiba,2019-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Romaria,2019-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sacramento,2019-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Santa_Rosa_da_Serra,2019-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sao_Gotardo,2019-12-31T00:00:00,1.0,2,100.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Serra_do_Salitre,2019-12-31T00:00:00,1.0,1,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Tiros,2019-12-31T00:00:00,2.0,2,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Unai,2019-12-31T00:00:00,3.0,3,0.0,validacao,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araguari,2020-12-31T00:00:00,1.979964695498676,2.1250016689300537,7.33,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araxa,2020-12-31T00:00:00,1.786008230452675,1.6057976484298706,-10.09,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Bambui,2020-12-31T00:00:00,1.7998829724985372,1.608297348022461,-10.64,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Buritizeiro,2020-12-31T00:00:00,3.0,3.8331851959228516,27.77,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Campos_Altos,2020-12-31T00:00:00,2.1000529941706416,1.9390180110931396,-7.67,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Carmo_do_Paranaiba,2020-12-31T00:00:00,1.947480785653288,1.9115612506866455,-1.84,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Claraval,2020-12-31T00:00:00,2.796008294453085,2.1810882091522217,-21.99,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Coromandel,2020-12-31T00:00:00,1.8189342403628117,2.4221351146698,33.16,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Estrela_do_Sul,2020-12-31T00:00:00,2.0627027027027025,2.1939451694488525,6.36,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Formiga,2020-12-31T00:00:00,1.441791044776119,2.086350202560425,44.71,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibia,2020-12-31T00:00:00,1.746,1.456018090248108,-16.61,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibiraci,2020-12-31T00:00:00,2.6315194346289745,2.6408803462982178,0.36,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Indianopolis,2020-12-31T00:00:00,1.8000000000000005,1.8976777791976929,5.43,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Joao_Pinheiro,2020-12-31T00:00:00,3.0,2.832023859024048,-5.6,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Medeiros,2020-12-31T00:00:00,1.980113636363636,1.840967059135437,-7.03,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Monte_Carmelo,2020-12-31T00:00:00,1.98,2.0409088134765625,3.08,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ninheira,2020-12-31T00:00:00,3.0,3.059391736984253,1.98,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Paracatu,2020-12-31T00:00:00,2.7000000000000006,2.8512251377105713,5.6,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Passos,2020-12-31T00:00:00,1.62,2.046848773956299,26.35,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Patrocinio,2020-12-31T00:00:00,1.7479986236953782,1.92318856716156,10.02,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Perdizes,2020-12-31T00:00:00,2.050845253576073,1.9712055921554565,-3.88,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Pimenta,2020-12-31T00:00:00,1.8000000000000005,2.3813300132751465,32.3,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Piumhi,2020-12-31T00:00:00,1.7399966931216928,1.6682419776916504,-4.12,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Presidente_Olegario,2020-12-31T00:00:00,1.9814229249011863,2.2006804943084717,11.07,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Rio_Paranaiba,2020-12-31T00:00:00,1.818897637795276,1.7008042335510254,-6.49,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Romaria,2020-12-31T00:00:00,2.330769230769231,2.2104623317718506,-5.16,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sacramento,2020-12-31T00:00:00,1.8321428571428573,1.4720206260681152,-19.66,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Santa_Rosa_da_Serra,2020-12-31T00:00:00,2.1,1.717393398284912,-18.22,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sao_Gotardo,2020-12-31T00:00:00,1.38027397260274,1.5301153659820557,10.86,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Serra_do_Salitre,2020-12-31T00:00:00,1.8000000000000005,1.9408005475997925,7.82,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Tiros,2020-12-31T00:00:00,1.740235294117647,1.7996764183044434,3.42,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Unai,2020-12-31T00:00:00,2.3398907103825146,2.783578872680664,18.96,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araguari,2021-12-31T00:00:00,1.8600340136054423,1.8194862604141235,-2.18,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araxa,2021-12-31T00:00:00,1.250440917107584,1.4797868728637695,18.34,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Bambui,2021-12-31T00:00:00,1.439910025706941,1.6573253870010376,15.1,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Buritizeiro,2021-12-31T00:00:00,3.0,2.232257604598999,-25.59,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Campos_Altos,2021-12-31T00:00:00,1.2,1.755873441696167,46.32,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Carmo_do_Paranaiba,2021-12-31T00:00:00,1.541078066914498,1.8708348274230957,21.4,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Claraval,2021-12-31T00:00:00,1.050113378684807,1.6104912757873535,53.36,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Coromandel,2021-12-31T00:00:00,1.249939246658566,1.712816834449768,37.03,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Estrela_do_Sul,2021-12-31T00:00:00,1.8000000000000005,1.7360637187957764,-3.55,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Formiga,2021-12-31T00:00:00,1.347576301615799,1.393923282623291,3.44,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibia,2021-12-31T00:00:00,1.2,1.4109811782836914,17.58,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibiraci,2021-12-31T00:00:00,1.327034071867436,1.699460744857788,28.06,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Indianopolis,2021-12-31T00:00:00,1.919956379498364,1.8625767230987549,-2.99,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Joao_Pinheiro,2021-12-31T00:00:00,2.4,2.2986648082733154,-4.22,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Medeiros,2021-12-31T00:00:00,1.500161864681127,1.760949969291687,17.38,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Monte_Carmelo,2021-12-31T00:00:00,1.8000000000000005,1.7828067541122437,-0.96,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ninheira,2021-12-31T00:00:00,2.100246002460024,2.895003080368042,37.84,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Paracatu,2021-12-31T00:00:00,2.4,2.493898868560791,3.91,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Passos,2021-12-31T00:00:00,0.99,1.6389182806015015,65.55,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Patrocinio,2021-12-31T00:00:00,1.4369951534733445,1.7462674379348755,21.52,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Perdizes,2021-12-31T00:00:00,1.261308677098151,1.4174884557724,12.38,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Pimenta,2021-12-31T00:00:00,1.3502325581395351,1.7900898456573486,32.58,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Piumhi,2021-12-31T00:00:00,1.5,1.5532443523406982,3.55,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Presidente_Olegario,2021-12-31T00:00:00,1.637164750957854,1.8834710121154785,15.04,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Rio_Paranaiba,2021-12-31T00:00:00,1.080031384856807,1.4871832132339478,37.7,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Romaria,2021-12-31T00:00:00,2.052631578947369,2.1585912704467773,5.16,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sacramento,2021-12-31T00:00:00,1.25645342312009,1.3591349124908447,8.17,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Santa_Rosa_da_Serra,2021-12-31T00:00:00,1.5,1.8115532398223877,20.77,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sao_Gotardo,2021-12-31T00:00:00,1.26027397260274,1.593332052230835,26.43,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Serra_do_Salitre,2021-12-31T00:00:00,1.4676384839650147,1.5583373308181763,6.18,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Tiros,2021-12-31T00:00:00,1.2,1.6327714920043945,36.06,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Unai,2021-12-31T00:00:00,2.639892904953146,2.4121358394622803,-8.63,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araguari,2022-12-31T00:00:00,1.919968366943456,1.832843542098999,-4.54,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araxa,2022-12-31T00:00:00,1.077872465471643,1.7824714183807373,65.37,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Bambui,2022-12-31T00:00:00,1.5,1.5836598873138428,5.58,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Buritizeiro,2022-12-31T00:00:00,3.0,2.9625213146209717,-1.25,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Campos_Altos,2022-12-31T00:00:00,1.680042238648363,1.5448143482208252,-8.05,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Carmo_do_Paranaiba,2022-12-31T00:00:00,1.392969472710453,1.8072874546051025,29.74,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Claraval,2022-12-31T00:00:00,1.289915966386555,2.0900135040283203,62.03,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Coromandel,2022-12-31T00:00:00,1.260025542784164,1.533634901046753,21.71,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Estrela_do_Sul,2022-12-31T00:00:00,1.460045146726862,1.9381670951843262,32.75,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Formiga,2022-12-31T00:00:00,1.8000000000000005,1.3851251602172852,-23.05,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibia,2022-12-31T00:00:00,1.14,1.5106511116027832,32.51,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibiraci,2022-12-31T00:00:00,1.377007874015748,1.8452680110931396,34.01,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Indianopolis,2022-12-31T00:00:00,1.8000000000000005,1.8522090911865234,2.9,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Joao_Pinheiro,2022-12-31T00:00:00,2.4,2.670039653778076,11.25,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Medeiros,2022-12-31T00:00:00,1.5,1.6395255327224731,9.3,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Monte_Carmelo,2022-12-31T00:00:00,1.7458874458874465,1.7759160995483398,1.72,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ninheira,2022-12-31T00:00:00,2.9401197604790417,3.169544219970703,7.8,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Paracatu,2022-12-31T00:00:00,2.4,2.4440712928771973,1.84,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Passos,2022-12-31T00:00:00,1.290038314176245,1.5059094429016113,16.73,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Patrocinio,2022-12-31T00:00:00,0.9438953724513282,1.541666030883789,63.33,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Perdizes,2022-12-31T00:00:00,1.501272727272727,1.6618484258651733,10.7,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Pimenta,2022-12-31T00:00:00,1.3502325581395351,1.642129898071289,21.62,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Piumhi,2022-12-31T00:00:00,1.2,1.5764216184616089,31.37,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Presidente_Olegario,2022-12-31T00:00:00,1.8899598393574304,1.910334587097168,1.08,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Rio_Paranaiba,2022-12-31T00:00:00,1.230031446540881,1.5410208702087402,25.28,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Romaria,2022-12-31T00:00:00,1.910526315789474,2.1843771934509277,14.33,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sacramento,2022-12-31T00:00:00,1.5575892857142857,1.5620496273040771,0.29,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Santa_Rosa_da_Serra,2022-12-31T00:00:00,2.22,1.8143669366836548,-18.27,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sao_Gotardo,2022-12-31T00:00:00,1.50025974025974,1.3634871244430542,-9.12,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Serra_do_Salitre,2022-12-31T00:00:00,1.5168750000000002,1.614557147026062,6.44,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Tiros,2022-12-31T00:00:00,1.6801801801801797,1.5496575832366943,-7.77,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Unai,2022-12-31T00:00:00,2.52010582010582,2.6885805130004883,6.69,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araguari,2023-12-31T00:00:00,3.0,2.022517442703247,-32.58,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Araxa,2023-12-31T00:00:00,1.638401296246287,1.6796739101409912,2.52,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Bambui,2023-12-31T00:00:00,1.5,1.7462910413742065,16.42,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Buritizeiro,2023-12-31T00:00:00,3.0,13.313468933105469,343.78,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Campos_Altos,2023-12-31T00:00:00,1.380040526849037,1.9145349264144897,38.73,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Carmo_do_Paranaiba,2023-12-31T00:00:00,2.369801007771799,1.9178944826126099,-19.07,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Claraval,2023-12-31T00:00:00,1.8600214362272245,2.0556256771087646,10.52,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Coromandel,2023-12-31T00:00:00,2.0926575541308825,1.8798540830612183,-10.17,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Estrela_do_Sul,2023-12-31T00:00:00,2.43015873015873,2.2124221324920654,-8.96,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Formiga,2023-12-31T00:00:00,2.021543985637344,1.775374174118042,-12.18,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibia,2023-12-31T00:00:00,1.668,1.7163395881652832,2.9,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ibiraci,2023-12-31T00:00:00,1.804169884169884,2.146646499633789,18.98,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Indianopolis,2023-12-31T00:00:00,2.7000000000000006,1.8996822834014893,-29.64,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Joao_Pinheiro,2023-12-31T00:00:00,3.3,3.130500078201294,-5.14,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Medeiros,2023-12-31T00:00:00,1.86,1.87199866771698,0.65,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Monte_Carmelo,2023-12-31T00:00:00,2.4900284900284904,1.9665112495422363,-21.02,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Ninheira,2023-12-31T00:00:00,3.0,2.874723434448242,-4.18,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Paracatu,2023-12-31T00:00:00,2.4,2.8211138248443604,17.55,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Passos,2023-12-31T00:00:00,1.829801324503311,2.2251925468444824,21.61,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Patrocinio,2023-12-31T00:00:00,2.4370275910039414,2.0604043006896973,-15.45,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Perdizes,2023-12-31T00:00:00,2.106451612903226,2.040642738342285,-3.12,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Pimenta,2023-12-31T00:00:00,1.504186046511628,1.7537884712219238,16.59,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Piumhi,2023-12-31T00:00:00,1.7399633363886338,1.7592065334320068,1.11,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Presidente_Olegario,2023-12-31T00:00:00,2.160074626865672,2.0029382705688477,-7.27,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Rio_Paranaiba,2023-12-31T00:00:00,1.8000000000000005,1.676086187362671,-6.88,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Romaria,2023-12-31T00:00:00,2.131578947368421,2.2876667976379395,7.32,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sacramento,2023-12-31T00:00:00,1.75,1.9603092670440674,12.02,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Santa_Rosa_da_Serra,2023-12-31T00:00:00,1.68,2.1659111976623535,28.92,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Sao_Gotardo,2023-12-31T00:00:00,1.5,1.476943016052246,-1.54,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Serra_do_Salitre,2023-12-31T00:00:00,1.489636363636364,1.8125910758972168,21.68,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Tiros,2023-12-31T00:00:00,2.382038834951457,1.9192068576812744,-19.43,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
IBGE Cluster 2 - Metodo 1 (2020-2023),Unai,2023-12-31T00:00:00,2.520095187731359,2.6146328449249268,3.75,teste,V24 - metodo 1,LSTM,"Modelo LSTM treinado com dados de 2012 a 2018, validado com os dados de 2019 e testado com os dados de 2020-2023.
Para esse treinamento foi utilizado o dataset V24, que possui duas versões, uma com o metodo 1 e outra com o metodo 2.
O metodo usado para esse modelo foi o metodo 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010251028747900914
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.2
weight_decay: 1e-05
steps: 100
",2025-08-27T13:59:08
